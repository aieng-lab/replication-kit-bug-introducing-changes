{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lschulte/VCS/cdbs_inducing_eval/env/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import ast\n",
    "from scipy.stats import chi2_contingency, fisher_exact, spearmanr, kruskal\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import pickle as pkl\n",
    "from dotenv import load_dotenv\n",
    "from threading import Thread\n",
    "from enum import Enum\n",
    "import shutil\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information = pd.read_csv('input/all_information.csv', sep='\\t')\n",
    "variables_overview = pd.read_csv('input/variables_overview.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_THRESHOLD = 8\n",
    "inverse_str = 'not'\n",
    "\n",
    "BUG_ID = 'F1 - Bug ID'\n",
    "\n",
    "class Aspect(Enum):\n",
    "    target = 'target'\n",
    "    topic = 'topic'\n",
    "    action = 'action'\n",
    "\n",
    "ASPECTS = [Aspect.target, Aspect.topic, Aspect.action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables_df() -> pd.DataFrame:\n",
    "    '''Returns the variables dataframe'''\n",
    "    return pd.read_excel(f'input/coded_dataset/variables.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "def get_nom_list_variables() -> pd.DataFrame:\n",
    "    '''Returns all nominal and list of nominal variables'''\n",
    "    return variables_overview[variables_overview['Scale'].isin(['Nominal', 'List of nominals', 'Boolean'])]\n",
    "\n",
    "def get_num_variables() -> pd.DataFrame:\n",
    "    '''Returns all numerical variables'''\n",
    "    return variables_overview[variables_overview['Scale'].isin(['Integer', 'Float'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_identifier(variable_name: str) -> bool:\n",
    "    '''Returns True if the variable name refers to an identifier, False otherwise'''\n",
    "    return variable_name.startswith('Identifier') or variable_name.endswith('ID') or variable_name.endswith('IDs') or variable_name.endswith('hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(bug, cg, comp):\n",
    "    ''' Returns the group of the variable based on the significance of the chi2 test\n",
    "    Args:\n",
    "        bug (bool): Whether the difference in distribution (variance) betweent the variables in the bug group is significant\n",
    "        cg (bool): Whether the difference in distribution (variance) betweent the variables in the control group is significant\n",
    "        comp (bool): Whether the difference in distribution (variance) between the two groups is significant\n",
    "    Returns:\n",
    "        str: The group of the variable\n",
    "    '''\n",
    "\n",
    "    if bug is None and cg is None and comp is None:\n",
    "        return 'E.1'\n",
    "\n",
    "    if None in [bug, cg] and True in [bug, cg]:\n",
    "        return '32' # data not available for one group, however significant variance in the other group\n",
    "\n",
    "    if None in [bug, cg] and False in [bug, cg]:\n",
    "        return '31' # data not available for one group, no significant variance in the other group\n",
    "\n",
    "    if bug == False and cg == False:\n",
    "        return '1' # no significant variance in both groups\n",
    "    \n",
    "    if bug == True and cg == True:\n",
    "        if comp is None: # comparison not possible, however variance in both groups are significant\n",
    "            return '4.1'\n",
    "        if comp == True: # significant difference between CG and BG\n",
    "            return '3.1'\n",
    "        if comp == False: # no significant difference between CG and BG\n",
    "            return '2.1'\n",
    "    \n",
    "    if (bug == True and cg == False) or (bug == False and cg == True):\n",
    "        # for 3.3 we do not need to compare the groups\n",
    "        return '3.3' # significant variance in one group, no significant variance in the other group\n",
    "    \n",
    "    raise Exception(f'This should not happen... {bug} {cg} {comp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/group_assignment/'\n",
    "figure_output_path = f'{output_path}figures/'\n",
    "\n",
    "def get_zip():\n",
    "    '''Creates a zip file of the output folder'''\n",
    "    now = datetime.datetime.now()\n",
    "    shutil.make_archive(f'output_{now.strftime(\"%Y-%m-%d_%H-%M-%S\")}', 'zip', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_ENC_SEP = '+++'\n",
    "\n",
    "def get_sub_column_name(column, label):\n",
    "    '''Returns formatted sub column name with separator'''\n",
    "    return f'{column}{BIN_ENC_SEP}{label}'\n",
    "\n",
    "def get_labels_from_sub_columns(df: pd.DataFrame, column: str):\n",
    "    '''Splits the nominal labels from the(formatted) sub columns \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe\n",
    "        column (str): The column name\n",
    "    Returns:\n",
    "        list: The nominal labels of column\n",
    "        list: The sub columns of column\n",
    "    '''\n",
    "    sub_columns = [c for c in df.columns if c.startswith(column)]\n",
    "    nominal_labels = [c.split(BIN_ENC_SEP)[1] for c in sub_columns if BIN_ENC_SEP in c]\n",
    "\n",
    "    return nominal_labels, sub_columns\n",
    "\n",
    "def multi_label_binarize(df: pd.DataFrame, column):\n",
    "    '''Binarizes the multi label column\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe\n",
    "        column (str): The column name that should be binarized\n",
    "    Returns:\n",
    "        pd.DataFrame: The binarized dataframe, new column names are formatted sub column name with separator\n",
    "    '''\n",
    "\n",
    "    labels = set()\n",
    "    def get_all_labels(x):\n",
    "        if x is not None:\n",
    "            labels.update(x)\n",
    "    \n",
    "    df[column].apply(lambda x: get_all_labels(x))\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([list(labels)])\n",
    "\n",
    "    encoded_df = pd.DataFrame(mlb.transform(df[column]), columns=[get_sub_column_name(column, mlb_class) for mlb_class in mlb.classes_]).astype(bool)\n",
    "    return pd.concat([df.drop(column, axis=1).reset_index(drop=True), encoded_df], axis=1)\n",
    "\n",
    "def single_label_binarize(df: pd.DataFrame, column):\n",
    "    '''Binarizes a single column\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe\n",
    "        column (str): The column name that should be binarized\n",
    "    Returns:\n",
    "        pd.DataFrame: The binarized dataframe (get_dummies), new column names are formatted sub column name with separator\n",
    "    '''\n",
    "    return pd.get_dummies(df, columns=[column], prefix=column, prefix_sep=BIN_ENC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(x):\n",
    "    '''Removes the axial code level topic for simplification of codes'''\n",
    "    if type(x) == str:\n",
    "        return x.split('::(')[0]\n",
    "    if type(x) == list:\n",
    "        return [remove_suffix(y) for y in x]\n",
    "\n",
    "def get_inv_name(name: str) -> str:\n",
    "    '''Returns the inverse name of the variable for One versus Rest encoding'''\n",
    "    return f'{name} [{inverse_str}]'\n",
    "\n",
    "def get_aspect_columns(column):\n",
    "    '''Returns the column names of aspect columns of a column'''\n",
    "    return [f'{column}: {aspect.name}' for aspect in ASPECTS]\n",
    "\n",
    "def transform_columns(df: pd.DataFrame, columns: list[str], scales: list[str]) -> pd.Series:\n",
    "    '''Transforms the columns based according to the scales and binarizes the columns where necessary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe\n",
    "        columns (list[str]): The columns that should be transformed\n",
    "        scales (list[str]): The scales of the columns\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed dataframe\n",
    "    '''\n",
    "\n",
    "    df = df.copy().dropna(subset=columns)\n",
    "\n",
    "    for column, scale in zip(columns, scales):\n",
    "        scale = scale.lower()\n",
    "        id = column.split(' - ')[0]\n",
    "        name = column.split(' - ')[1]\n",
    "\n",
    "        if scale == 'list of nominals':\n",
    "\n",
    "            def get_aspect(index, list_of_aspects):\n",
    "                if list_of_aspects is None:\n",
    "                    return None\n",
    "                return [x[index] for x in [x.split('_') for x in list_of_aspects] if len(x) > index]\n",
    "\n",
    "            df[column] = df[column].apply(lambda x: ast.literal_eval(x) if not pd.isna(x) else None)\n",
    "\n",
    "            if name.startswith('Aspect'):\n",
    "                aspect = Aspect[name.split(': ')[1]]\n",
    "                aspect_index = ASPECTS.index(aspect)\n",
    "\n",
    "                df[column] = df[column].apply(lambda x: get_aspect(aspect_index, x) if x is not None else None)\n",
    "\n",
    "            df = multi_label_binarize(df, column)\n",
    "        else:\n",
    "            if scale not in ['integer', 'float']:\n",
    "                df[column] = df[column].astype(str)\n",
    "\n",
    "            if scale == 'integer':\n",
    "                df[column] = df[column].astype(int)\n",
    "\n",
    "            if id in ['C2.1', 'C2.2', 'C2.3', 'B2']:\n",
    "                df[column] = df[column].apply(lambda x: remove_suffix(x))\n",
    "\n",
    "            if scale not in ['integer', 'float']:\n",
    "                df = single_label_binarize(df, column)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_error(column_a, column_b, reason, tag=None):\n",
    "    '''Returns a dictionary with the error information that can subsitute the result of the comparison functions'''\n",
    "    return {\n",
    "        'column_a': column_a,\n",
    "        'column_b': column_b,\n",
    "        'group': 'E',\n",
    "        'method': tag,\n",
    "        'reason': reason,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig: plt.Figure, file_path):\n",
    "    '''Saves the figure to the file path'''\n",
    "    fig.savefig(file_path, format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_labels(df, columns):\n",
    "    ''' Returns valid and threshold labels based on the frequency. \n",
    "        Valid labels are those that meet the frequency threshold for both the label and its inverse.\n",
    "        Threshold labels are those that meet the frequency threshold for the label.\n",
    "\n",
    "        If there is not at least one label meeting the threshold, the variable will not be considered.\n",
    "    '''\n",
    "    valid_labels = set()\n",
    "    threshold_labels = set()\n",
    "    for column in columns:\n",
    "        self_count = df[column].sum()\n",
    "        rest_count = (~df[column]).sum()\n",
    "\n",
    "        if self_count >= FREQ_THRESHOLD:\n",
    "            threshold_labels.add(column)\n",
    "            if rest_count >= FREQ_THRESHOLD:\n",
    "                valid_labels.add(column)\n",
    "\n",
    "    return valid_labels, threshold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_main_df = None\n",
    "\n",
    "def create_iterator(vars_a, vars_b, test_specific_pair=(None, None)):\n",
    "    '''Creates an iterator that yields the columns of the variables that should be compared. Skips the identifier columns and expands the aspect columns.\n",
    "\n",
    "    Args:\n",
    "        vars_a (pd.DataFrame): The variables of group A\n",
    "        vars_b (pd.DataFrame): The variables of group B\n",
    "        test_specific_pair (tuple): The specific pair of variables that should be compared\n",
    "    Returns:\n",
    "        tuple: The columns of the variables that should be compared\n",
    "    '''\n",
    "\n",
    "    global variables_main_df\n",
    "    \n",
    "    variables_main_df = get_variables_df()\n",
    "\n",
    "    def handle_aspects(column: str):\n",
    "        aspect_columns = get_aspect_columns(column)\n",
    "        for aspect_column in aspect_columns:\n",
    "            variables_main_df[aspect_column] = variables_main_df[column]\n",
    "        return aspect_columns\n",
    "\n",
    "    variables_a_done = set()\n",
    "\n",
    "    data_a = vars_a[vars_a['ID'] == test_specific_pair[0]] if test_specific_pair[0] is not None else vars_a\n",
    "    for id_a, name_a, scale_a, aggregation_a in data_a[['ID', 'Name', 'Scale', 'Aggregation']].values:\n",
    "        variables_a_done.add(id_a)\n",
    "        if is_identifier(name_a):\n",
    "            continue\n",
    "\n",
    "        columns_a = [f'{id_a} - {name_a}' + (f' [{aggregation_a}]' if pd.notna(aggregation_a) else '')]\n",
    "        if name_a.startswith('Aspects'):\n",
    "            columns_a = handle_aspects(columns_a[0])\n",
    "\n",
    "        for i_column_a, column_a in enumerate(columns_a):\n",
    "            data_b = vars_b[vars_b['ID'] == test_specific_pair[1]] if test_specific_pair[1] is not None else vars_b\n",
    "            for id_b, name_b, scale_b, aggregation_b in data_b[['ID', 'Name', 'Scale', 'Aggregation']].values:\n",
    "                if id_b in variables_a_done:\n",
    "                    continue\n",
    "\n",
    "                if is_identifier(name_b):\n",
    "                    continue\n",
    "\n",
    "                columns_b = [f'{id_b} - {name_b}' + (f' [{aggregation_b}]' if pd.notna(aggregation_b) else '')]\n",
    "                if name_b.startswith('Aspects'):\n",
    "                    columns_b = handle_aspects(columns_b[0])\n",
    "\n",
    "                for i_column_b, column_b in enumerate(columns_b):\n",
    "                    aspect_a = None\n",
    "                    aspect_b = None\n",
    "                    if len(columns_b) > 1:\n",
    "                        aspect_b = ASPECTS[i_column_b]\n",
    "                    if len(columns_a) > 1:\n",
    "                        aspect_a = ASPECTS[i_column_a]\n",
    "\n",
    "                    yield (column_a, column_b, scale_a, scale_b, aspect_a, aspect_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal Pairs\n",
    "\n",
    "Cross tablulation and fishers exact test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_crosstab(df: pd.DataFrame, flat_column_name) -> pd.DataFrame:\n",
    "    '''Flattens the crosstab dataframe with frequencies in a single column and pairs of labels as index'''\n",
    "    flattened = df.stack()\n",
    "    flattened.index = flattened.index.map(lambda x: '_'.join(map(str, x)))\n",
    "    return flattened.to_frame(flat_column_name)\n",
    "\n",
    "def get_comparison_table(crosstab_bug: pd.DataFrame, crosstab_cg: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Returns the comparison table based on the flattened crosstab dataframes'''\n",
    "    df_bug = flatten_crosstab(crosstab_bug, 'bug')\n",
    "    df_cg = flatten_crosstab(crosstab_cg, 'cg')\n",
    "\n",
    "    comp = pd.concat([df_bug, df_cg], axis=1).fillna(0)\n",
    "    return comp\n",
    "\n",
    "def get_crosstab(variables_df: pd.DataFrame, columns_a, columns_b, inverse, filter_frequency, cg=None):\n",
    "    '''Returns the crosstab of the columns_a and columns_b of the variables dataframe\n",
    "    \n",
    "    Args:\n",
    "        variables_df (pd.DataFrame): The variables dataframe\n",
    "        columns_a (list): The columns of group A\n",
    "        columns_b (list): The columns of group B\n",
    "        inverse (bool): Whether the inverse of the columns should be considered\n",
    "        filter_frequency (bool): Whether the frequency threshold should be applied\n",
    "        cg (bool): Whether the control group should be considered\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cross tabulation\n",
    "    '''\n",
    "\n",
    "    if cg is True:\n",
    "        variables_df = variables_df[variables_df['CG'] == True]\n",
    "    elif cg is False:\n",
    "        variables_df = variables_df[variables_df['CG'] == False]\n",
    "\n",
    "    def get_labels_df(columns):\n",
    "        items = []\n",
    "        for column in columns:\n",
    "            column_items = []\n",
    "            column_items_inverse = []\n",
    "            for bug_id, item in variables_df[[BUG_ID, column]].values:\n",
    "                if item == 1:\n",
    "                    column_items.append({\n",
    "                        BUG_ID: bug_id,\n",
    "                        'label': column\n",
    "                    })\n",
    "                elif inverse:\n",
    "                    column_items_inverse.append({\n",
    "                        BUG_ID: bug_id,\n",
    "                        'label': get_inv_name(column)\n",
    "                    })\n",
    "            if filter_frequency:\n",
    "                # Only add items if they meet the frequency threshold\n",
    "                if len(column_items) >= FREQ_THRESHOLD:\n",
    "                    items.extend(column_items)\n",
    "                \n",
    "                    if len(column_items_inverse) >= FREQ_THRESHOLD:\n",
    "                        items.extend(column_items_inverse)\n",
    "            else:\n",
    "                items.extend(column_items)\n",
    "                items.extend(column_items_inverse)\n",
    "\n",
    "        if len(items) == 0:\n",
    "            items = [{BUG_ID: None, 'label': None}]\n",
    "        return pd.DataFrame(items)\n",
    "\n",
    "    labels_a = get_labels_df(columns_a)\n",
    "    labels_b = get_labels_df(columns_b)\n",
    "\n",
    "    joined = labels_a.merge(labels_b, on=BUG_ID, how='outer', suffixes=('_a', '_b'))\n",
    "    crosstab = pd.crosstab(joined['label_a'], joined['label_b'])\n",
    "\n",
    "    # crosstab.to_csv(f'{output_path}{inverse_a}_{inverse_b}_{cg}.csv')\n",
    "\n",
    "    return crosstab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_significance_count = []\n",
    "\n",
    "def get_fisher_significance(crosstab):\n",
    "    \"\"\" Perform a Fisher's exact test of independence on the crosstab\n",
    "\n",
    "    Args:\n",
    "        crosstab (pd.DataFrame): Crosstab of the two variables\n",
    "    Returns:\n",
    "        tuple: (bool, float) Whether the difference in distribution is significant, and the p-value\n",
    "    \"\"\"\n",
    "    global fisher_significance_count\n",
    "\n",
    "    res = fisher_exact(crosstab)\n",
    "    p_value = res.pvalue\n",
    "\n",
    "    if p_value is not None and not np.isnan(p_value):\n",
    "        fisher_significance_count.append(p_value <= 0.05)\n",
    "\n",
    "    return (p_value <= 0.05, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_bool_vars = get_nom_list_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(crosstab_bug, crosstab_cg, column_a, column_b):\n",
    "    '''Returns the group of the variable based on the significance of the fisher test\n",
    "\n",
    "    Args:\n",
    "        crosstab_bug (pd.DataFrame): The crosstab of the bug group\n",
    "        crosstab_cg (pd.DataFrame): The crosstab of the control group\n",
    "        column_a (str): The name of the first column of the pair\n",
    "        column_b (str): The name of the second column of the pair\n",
    "    Returns:\n",
    "        dict: The group of the variable and the reason\n",
    "    '''\n",
    "\n",
    "    reason = []\n",
    "\n",
    "    def get_significance(crosstab, tag):\n",
    "        significant = None\n",
    "        try:\n",
    "            (significant, p_value) = get_fisher_significance(crosstab)\n",
    "            reason.append(f'{tag}: p-value: {p_value}, {crosstab.shape}')\n",
    "        except ValueError as e:\n",
    "            reason.append(f'{tag} has no data: {e}')\n",
    "        return significant\n",
    "    \n",
    "    significant_cg = get_significance(crosstab_cg, 'CG')\n",
    "    significant_bug = get_significance(crosstab_bug, 'BG')\n",
    "\n",
    "    if significant_bug is None and significant_cg is None:\n",
    "        return set_error(column_a, column_b, '\\n'.join(reason))\n",
    "\n",
    "    significant_comp = None\n",
    "    comparison_table = get_comparison_table(crosstab_bug, crosstab_cg)\n",
    "\n",
    "    significant_comp = get_significance(comparison_table, 'Comparison Table')\n",
    "    side = ''\n",
    "\n",
    "    if significant_bug is not None and significant_cg is not None:\n",
    "        if significant_bug and not significant_cg:\n",
    "            side = 'BG'\n",
    "        if not significant_bug and significant_cg:\n",
    "            side = 'CG'\n",
    "        \n",
    "\n",
    "    return {\n",
    "        'group': get_group(significant_bug, significant_cg, significant_comp),\n",
    "        'reason': '\\n'.join(reason),\n",
    "        'side': side\n",
    "    }\n",
    "    \n",
    "def df_get(df: pd.DataFrame, a, b, fallback = 0) -> int:\n",
    "    '''Returns the value of the dataframe at the index a and column b, if the value is not available, the fallback value is returned'''\n",
    "    try:\n",
    "        return int(df.loc[a, b])\n",
    "    except:\n",
    "        return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B1 - Build tools: : 41it [00:01, 26.40it/s, B2 - Dependency resolution]                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significant results: [0.12380952]\n",
      "Done - 2025-04-10 21:31:11.421694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "group\n",
       "1      18\n",
       "31     12\n",
       "32      6\n",
       "3.3     4\n",
       "2.1     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "fisher_significance_count = []\n",
    "\n",
    "TEST_SPECIFIC_PAIR = (None, None)\n",
    "# TEST_SPECIFIC_PAIR = (None, 'B2')\n",
    "\n",
    "result_threads = []\n",
    "\n",
    "p_bar = tqdm.tqdm(create_iterator(nom_bool_vars, nom_bool_vars, TEST_SPECIFIC_PAIR))\n",
    "for column_a, column_b, scale_a, scale_b, _, _ in p_bar:\n",
    "    p_bar.set_description(f'Processing {column_a}')\n",
    "    p_bar.set_postfix_str(f'{column_b}')\n",
    "    \n",
    "    variables_df = transform_columns(variables_main_df, [column_a, column_b], [scale_a, scale_b])\n",
    "\n",
    "\n",
    "    _, column_a_sub_columns = get_labels_from_sub_columns(variables_df, column_a)\n",
    "    _, column_b_sub_columns = get_labels_from_sub_columns(variables_df, column_b)\n",
    "\n",
    "    add_inverse = False\n",
    "    filter_frequency = False\n",
    "\n",
    "    crosstab_bug = get_crosstab(variables_df, column_a_sub_columns, column_b_sub_columns, add_inverse, filter_frequency, cg=False)\n",
    "    crosstab_cg = get_crosstab(variables_df, column_a_sub_columns, column_b_sub_columns, add_inverse, filter_frequency, cg=True)\n",
    "    \n",
    "    result_ovr_ab = get_relation(crosstab_bug, crosstab_cg, column_a, column_b)\n",
    "\n",
    "\n",
    "    result.append({\n",
    "        'column_a': column_a,\n",
    "        'column_b': column_b,\n",
    "        'group': result_ovr_ab['group'],\n",
    "        'reason': result_ovr_ab['reason'],\n",
    "        'onesided_trend': result_ovr_ab['side'] if 'side' in result_ovr_ab else ''\n",
    "    })\n",
    "\n",
    "relation_overview_df = pd.DataFrame(result)\n",
    "relation_overview_df.to_excel(output_path + 'relation_overview_fisher.xlsx', index=False)\n",
    "\n",
    "print(f'significant results: {sum(fisher_significance_count) / len(fisher_significance_count)}')\n",
    "\n",
    "print(f'Done - {datetime.datetime.now()}')\n",
    "\n",
    "relation_overview_df['group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal/Numeric Pairs\n",
    "\n",
    "Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_bool_vars = get_nom_list_variables()\n",
    "num_vars = get_num_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df = get_variables_df()\n",
    "\n",
    "# variables_df = variables_df.copy().dropna(subset=['I8 - does it have a wiki? or does it have a specification? [any]', 'C4 - Commit file bugginess'])\n",
    "# variables_df = variables_df[variables_df['CG'] == True]\n",
    "# variables_df['I8 - does it have a wiki? or does it have a specification? [any]'] = variables_df['I8 - does it have a wiki? or does it have a specification? [any]'].astype(bool).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_based_on_order(list_to_sort) -> list:\n",
    "    '''Sorts the list based on a predefined order list, so that the order is consistent'''\n",
    "\n",
    "    order_list = (['undecided', 'low', 'medium', 'high', 'critical']\n",
    "    + ['core', 'peripheral', 'not involved']\n",
    "    + ['bug', 'feature', 'improvement']\n",
    "    + ['yes', 'no']\n",
    "    + ['true', 'false'])\n",
    "\n",
    "    if all([str(x).lower() in order_list for x in list_to_sort]):\n",
    "        order_index = {value: index for index, value in enumerate(order_list)}\n",
    "        sorted_list = sorted(list_to_sort, key=lambda x: order_index.get(str(x).lower(), float('inf')))\n",
    "    else:\n",
    "        sorted_list = sorted(list_to_sort, key=lambda x: str(x).lower())\n",
    "\n",
    "    return sorted_list\n",
    "\n",
    "\n",
    "def draw_barchart_distribution(variables_ab_df: pd.DataFrame, column_a, ax5: plt.Axes, ax6: plt.Axes):\n",
    "    '''Draws a barchart of the distribution of the nominal variable in the dataset'''\n",
    "    \n",
    "    nominal_labels, _ = get_labels_from_sub_columns(variables_ab_df, column_a)\n",
    "    \n",
    "    variables_ab_df_bg = variables_ab_df[variables_ab_df['CG'] == False]\n",
    "    variables_ab_df_cg = variables_ab_df[variables_ab_df['CG'] == True]\n",
    "\n",
    "    id_count_bg = len(variables_ab_df_bg[BUG_ID].unique())\n",
    "    id_count_cg = len(variables_ab_df_cg[BUG_ID].unique())\n",
    "\n",
    "    labels = nominal_labels\n",
    "    labels = sort_based_on_order(labels)\n",
    "    sub_columns = [get_sub_column_name(column_a, label) for label in labels]\n",
    "\n",
    "    label_counts_bg = variables_ab_df_bg[sub_columns].sum().values\n",
    "    label_counts_cg = variables_ab_df_cg[sub_columns].sum().values\n",
    "\n",
    "    def draw_barchart(ax: plt.Axes, counts_a: list[int], counts_b: list[int], labels, div_a = 1, div_b = 1):\n",
    "        width = 0.35\n",
    "        x = np.arange(len(labels))\n",
    "\n",
    "        div_a = max(div_a, 1)\n",
    "        div_b = max(div_b, 1)\n",
    "\n",
    "        percentages_a = [count/div_a for count in counts_a]\n",
    "        percentages_b = [count/div_b for count in counts_b]\n",
    "\n",
    "        bars_1 = ax.bar(x - width/2, percentages_a, width, label='BG')\n",
    "        bars_2 = ax.bar(x + width/2, percentages_b, width, label='CG')\n",
    "\n",
    "        ax.bar_label(bars_1, labels=[f'{round(count/div_a, 2)}% ({count})' for count in counts_a] if div_a > 1 else [f'{count}' for count in counts_a])\n",
    "        ax.bar_label(bars_2, labels=[f'{round(count/div_b, 2)}% ({count})' for count in counts_b] if div_b > 1 else [f'{count}' for count in counts_b])\n",
    "\n",
    "        for bar, count in list(zip(bars_1, counts_a)) + list(zip(bars_2, counts_b)):\n",
    "            if count < FREQ_THRESHOLD:\n",
    "                bar.set_facecolor('gray')\n",
    "                bar.set_alpha(0.2)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_xlabel(column_a)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    draw_barchart(ax5, label_counts_bg, label_counts_cg, labels, id_count_bg, id_count_cg)\n",
    "    draw_barchart(ax6, [id_count_bg], [id_count_cg], ['ID count'])\n",
    "\n",
    "    return f'Abs. count: {id_count_bg} (BG); {id_count_cg} (CG)'\n",
    "\n",
    "\n",
    "def draw_num_distribution_with_boxplot(variables_df: pd.DataFrame, column_a, column_b, ax1: plt.Axes, ax2: plt.Axes, cg=None) -> tuple[bool, str]:\n",
    "    ''' Draw a histogram and boxplot of the variable in the dataset\n",
    "\n",
    "        Args:\n",
    "            variable_df (pd.DataFrame): The dataset\n",
    "            column_a (str): The name of the nominal/boolean variable\n",
    "            column_b (str): The name of the numeric variable\n",
    "            cg (bool): Whether to filter the data by CG or BIC\n",
    "    '''\n",
    "    labels, _ = get_labels_from_sub_columns(variables_df, column_a)\n",
    "    labels = sort_based_on_order(labels)\n",
    "\n",
    "    if cg is True:\n",
    "        variables_df = variables_df[variables_df['CG'] == True]\n",
    "    elif cg is False:\n",
    "        variables_df = variables_df[variables_df['CG'] == False]\n",
    "\n",
    "    title = f'{column_b.split(\" - \")[0]}/{column_a.split(\" - \")[0]}'\n",
    "    plot_data = []\n",
    "    plot_data_no_inverse = []\n",
    "    plot_labels = []\n",
    "    plot_labels_count = []\n",
    "\n",
    "    for label in labels:\n",
    "        plot_labels.append(label)\n",
    "        sub_column_name = get_sub_column_name(column_a, label)\n",
    "        variables_column_b = variables_df[variables_df[sub_column_name] == True][column_b].dropna()\n",
    "        plot_labels_count.append(f'{label} ({variables_column_b.count()})')\n",
    "        plot_data.append(variables_column_b)\n",
    "        plot_data_no_inverse.append(variables_column_b)\n",
    "\n",
    "        if len(labels) > 2:\n",
    "            plot_labels.append(get_inv_name(label))\n",
    "            variables_column_b_not = variables_df[variables_df[sub_column_name] == False][column_b].dropna()\n",
    "            plot_labels_count.append(f'{get_inv_name(label)} ({variables_column_b_not.count()})')\n",
    "            plot_data.append(variables_column_b_not)\n",
    "        \n",
    "    def draw(ax: plt.Axes, fliers, labels):\n",
    "        # ax.set_title(f'Distribution of {title} ({\"CG\" if cg else \"BG\"})' + (' no outliers' if not fliers else ''))\n",
    "        ax.set_ylabel(column_b)\n",
    "        ax.set_xlabel(column_a)\n",
    "\n",
    "        boxplots = ax.boxplot(plot_data, showfliers=fliers, patch_artist=True)\n",
    "\n",
    "        # if fliers == False and cg == False:\n",
    "        #     ax.set_ylim(None, 45)\n",
    "        \n",
    "        for boxplot, data, label in zip(boxplots['boxes'], plot_data, plot_labels):\n",
    "            if label.endswith(f'[{inverse_str}]'):\n",
    "                boxplot.set_facecolor('skyblue')\n",
    "            if len(data) < FREQ_THRESHOLD:\n",
    "                boxplot.set_facecolor('gray')\n",
    "                boxplot.set_alpha(0.2)\n",
    "        \n",
    "        ax.set_xticks(range(1, len(labels) + 1))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "    draw(ax1, False, plot_labels_count)\n",
    "    if ax2 is not None:\n",
    "        draw(ax2, True, plot_labels_count)\n",
    "\n",
    "    kruskal_data = [data for data in plot_data_no_inverse if len(data) >= FREQ_THRESHOLD]\n",
    "    if len(kruskal_data) > 1:\n",
    "        kr, p = kruskal(*kruskal_data)\n",
    "\n",
    "    return True, ('CG' if cg else 'BG') + f': Kruskal-Wallis - Significant difference {p <= 0.05}' if len(kruskal_data) > 1 else 'Not enough data for Kruskal-Wallis'\n",
    "    # return True, ('CG' if cg else 'BG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing R4 - Reviewer types: : 1it [00:00,  7.40it/s, I7 - # Introducing issue commits [sum]]\n",
      "Saving figures: 100%|██████████| 1/1 [00:00<00:00, 22.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 done! - 2025-04-11 09:32:13.679610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relation_overview = {}\n",
    "\n",
    "variables_a_done = set()\n",
    "save_threads = []\n",
    "\n",
    "TEST_SPECIFIC_PAIR = (None, None)\n",
    "# TEST_SPECIFIC_PAIR = ('R4', 'I7')\n",
    "\n",
    "p_bar = tqdm.tqdm(create_iterator(nom_bool_vars, num_vars, TEST_SPECIFIC_PAIR))\n",
    "for column_a, column_b, scale_a, scale_b, aspect, _ in p_bar:\n",
    "    p_bar.set_description(f'Processing {column_a}')\n",
    "    p_bar.set_postfix_str(f'{column_b}')\n",
    "\n",
    "    variables_ab_df = variables_main_df.copy()\n",
    "    variables_ab_df = transform_columns(variables_ab_df, [column_a, column_b], [scale_a, scale_b])\n",
    "\n",
    "    nominal_labels, sub_columns_a = get_labels_from_sub_columns(variables_ab_df, column_a)\n",
    "    \n",
    "    hints = []\n",
    "\n",
    "    filename = [column_a.split(' - ')[0], column_b.split(' - ')[0]]\n",
    "    if aspect is not None:\n",
    "        filename.append(aspect.name)\n",
    "    file_path = f'{figure_output_path}boxplots/{\"_\".join(filename)}_histogram_boxplot.pdf'\n",
    "\n",
    "    variables_ab_df_bg = variables_ab_df[variables_ab_df['CG'] == False]\n",
    "    variables_ab_df_cg = variables_ab_df[variables_ab_df['CG'] == True]\n",
    "\n",
    "    if len(variables_ab_df_bg) == 0 and len(variables_ab_df_cg) == 0:\n",
    "        relation_overview[f'{column_a} {column_b}'] = set_error(column_a, column_b, 'No data for both groups')\n",
    "        continue\n",
    "\n",
    "    valid_labels_bg, semi_valid_labels_bg = get_valid_labels(variables_ab_df_bg, sub_columns_a)\n",
    "    valid_labels_cg, semi_valid_labels_cg = get_valid_labels(variables_ab_df_cg, sub_columns_a)\n",
    "\n",
    "    if len(semi_valid_labels_bg) == 0 and len(semi_valid_labels_cg) == 0:\n",
    "        relation_overview[f'{column_a} {column_b}'] = set_error(column_a, column_b, 'No frequent labels for both groups')\n",
    "        continue\n",
    "\n",
    "    \n",
    "    if len(semi_valid_labels_bg) == 0 and len(semi_valid_labels_cg) == 0:\n",
    "        hints.append('BG and CG: no frequent labels')\n",
    "    elif len(valid_labels_bg & valid_labels_cg) == 0:\n",
    "        hints.append('BG and CG: no common fully valid labels')\n",
    "    if len(semi_valid_labels_bg) == 0:\n",
    "        hints.append('BG: no frequent labels')\n",
    "    elif len(valid_labels_bg) == 0:\n",
    "        hints.append('BG: no fully valid labels')\n",
    "    if len(semi_valid_labels_cg) == 0:\n",
    "        hints.append('CG: no frequent labels')\n",
    "    elif len(valid_labels_cg) == 0:\n",
    "        hints.append('CG: no fully valid labels')\n",
    "    if len(valid_labels_bg) == 0 and len(valid_labels_cg) == 0:\n",
    "        relation_overview[f'{column_a} {column_b}'] = set_error(column_a, column_b, 'No fully valid labels for both groups. Cannot compare.')\n",
    "        continue\n",
    "\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    width = max(len(nominal_labels) * 2, 14)\n",
    "    # fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(width, 22))\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    ax3, ax4, ax5, ax6 = None, None, None, None\n",
    "\n",
    "\n",
    "    drew_cg, hint_cg = draw_num_distribution_with_boxplot(variables_ab_df, column_a, column_b, ax2, ax4, cg=True) if len(semi_valid_labels_cg) else (None, None)\n",
    "    drew_bug, hint_bg = draw_num_distribution_with_boxplot(variables_ab_df, column_a, column_b, ax1, ax3, cg=False) if len(semi_valid_labels_bg) else (None, None)\n",
    "\n",
    "    hints.append(hint_bg)\n",
    "    hints.append(hint_cg)\n",
    "    \n",
    "    if (drew_bug or drew_cg) and None not in [ax5, ax6]:\n",
    "        # hints.append(f'Y-axis mean: {round(variables_ab_df_bg[column_b].mean(), 2)} (BG); {round(variables_ab_df_cg[column_b].mean())} (CG)')    \n",
    "        hint = draw_barchart_distribution(variables_ab_df, column_a, ax5, ax6)\n",
    "        hints.append(hint)\n",
    "\n",
    "    if os.path.isdir(f'{figure_output_path}boxplots') is False:\n",
    "        os.mkdir(f'{figure_output_path}boxplots')\n",
    "\n",
    "    if drew_bug or drew_cg:\n",
    "        plt.tight_layout()\n",
    "        thread = Thread(target=save_fig, args=(fig, file_path), daemon=True)\n",
    "        thread.start()\n",
    "        save_threads.append(thread)\n",
    "\n",
    "    relation_overview[f'{column_a} {column_b}'] = {\n",
    "        'column_a': column_a,\n",
    "        'column_b': column_b,\n",
    "        'group': None,\n",
    "        'reason': None,\n",
    "        'hint': '\\n'.join([hint for hint in hints if hint is not None]),\n",
    "        'hyperlink': f'=HYPERLINK(J1 & \"{file_path.replace(output_path, \"\")}\", \"CLICK\")' if drew_bug or drew_cg else None\n",
    "    }\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "for thread in tqdm.tqdm(save_threads, desc='Saving figures'):\n",
    "    thread.join()\n",
    "\n",
    "print(f'All {len(save_threads)} done! - {datetime.datetime.now()}')\n",
    "\n",
    "relation_overview_pd = pd.DataFrame(relation_overview.values())\n",
    "relation_overview_pd.to_excel(output_path + 'relation_overview_boxplots.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Pairs\n",
    "\n",
    "Spearman's rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = get_num_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_476837/515221555.py:38: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho_bg, p_value_bg = spearmanr(variables_ab_df_bg[column_a], variables_ab_df_bg[column_b])\n",
      "100%|██████████| 15/15 [00:00<00:00, 117.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significant results: 0.17699115044247787 out of 113\n",
      "Done - 2025-04-10 16:40:55.620176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "group\n",
       "31     51\n",
       "1      20\n",
       "E      18\n",
       "32     10\n",
       "2.1     4\n",
       "3.3     2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_overview = {}\n",
    "spearman_significance_count = []\n",
    "\n",
    "variables_df = get_variables_df()\n",
    "\n",
    "variables_a_done = set()\n",
    "\n",
    "save_threads = []\n",
    "\n",
    "TEST_SPECIFIC_PAIR = (None, None)\n",
    "# TEST_SPECIFIC_PAIR = ('I5', 'ML5')\n",
    "\n",
    "data_a = num_vars[num_vars['ID'] == TEST_SPECIFIC_PAIR[0]] if TEST_SPECIFIC_PAIR[0] is not None else num_vars\n",
    "for id_a, name_a, scale_a, aggregation_a in tqdm.tqdm(data_a[['ID', 'Name', 'Scale', 'Aggregation']].values):\n",
    "    if is_identifier(name_a):\n",
    "        continue\n",
    "\n",
    "    column_a = f'{id_a} - {name_a}' + (f' [{aggregation_a}]' if pd.notna(aggregation_a) else '')\n",
    "\n",
    "    variables_a_done.add(id_a)\n",
    "    \n",
    "    data_b = num_vars[num_vars['ID'] == TEST_SPECIFIC_PAIR[1]] if TEST_SPECIFIC_PAIR[1] is not None else num_vars\n",
    "    for id_b, name_b, scale_b, aggregation_b in data_b[['ID', 'Name', 'Scale', 'Aggregation']].values:\n",
    "        if is_identifier(name_b):\n",
    "            continue\n",
    "\n",
    "        if id_b in variables_a_done:\n",
    "            continue\n",
    "\n",
    "        column_b = f'{id_b} - {name_b}' + (f' [{aggregation_b}]' if pd.notna(aggregation_b) else '')\n",
    "\n",
    "        variables_ab_df = variables_df.copy()\n",
    "        variables_ab_df = transform_columns(variables_ab_df, [column_a, column_b], [scale_a, scale_b])\n",
    "\n",
    "        variables_ab_df_bg = variables_ab_df[variables_ab_df['CG'] == False]\n",
    "        variables_ab_df_cg = variables_ab_df[variables_ab_df['CG'] == True]\n",
    "\n",
    "        rho_bg, p_value_bg = spearmanr(variables_ab_df_bg[column_a], variables_ab_df_bg[column_b])\n",
    "        rho_cg, p_value_cg = spearmanr(variables_ab_df_cg[column_a], variables_ab_df_cg[column_b])\n",
    "\n",
    "        bg_significant = None\n",
    "        cg_significant = None\n",
    "        if not math.isnan(p_value_bg):\n",
    "            bg_significant = p_value_bg <= 0.05\n",
    "            spearman_significance_count.append(bg_significant)\n",
    "        if not math.isnan(p_value_cg):\n",
    "            cg_significant = p_value_cg <= 0.05\n",
    "            spearman_significance_count.append(cg_significant)\n",
    "\n",
    "\n",
    "        if bg_significant and abs(rho_bg) < 0.2:\n",
    "            print(\"WARNING: BG significant but low correlation for \", column_a, column_b)\n",
    "        if cg_significant and abs(rho_cg) < 0.2:\n",
    "            print(\"WARNING: CG significant but low correlation for \", column_a, column_b)\n",
    "\n",
    "        if bg_significant is None and cg_significant is None:\n",
    "            relation_overview[f'{column_a} {column_b}'] = set_error(column_a, column_b, 'BG and CG: Not enough data')\n",
    "            continue\n",
    "\n",
    "        relation_different = abs(rho_bg - rho_cg) > 0.2    \n",
    "\n",
    "        group = get_group(bg_significant, cg_significant, relation_different)\n",
    "\n",
    "        relation_overview[f'{column_a} {column_b}'] = {\n",
    "            'column_a': column_a,\n",
    "            'column_b': column_b,\n",
    "            'group': group,\n",
    "            'reason': f'Spearman correlation: BG: {round(rho_bg, 2)} (p-value: {p_value_bg}), CG: {round(rho_cg, 2)} (p-value: {p_value_cg})'\n",
    "        }\n",
    "\n",
    "print(f'significant results: {sum(spearman_significance_count) / len(spearman_significance_count)} out of {len(spearman_significance_count)}')\n",
    "\n",
    "print(f'Done - {datetime.datetime.now()}')\n",
    "\n",
    "relation_overview_pd = pd.DataFrame(relation_overview.values())\n",
    "relation_overview_pd.to_excel(output_path + 'relation_overview_spearmanr.xlsx', index=False)\n",
    "\n",
    "relation_overview_pd['group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 138\n",
      "significant results: [0.16733068]\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in spearman_significance_count if i is not None]), len(fisher_significance_count))\n",
    "\n",
    "significance_count = spearman_significance_count + fisher_significance_count\n",
    "\n",
    "print(f'significant results: {sum(significance_count) / len(significance_count)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
