bug_id,archive_id,id,message
NOVA 1294939 - CG,msg11310,<52A78D60.204@linux.vnet.ibm.com>,"[openstack-dev] [Nova][Cells] compute api and objects

--



On Monday, December 09, 2013 4:58:31 PM, Sam Morrison wrote:
> Hi,
>
> I?m trying to fix up some cells issues related to objects. Do all compute api methods take objects now?
> cells is still sending DB objects for most methods (except start and stop) and I know there are more than that.
>
> Eg. I know lock/unlock, shelve/unshelve take objects, I assume there are others if not all methods now?
>
> Cheers,
> Sam
>
>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

I don't know the answer about cells, but posting a few bugs you've 
opened on the topic:

https://bugs.launchpad.net/nova/+bug/1251043
https://bugs.launchpad.net/nova/+bug/1257168

As for ""Do all compute api methods take objects now?"", I believe the 
answer is 'no'.  There are still some objects blueprints in the works.  
Here is a big one:

https://blueprints.launchpad.net/nova/+spec/compute-manager-objects

--

Thanks,

Matt Riedemann


"
NOVA 1314677 - CG,msg02607,<143319292.26341739.1375750900977.JavaMail.root@redhat.com>,"[openstack-dev] instances fail to boot on el6 (glance schema error
	issue)

--

As of an hour ago the el6 (Centos) builds in SmokeStack all started failing. I've documented the initial issue I'm seeing in this ticket:

 https://bugs.launchpad.net/nova/+bug/1208656

The issue seems to be that we now hit a SchemaError which bubbles up from glanceclient when the new direct download plugin code runs. This only seems to happen on distributions using python 2.6 as I'm not seeing the same thing on Fedora.

This stack trace also highlights the fact that the Glance v2 API now seems to be a requirement for Nova... and I'm not sure this is a good thing considering we still use the v1 API for many things as well. Ideally we'd have all Nova -> Glance communication use a single version of the Glance API (either v1 or v2... not both) right?

----

Sorry I didn't catch this one sooner. We only recently enabled Centos testing again (due to some resource limitations). Plus I just got back from vacation. :)

Dan

"
NOVA 1314677 - CG,msg02652,<1456368584.27125203.1375799345778.JavaMail.root@redhat.com>,"[openstack-dev] instances fail to boot on el6 (glance schema
 error	issue)

--

Okay. The quick fix is to remove the extra Glance V2 call when CONF.allowed_direct_url_schemes is disabled.

 https://review.openstack.org/#/c/40426/1

This effectively avoids calling the Glance V2 API on python 2.6 (thus avoiding the schema validation issue).

The real issue here is still unresolved however and it looks like we still have some work to do to get all the fancy new Glance V2 stuff fully working on python 2.6 distros (RHEL/Centos, etc).

Dan

----- Original Message -----
> From: ""Dan Prince"" <dprince at redhat.com>
> To: ""OpenStack Development Mailing List"" <openstack-dev at lists.openstack.org>
> Sent: Monday, August 5, 2013 9:01:40 PM
> Subject: [openstack-dev] instances fail to boot on el6 (glance schema error	issue)
> 
> As of an hour ago the el6 (Centos) builds in SmokeStack all started failing.
> I've documented the initial issue I'm seeing in this ticket:
> 
>  https://bugs.launchpad.net/nova/+bug/1208656
> 
> The issue seems to be that we now hit a SchemaError which bubbles up from
> glanceclient when the new direct download plugin code runs. This only seems
> to happen on distributions using python 2.6 as I'm not seeing the same thing
> on Fedora.
> 
> This stack trace also highlights the fact that the Glance v2 API now seems to
> be a requirement for Nova... and I'm not sure this is a good thing
> considering we still use the v1 API for many things as well. Ideally we'd
> have all Nova -> Glance communication use a single version of the Glance API
> (either v1 or v2... not both) right?
> 
> Dan

"
NOVA 1314677 - CG,msg02954,<OF26FE6722.B71E5577-ON86257BC3.000B26BE-86257BC3.000B4F55@us.ibm.com>,"[openstack-dev] instances fail to boot on el6 (glance schema
	error	issue)

--

Dan, I ran into problems with the glance v2 schema tests in tempest on 
RHEL 6.4 (python 2.6) until I updated jsonschema (1.3.0) and warlock 
(1.0.1).  Could that be related to your issue?

This was the related bug: https://bugs.launchpad.net/glance/+bug/1202391 



Thanks,

MATT RIEDEMANN
Advisory Software Engineer
Cloud Solutions and OpenStack Development

Phone: 1-507-253-7622 | Mobile: 1-507-990-1889
E-mail: mriedem at us.ibm.com


3605 Hwy 52 N
Rochester, MN 55901-1407
United States




From:   Dan Prince <dprince at redhat.com>
To:     OpenStack Development Mailing List 
<openstack-dev at lists.openstack.org>, 
Date:   08/06/2013 09:45 AM
Subject:        Re: [openstack-dev] instances fail to boot on el6 (glance 
schema error    issue)



Okay. The quick fix is to remove the extra Glance V2 call when 
CONF.allowed_direct_url_schemes is disabled.

 https://review.openstack.org/#/c/40426/1

This effectively avoids calling the Glance V2 API on python 2.6 (thus 
avoiding the schema validation issue).

The real issue here is still unresolved however and it looks like we still 
have some work to do to get all the fancy new Glance V2 stuff fully 
working on python 2.6 distros (RHEL/Centos, etc).

Dan

----- Original Message -----
> From: ""Dan Prince"" <dprince at redhat.com>
> To: ""OpenStack Development Mailing List"" 
<openstack-dev at lists.openstack.org>
> Sent: Monday, August 5, 2013 9:01:40 PM
> Subject: [openstack-dev] instances fail to boot on el6 (glance schema 
error            issue)
> 
> As of an hour ago the el6 (Centos) builds in SmokeStack all started 
failing.
> I've documented the initial issue I'm seeing in this ticket:
> 
>  https://bugs.launchpad.net/nova/+bug/1208656
> 
> The issue seems to be that we now hit a SchemaError which bubbles up 
from
> glanceclient when the new direct download plugin code runs. This only 
seems
> to happen on distributions using python 2.6 as I'm not seeing the same 
thing
> on Fedora.
> 
> This stack trace also highlights the fact that the Glance v2 API now 
seems to
> be a requirement for Nova... and I'm not sure this is a good thing
> considering we still use the v1 API for many things as well. Ideally 
we'd
> have all Nova -> Glance communication use a single version of the Glance 
API
> (either v1 or v2... not both) right?
> 
> Dan

_______________________________________________
OpenStack-dev mailing list
OpenStack-dev at lists.openstack.org
http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20130809/32a9a725/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/gif
Size: 1851 bytes
Desc: not available
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20130809/32a9a725/attachment.gif>
"
NOVA 1343080,msg29884,<53CAD6A2.8040504@gmail.com>,"[openstack-dev] [nova] Some help needed on these darn API sample
	tests

--

Getting very frustrated, hoping someone can walk me back from the cliff.

I am trying to fix this simple bug:

https://bugs.launchpad.net/nova/+bug/1343080

The fix for it is ridiculously simple. It is removing this line:

https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/views/images.py#L121

Removing that one line breaks around 26 tests, 25 of which are related 
to the API sample tests in the nova/tests/integrated/ directory. I 
expected this, and ran the unit test suite after removing the one line 
above specifically to identify the places that would need to be changed.

I went to all of the API sample template files in 
/nova/tests/integrated/api_samples and removed the offending piece of 
the changed URI. A total of 14 API sample template files needed to be 
changed (something quite ridiculous in my opinion):

http://paste.openstack.org/show/87225/

When I re-ran the tests, all of them failed again. Suffice to say, the 
test failure outputs are virtually pointless, as they have a misleading 
error message:

http://paste.openstack.org/show/87226/

It's not that there are extra list items in the template, which is what 
the failure message says. The problem is that the API samples from the 
templates into the /doc/api_samples/ directory were not re-generated 
when the templates changed.

The README in the /nova/tests/integrated/api_samples/ directory has this 
useful advice:

== start ==

Then run the following command:

GENERATE_SAMPLES=True tox -epy27 nova.tests.integrated
Which will create the files on doc/api_samples.

If new tests are added or the .tpl files are changed due to bug fixes, 
the samples must be regenerated so they are in sync with the templates, 
as there is an additional test which reloads the documentation and 
ensures that it's in sync.

Debugging sample generation

If a .tpl is changed, its matching .xml and .json must be removed else 
the samples won't be generated. If an entirely new extension is added, a 
directory for it must be created before its samples will be generated.

== end ==

So, I took the advice of the README and deleted the api samples in 
question from the /docs/api_samples/ folder:

rm doc/api_samples/images-*
rm doc/api_samples/OS-DCF/image-*

And then ran the tox invocation from above:

GENERATE_SAMPLES=True tox -epy27 nova.tests.integrated.test_api_samples

Unfortunately, the above bombed out in testr and produces a bunch of 
garbage, which I am uncertain how to debug:

http://paste.openstack.org/show/87227/

Would someone mind giving me a hand on this? I'd really appreciate it. 
It really should not be this hard to make such a simple fix to the API 
code. :(

Thanks in advance,
-jay

"
NOVA 1343080,msg29885,<CANw6fcGvZj0C74tRTzmrvgh2zz95GbH-Jn+qA=SbcM9=hKf9Yw@mail.gmail.com>,"[openstack-dev] [nova] Some help needed on these darn API
	sample tests

--

Jay,

I got this far - https://review.openstack.org/#/c/108220/ - there are
a handful of failures in ImagesControllerTest left. hope this helps.

-- dims

On Sat, Jul 19, 2014 at 4:35 PM, Jay Pipes <jaypipes at gmail.com> wrote:
> Getting very frustrated, hoping someone can walk me back from the cliff.
>
> I am trying to fix this simple bug:
>
> https://bugs.launchpad.net/nova/+bug/1343080
>
> The fix for it is ridiculously simple. It is removing this line:
>
> https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/views/images.py#L121
>
> Removing that one line breaks around 26 tests, 25 of which are related to
> the API sample tests in the nova/tests/integrated/ directory. I expected
> this, and ran the unit test suite after removing the one line above
> specifically to identify the places that would need to be changed.
>
> I went to all of the API sample template files in
> /nova/tests/integrated/api_samples and removed the offending piece of the
> changed URI. A total of 14 API sample template files needed to be changed
> (something quite ridiculous in my opinion):
>
> http://paste.openstack.org/show/87225/
>
> When I re-ran the tests, all of them failed again. Suffice to say, the test
> failure outputs are virtually pointless, as they have a misleading error
> message:
>
> http://paste.openstack.org/show/87226/
>
> It's not that there are extra list items in the template, which is what the
> failure message says. The problem is that the API samples from the templates
> into the /doc/api_samples/ directory were not re-generated when the
> templates changed.
>
> The README in the /nova/tests/integrated/api_samples/ directory has this
> useful advice:
>
> == start ==
>
> Then run the following command:
>
> GENERATE_SAMPLES=True tox -epy27 nova.tests.integrated
> Which will create the files on doc/api_samples.
>
> If new tests are added or the .tpl files are changed due to bug fixes, the
> samples must be regenerated so they are in sync with the templates, as there
> is an additional test which reloads the documentation and ensures that it's
> in sync.
>
> Debugging sample generation
>
> If a .tpl is changed, its matching .xml and .json must be removed else the
> samples won't be generated. If an entirely new extension is added, a
> directory for it must be created before its samples will be generated.
>
> == end ==
>
> So, I took the advice of the README and deleted the api samples in question
> from the /docs/api_samples/ folder:
>
> rm doc/api_samples/images-*
> rm doc/api_samples/OS-DCF/image-*
>
> And then ran the tox invocation from above:
>
> GENERATE_SAMPLES=True tox -epy27 nova.tests.integrated.test_api_samples
>
> Unfortunately, the above bombed out in testr and produces a bunch of
> garbage, which I am uncertain how to debug:
>
> http://paste.openstack.org/show/87227/
>
> Would someone mind giving me a hand on this? I'd really appreciate it. It
> really should not be this hard to make such a simple fix to the API code. :(
>
> Thanks in advance,
> -jay
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev



-- 
Davanum Srinivas :: http://davanum.wordpress.com

"
NOVA 1370177,msg53958,<CAD5gMQ5Ot5OpSmdXLkzwToOZTTwP7__FTuAAVtSpUSs=EpG=aA@mail.gmail.com>,"[openstack-dev] [nova] Availability of device names for
 operations with volumes and BDM and other features.

--

Nicola, i would add some words to Alexandre repsonse.

We (standalone ec2api project guys) have filed some bugs (the main is [1]),
but we don't know how to fix them since the way Nova's device names are
moved on is unclear for us. Neither BP, nor wiki you've mentioned above
don't explain what was happened with device names in images.
Other bug which we filed by results of bdm v2 implementation [2] was
resolved, but the fix returns only two devices (even if more than two
volumes are defined in the image) instead of to write device names to the
image and to return full bdm.

I hope you will clarify this question (Alexandre referred to the patch with
explicit elimination of device names for images).

Also you mentioned that we can still use bdm v1. We do it now for instance
launch, but we would like to switch to v2 to use new features like blank
volumes which are provided by AWS as well. However v2 based launch has a
suspicious feature which i asked about in ML [3], but no one answered me.
It would be great if you clarify that question too.

[1] https://bugs.launchpad.net/nova/+bug/1370177
[2] https://bugs.launchpad.net/nova/+bug/1370265
[3] http://lists.openstack.org/pipermail/openstack-dev/2015-May/063769.html


On Wed, May 27, 2015 at 8:55 PM, Nikola ?ipanov <ndipanov at redhat.com> wrote:

> On 05/27/2015 09:47 AM, Alexandre Levine wrote:
> > Hi all,
> >
> > I'd like to bring up this matter again, although it was at some extent
> > discussed during the recent summit.
> >
> > The problem arises from the fact that the functionality exposing device
> > names for usage through public APIs is deteriorating in nova. It's being
> > deliberately removed because as I understand, it doesn't universally and
> > consistently work in all of the backends. It happens  since IceHouse and
> > introduction of bdm v2. The following very recent review is one of the
> > ongoing efforts in this direction:
> > https://review.openstack.org/#/c/185438/
> >
>
> I've abandoned the change as it is clear we need to discuss how to go
> about this some more.
>
> But first let me try to give a bit more detailed explanation and
> background to what the deal is with device names. Supplying device names
> that will be honoured by the guests is really only possible with Xen PV
> guests (meaning the guest needs to be running PV-enabled kernel and
> drivers).
>
> Back in Havana, when we were working on [1] (see [2] for more details)
> the basic idea was that we will still accept device names because
> removing them from the public API is not likely to happen (mostly
> because of the EC2 compatibility), but in case of libvirt driver, we
> will treat them as hints only, and provide our own (by mostly
> replicating the logic libvirt uses to order devices [3]). We also
> allowed for device names to not be specified by the user as this is
> really what anyone not using the EC2 API should be doing (users using
> the EC2 API do however need to be aware the fact that i may not be
> honoured).
>
> [1]
> https://blueprints.launchpad.net/nova/+spec/improve-block-device-handling
> [2] https://wiki.openstack.org/wiki/BlockDeviceConfig
> [3]
>
> https://github.com/openstack/nova/blob/master/nova/virt/libvirt/blockinfo.py
>
> > The reason for my concern is that EC2 API have some important cases
> > relying on this information (some of them have no workarounds). Namely:
> > 1. Change of parameters set by image for instance booting.
> > 2. Showing instance's devices information by euca2ools.
> > 3. Providing additional volumes for instance booting
> > 4. Attaching volume
> > etc...
> >
>
> So based on the above - it seems to me that you think we are removing
> the information about device names completely. That's not the case -
> currently it is simply not mandatory for the Nova boot API call (it was
> never mandatory for volume attach afaict) - you can still pass it in,
> though libvirt may not honour it. It will still be tracked by the Nova
> DB and available for users to refer to.
>
> > Related to device names and additional related features we have troubles
> > with now:
> > 1. All device name related features
>
> As I said - they are not removed, in addition, you can still completely
> disregard the BDMv2 syntax as Nova should transparently handle old-style
> syntax when passed in (actually since BDM info is stored with images
> when snapshotting and it may have been v1 syntax - it is likely that we
> will never remove this support). If you are seeing some bugs related to
> this - please report them.
>
> > 2. Modification of deleteOnTermination flag
>
> I don't have enough details on this but if some behaviour has changed
> when using the old syntax - it is likely a bug so please report it.
>
> > 3. Modification of parameters for instance booting
>
> Again - I am not sure what this is related to exactly - but none of the
> parameters have changed really (only new ones were added). It would be
> good to get more information on this (preferably a bug report).
>
> > 4. deleteOnTermination and size of volume aren't stored into instance
> > snapshots now.
> >
>
> This does sound like a bug - and hopefully an easy to fix one.
>
> > Discussions during the summit on the matter were complicated because
> > nobody present really understood in details why and what is happening
> > with this functionality in nova. It was decided though, that overall
> > direction would be to add necessary features or restore them unless
> > there is something really showstopping:
> > https://etherpad.openstack.org/p/YVR-nova-contributor-meetup
> >
> > As I understand, Nikola Depanov is the one working on the matter for
> > some time obviously is the best person who can help to resolve the
> > situation. Nikola, if possible, could you help with it and clarify the
> > issue.
> >
> > My suggestion, based on my limited knowledge at the moment, still is to
> > restore back or add all of the necessary APIs and provide tickets or
> > known issues for the cases where the functionality is suffering from the
> > backend limitations.
> >
> > Please let me know what you think.
> >
>
> As explained above - nothing was intentionally removed, and if something
> broke - it's a bug that we should fix, so I urge the team behind the EC2
> API on stackforge to report those, and I will try to at least look into
> them, if not fix them. We might want to have a tag for EC2 related bugs
> in LP (I seem to remember there being such a thing before).
>
> Device names though are not something we can easily resolve without
> having the users of the EC2 API be aware that they may be talking to a
> Nova deployment, however I am unsure how much of a problem this is in
> practice, as I believe most users would have to be anyway.
>
> Hope this lifts some of the mystery around the topic - and I look
> forward to helping the EC2 efforts - but we really need to make sure we
> have the issues written down first :)
>
> N.
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150529/b9f932f9/attachment.html>
"
NOVA 1370177,msg54057,<5568CCEC.6030001@redhat.com>,"[openstack-dev] [nova] Availability of device names for
 operations with volumes and BDM and other features.

--

On 05/29/2015 12:55 AM, Feodor Tersin wrote:
> Nicola, i would add some words to Alexandre repsonse.
> 
> We (standalone ec2api project guys) have filed some bugs (the main is
> [1]), but we don't know how to fix them since the way Nova's device
> names are moved on is unclear for us. Neither BP, nor wiki you've
> mentioned above don't explain what was happened with device names in images.
> Other bug which we filed by results of bdm v2 implementation [2] was
> resolved, but the fix returns only two devices (even if more than two
> volumes are defined in the image) instead of to write device names to
> the image and to return full bdm.
> 
> I hope you will clarify this question (Alexandre referred to the patch
> with explicit elimination of device names for images).
> 
> Also you mentioned that we can still use bdm v1. We do it now for
> instance launch, but we would like to switch to v2 to use new features
> like blank volumes which are provided by AWS as well. However v2 based
> launch has a suspicious feature which i asked about in ML [3], but no
> one answered me. It would be great if you clarify that question too.
> 
> [1] https://bugs.launchpad.net/nova/+bug/1370177
> [2] https://bugs.launchpad.net/nova/+bug/1370265
> [3] http://lists.openstack.org/pipermail/openstack-dev/2015-May/063769.html
> 

Hey Feodor and Alexandre - Thanks for the detailed information!

I have already commented on some of the bugs above and provided a small
patch that I think fixes one bit of it. As described on the bug - device
names might be a bit trickier, but I hope to have something posted next
week.

Help with testing (while patches are in review) would be hugely appreciated!

On 05/28/2015 02:24 PM, Alexandre Levine wrote:
> 1. RunInstance. Change parameters of devices during instance booting
> from image. In Grizzly it worked so we could specify changed BDM in
> parameters, it overwrote in nova DB the one coming from image and then
> started the instance with new parameters. The only key for addressing
> devices in this use case is the very device name. And now we don't have
> it for the volumes in BDM coming from the image, because nova stopped
> putting this information into the image.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html
>
> 2. Devices names for Xen-backed instances to work fully. We should be
> able to specify required device names during initial instance creation,
> they should be stored into an image when the instance is shapshotted, we
> can fetch info and change parameters of such volume during subsequent
> operations, and the device names inside the instance should be named
> exactly.
>
> 3. DescribeInstances and DescribeInstanceAttributes to return BDM with
> device names ideally corresponding to actual device naming in instance.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html
>
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstanceAttribute.html
>
>
> 4. DescribeImages and DescribeImageAttributes to return BDM with device
> names ideally corresponding to the ones in instance before snapshotting.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeImages.html
>
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeImageAttribute.html
>

I think all of the above is pretty much covered by
https://bugs.launchpad.net/nova/+bug/1370177

>
> 5. AttachVolume with the specified device name.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AttachVolume.html
>
> 6. ModifyInstanceAttribute with BDM as parameter.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyInstanceAttribute.html
>
>
> 7. ModifyImageAttribute with BDM as parameter.
>
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyImageAttribute.html

I am not sure about these 3 cases - would it be possible to actually
report bugs for them as I don't think I have enough information this way.

N.

"
NOVA 1370177,msg54311,<556DA3F8.7000304@cloudscaling.com>,"[openstack-dev] [nova] Availability of device names for
 operations with volumes and BDM and other features.

--

Thank you Nikola.

We'll be adding the required tickets and will follow your reviews, 
however the person working primarily on this subject (Feodor Tersin) is 
out for his vacation for a couple of weeks so some of our responses 
might be delayed until then. Still we'll try to do whatever can be done 
without him at the time being.

Best regards,
   Alex Levine


On 5/29/15 11:32 PM, Nikola ?ipanov wrote:
> On 05/29/2015 12:55 AM, Feodor Tersin wrote:
>> Nicola, i would add some words to Alexandre repsonse.
>>
>> We (standalone ec2api project guys) have filed some bugs (the main is
>> [1]), but we don't know how to fix them since the way Nova's device
>> names are moved on is unclear for us. Neither BP, nor wiki you've
>> mentioned above don't explain what was happened with device names in images.
>> Other bug which we filed by results of bdm v2 implementation [2] was
>> resolved, but the fix returns only two devices (even if more than two
>> volumes are defined in the image) instead of to write device names to
>> the image and to return full bdm.
>>
>> I hope you will clarify this question (Alexandre referred to the patch
>> with explicit elimination of device names for images).
>>
>> Also you mentioned that we can still use bdm v1. We do it now for
>> instance launch, but we would like to switch to v2 to use new features
>> like blank volumes which are provided by AWS as well. However v2 based
>> launch has a suspicious feature which i asked about in ML [3], but no
>> one answered me. It would be great if you clarify that question too.
>>
>> [1] https://bugs.launchpad.net/nova/+bug/1370177
>> [2] https://bugs.launchpad.net/nova/+bug/1370265
>> [3] http://lists.openstack.org/pipermail/openstack-dev/2015-May/063769.html
>>
> Hey Feodor and Alexandre - Thanks for the detailed information!
>
> I have already commented on some of the bugs above and provided a small
> patch that I think fixes one bit of it. As described on the bug - device
> names might be a bit trickier, but I hope to have something posted next
> week.
>
> Help with testing (while patches are in review) would be hugely appreciated!
>
> On 05/28/2015 02:24 PM, Alexandre Levine wrote:
>> 1. RunInstance. Change parameters of devices during instance booting
>> from image. In Grizzly it worked so we could specify changed BDM in
>> parameters, it overwrote in nova DB the one coming from image and then
>> started the instance with new parameters. The only key for addressing
>> devices in this use case is the very device name. And now we don't have
>> it for the volumes in BDM coming from the image, because nova stopped
>> putting this information into the image.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html
>> 2. Devices names for Xen-backed instances to work fully. We should be
>> able to specify required device names during initial instance creation,
>> they should be stored into an image when the instance is shapshotted, we
>> can fetch info and change parameters of such volume during subsequent
>> operations, and the device names inside the instance should be named
>> exactly.
>>
>> 3. DescribeInstances and DescribeInstanceAttributes to return BDM with
>> device names ideally corresponding to actual device naming in instance.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstanceAttribute.html
>>
>> 4. DescribeImages and DescribeImageAttributes to return BDM with device
>> names ideally corresponding to the ones in instance before snapshotting.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeImages.html
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeImageAttribute.html
> I think all of the above is pretty much covered by
> https://bugs.launchpad.net/nova/+bug/1370177
>
>> 5. AttachVolume with the specified device name.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AttachVolume.html
>> 6. ModifyInstanceAttribute with BDM as parameter.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyInstanceAttribute.html
>>
>> 7. ModifyImageAttribute with BDM as parameter.
>>
> http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyImageAttribute.html
>
> I am not sure about these 3 cases - would it be possible to actually
> report bugs for them as I don't think I have enough information this way.
>
> N.
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1370590,msg61340,<55D536AB.6080403@linux.vnet.ibm.com>,"[openstack-dev] [nova][qa] libvirt + LXC CI - where's the beef?

--

After spending a few hours on 
https://bugs.launchpad.net/nova/+bug/1370590 I'm annoyed by the fact we 
don't yet have a CI system for testing libvirt + LXC.

At the Juno midcycle in Portland I thought I remember some guy(s) from 
Rackspace talking about getting a CI job running, whatever happened with 
that?

It seems like we should be able to get this going using community infra, 
right?  Just need some warm bodies to get the parts together and figure 
out which Tempest tests can't be run with that setup - but we have the 
hypervisor support matrix to help us out as a starter.

It also seems unfair to require third party CI for libvirt + parallels 
(virtuozzo) but we don't have the same requirement for LXC.

What gives?!

-- 

Thanks,

Matt Riedemann


"
NOVA 1370590,msg61365,<55D5CD8D.5070405@linux.vnet.ibm.com>,"[openstack-dev] [nova][qa] libvirt + LXC CI - where's the beef?

--



On 8/20/2015 5:33 AM, John Garbutt wrote:
> On 20 August 2015 at 03:08, Matt Riedemann <mriedem at linux.vnet.ibm.com> wrote:
>> After spending a few hours on https://bugs.launchpad.net/nova/+bug/1370590
>> I'm annoyed by the fact we don't yet have a CI system for testing libvirt +
>> LXC.
>
> Bit thank you for raising this one.
>
>> At the Juno midcycle in Portland I thought I remember some guy(s) from
>> Rackspace talking about getting a CI job running, whatever happened with
>> that?
>
> Now you mention it, I remember that.
> I haven't heard any news about that, let me poke some people.
>
>> It seems like we should be able to get this going using community infra,
>> right?  Just need some warm bodies to get the parts together and figure out
>> which Tempest tests can't be run with that setup - but we have the
>> hypervisor support matrix to help us out as a starter.
>
> +1
>
>> It also seems unfair to require third party CI for libvirt + parallels
>> (virtuozzo) but we don't have the same requirement for LXC.
>
> The original excuse was that it didn't bring much value, as most of
> the LXC differences were in libvirt.
> But given the recent bugs that have cropped up, that is totally the wrong call.
>
> I think we need to add a log message saying:
> ""LXC support is untested, and will be removed during Mitka if we do
> not get a CI in place"".
>
> Following the rules here:
> https://wiki.openstack.org/wiki/HypervisorSupportMatrix/DeprecationPlan#Specific_Requirements
>
> Does that make sense?

There should at least be a quality warning that it's untested.  I can 
push that up today.

>
> John
>
> PS
> I must to kick off the ""feature classification"" push, so we can get
> discuss that for real at the summit.
>
> Really I am looking for folks to help with that, help monitor what
> bits of the support matrix are actually tested.
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

-- 

Thanks,

Matt Riedemann


"
NOVA 1402728 - CG,msg14433,<CF0457C3.3A9D3%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI passthrough SRIOV

--

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140121/b7105a4f/attachment.html>
"
NOVA 1402728 - CG,msg14469,<9D25E123B44F4A4291F4B5C13DA94E77882F4331@MTLDAG02.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140122/ff2fe465/attachment.html>
"
NOVA 1402728 - CG,msg14497,<CF052AE6.3AC21%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let?s try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140122/8c877ed8/attachment.html>
"
NOVA 1402728 - CG,msg14851,<CF084667.3BFDE%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let?s try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140124/864a0df2/attachment.html>
"
NOVA 1402728 - CG,msg14969,<CF0BD58F.3C6D8%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot ?flavor m1.large ?image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let?s try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/2aede665/attachment.html>
"
NOVA 1402728 - CG,msg14975,<9D25E123B44F4A4291F4B5C13DA94E77882FB7D2@MTLDAG02.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/69dc762a/attachment.html>
"
NOVA 1402728 - CG,msg15022,<CF0C2B85.3CAB3%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV on Jan. 28th

--

Hi Folks,

Check out  1 Agenda on Jan 28th, 2014<https://wiki.openstack.org/wiki/Meetings/Passthrough#Agenda_on_Jan_28th.2C_2014>. Please update if I have missed any thing. Let's finalize who's doing what tomorrow.

I'm thinking to work on the nova SRIOV items, but the live migration may be a stretch for the initial release.

thanks,
Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/f12a10dc/attachment.html>
"
NOVA 1402728 - CG,msg15036,<CAPoubz6PNWQ8cTToosEfsbDVX+oJXf8+rXbkPgELOWMtMzZRag@mail.gmail.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV on Jan.
	28th

--

Live migration for the first release is intended to be covered by macvtap,
in my mind - direct mapped devices have limited support in hypervisors
aiui.  It seemed we had a working theory for that, which we test out and
see if it's going to work.
-- 
Ian.


On 27 January 2014 21:38, Robert Li (baoli) <baoli at cisco.com> wrote:

>  Hi Folks,
>
>  Check out  1 Agenda on Jan 28th, 2014<https://wiki.openstack.org/wiki/Meetings/Passthrough#Agenda_on_Jan_28th.2C_2014>.
> Please update if I have missed any thing. Let's finalize who's doing what
> tomorrow.
>
>  I'm thinking to work on the nova SRIOV items, but the live migration may
> be a stretch for the initial release.
>
>  thanks,
> Robert
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/448b7694/attachment.html>
"
NOVA 1402728 - CG,msg15028,<9D25E123B44F4A4291F4B5C13DA94E77882FBCA6@MTLDAG02.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/c3374c00/attachment.html>
"
NOVA 1402728 - CG,msg15031,<CF0C3482.3CB3F%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Ok, this is something that's going to be added in ml2. I was looking at the bind_port() routine in mech_agent.py. The routine check_segment_for_agent() seems to be performing static check. So we are going to add something like check_vnic_type_for_agent(), I guess? Is the pairing of an agent with the mech driver predetermined? The routine bind_port() just throws warnings, though.

In any case, this is after the fact the scheduler has decided to place the VM onto the host.

Maybe not for now, but we need to consider how to support the hybrid compute nodes. Would an agent be able to support multiple vnic types? Or is it possible to reuse ovs agent, in the same time running another agent to support sriov? Any thoughts?

--Robert

On 1/27/14 4:01 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with ?virtio? vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot ?flavor m1.large ?image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let?s try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/ef419d90/attachment.html>
"
NOVA 1402728 - CG,msg15064,<9D25E123B44F4A4291F4B5C13DA94E77882FD2BB@MTLDAG02.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Robert,
Thank you for raising this issue.
Neutron side support for hybrid compute node is part of the mission  I want to achieve by implementing:
https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type.
I think it should be allowed to run more than one agent on certain node and Mechanism driver will bind the port if:

1.       It supports requested vnic_type

2.       Capable to manage segment for requested port (taking in to account physical network, network type, alive agent,..)
I think at least for now, new agents will be added and not mixed into existing one. But it may be a good idea to come up with Modular Agent.

BR,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 11:16 PM
To: Irena Berezovsky; OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Ok, this is something that's going to be added in ml2. I was looking at the bind_port() routine in mech_agent.py. The routine check_segment_for_agent() seems to be performing static check. So we are going to add something like check_vnic_type_for_agent(), I guess? Is the pairing of an agent with the mech driver predetermined? The routine bind_port() just throws warnings, though.

In any case, this is after the fact the scheduler has decided to place the VM onto the host.

Maybe not for now, but we need to consider how to support the hybrid compute nodes. Would an agent be able to support multiple vnic types? Or is it possible to reuse ovs agent, in the same time running another agent to support sriov? Any thoughts?

--Robert

On 1/27/14 4:01 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140128/13999408/attachment-0001.html>
"
NOVA 1402728 - CG,msg15032,<096DE3A060628644893976A610FDE6F26B89A300@FMSMSX154.amr.corp.intel.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi,

There are two possibilities for the hybrid compute nodes

-          In the first case, a compute node has two NICs,  one SRIOV NIC & the other NIC for the VirtIO

-          In the 2nd case, Compute node has only one SRIOV NIC, where VFs are used for the VMs, either macvtap or direct assignment.  And the PF is used for the uplink to the linux bridge or OVS!!

My question to the team is whether we consider both of these deployments or not?

Thx,

Nrupal

From: Irena Berezovsky [mailto:irenab at mellanox.com]
Sent: Monday, January 27, 2014 1:01 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/7ee6c352/attachment-0001.html>
"
NOVA 1402728 - CG,msg15040,<1390862538.16278.15.camel@yjiang5-linux1>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

On Mon, 2014-01-27 at 21:14 +0000, Jani, Nrupal wrote:
> Hi,
> 
>  
> 
> There are two possibilities for the hybrid compute nodes
> 
> -         In the first case, a compute node has two NICs,  one SRIOV
> NIC & the other NIC for the VirtIO
> 
> -         In the 2nd case, Compute node has only one SRIOV NIC, where
> VFs are used for the VMs, either macvtap or direct assignment.  And
> the PF is used for the uplink to the linux bridge or OVS!!
> 
>  
> 
> My question to the team is whether we consider both of these
> deployments or not?
> 
Nrupal, good question. I assume if a NIC will be used for vNIC type, it
will not be reported by hypervisor as assignable PCI devices since host
will own it and the OVS is setup based on it.

Irena/Ian, please correct me. At least this is assumption in nova PCI
code, I think.

Thanks
--jyh 


"
NOVA 1402728 - CG,msg15065,<9D25E123B44F4A4291F4B5C13DA94E77882FD34D@MTLDAG02.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi Nrupal,
We definitely consider both these cases.
Agree with you that we should aim to support both.

BR,
Irena


From: Jani, Nrupal [mailto:nrupal.jani at intel.com]
Sent: Monday, January 27, 2014 11:17 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi,

There are two possibilities for the hybrid compute nodes

-          In the first case, a compute node has two NICs,  one SRIOV NIC & the other NIC for the VirtIO

-          In the 2nd case, Compute node has only one SRIOV NIC, where VFs are used for the VMs, either macvtap or direct assignment.  And the PF is used for the uplink to the linux bridge or OVS!!

My question to the team is whether we consider both of these deployments or not?

Thx,

Nrupal

From: Irena Berezovsky [mailto:irenab at mellanox.com]
Sent: Monday, January 27, 2014 1:01 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140128/3e3e0f55/attachment.html>
"
NOVA 1402728 - CG,msg15111,<CF0D3C3E.3CF8E%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

Hi,

For the second case, supposed that the PF is properly configured on the host, is it a matter of configuring it as you normally do with a regular ethernet interface to add it to the linux bridge or OVS?

--Robert

On 1/28/14 1:03 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Nrupal,
We definitely consider both these cases.
Agree with you that we should aim to support both.

BR,
Irena


From: Jani, Nrupal [mailto:nrupal.jani at intel.com]
Sent: Monday, January 27, 2014 11:17 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi,

There are two possibilities for the hybrid compute nodes

-          In the first case, a compute node has two NICs,  one SRIOV NIC & the other NIC for the VirtIO

-          In the 2nd case, Compute node has only one SRIOV NIC, where VFs are used for the VMs, either macvtap or direct assignment.  And the PF is used for the uplink to the linux bridge or OVS!!

My question to the team is whether we consider both of these deployments or not?

Thx,

Nrupal

From: Irena Berezovsky [mailto:irenab at mellanox.com]
Sent: Monday, January 27, 2014 1:01 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with ?virtio? vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot ?flavor m1.large ?image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let?s try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for ?nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140128/42f61b18/attachment.html>
"
NOVA 1402728 - CG,msg15116,<096DE3A060628644893976A610FDE6F26B89E97E@ORSMSX107.amr.corp.intel.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

My comments inline below.

Nrupal.

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Tuesday, January 28, 2014 8:32 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi,

For the second case, supposed that the PF is properly configured on the host, is it a matter of configuring it as you normally do with a regular ethernet interface to add it to the linux bridge or OVS?
[nrj] yes. While technically it is possible, we as a team can decide about the final recommendation:)  Given that VFs are going to be used for the high-performance VMs, mixing VMs with virtio & VFs may not be a good option.  Initially we can use PF interface for the management traffic and/or VF configuration!!

--Robert

On 1/28/14 1:03 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Nrupal,
We definitely consider both these cases.
Agree with you that we should aim to support both.

BR,
Irena


From: Jani, Nrupal [mailto:nrupal.jani at intel.com]
Sent: Monday, January 27, 2014 11:17 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi,

There are two possibilities for the hybrid compute nodes

-          In the first case, a compute node has two NICs,  one SRIOV NIC & the other NIC for the VirtIO

-          In the 2nd case, Compute node has only one SRIOV NIC, where VFs are used for the VMs, either macvtap or direct assignment.  And the PF is used for the uplink to the linux bridge or OVS!!

My question to the team is whether we consider both of these deployments or not?

Thx,

Nrupal

From: Irena Berezovsky [mailto:irenab at mellanox.com]
Sent: Monday, January 27, 2014 1:01 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Robert,
Please see inline

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 10:29 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Irena,

I agree on your first comment.

see inline as well.

thanks,
Robert

On 1/27/14 10:54 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
My comments inline

Regards,
Irena
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, January 27, 2014 5:05 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV

Hi Folks,

In today's meeting, we discussed a scheduler issue for SRIOV. The basic requirement is for coexistence of the following compute nodes in a cloud:
      -- SRIOV only compute nodes
      -- non-SRIOV only compute nodes
      -- Compute nodes that can support both SRIOV and non-SRIOV ports. Lack of a proper name, let's call them compute nodes with hybrid NICs support, or simply hybrid compute nodes.

I'm not sure if it's practical in having hybrid compute nodes in a real cloud. But it may be useful in the lab to bench mark the performance differences between SRIOV, non-SRIOV, and coexistence of both.
[IrenaB]
I would like to clarify a bit on the requirements you stated below.
As I see it, the hybrid compute nodes actually can be preferred in the real cloud, since one can define VM with one vNIC attached via SR-IOV virtual function while the other via some vSwitch.
But it definitely make sense to land VM with 'virtio' vNICs only on the non-SRIOV compute node.

Maybe there should be some sort of preference order of suitable nodes in scheduler choice, based on vnic types required for the VM.

In a cloud that supports SRIOV in some of the compute nodes, a request such as:

     nova boot -flavor m1.large -image <image-uuid> --nic net-id=<net-uuid> vm

doesn't require a SRIOV port. However, it's possible for the nova scheduler to place it on a compute node that supports sriov port only. Since neutron plugin runs on the controller, port-create would succeed unless neutron knows the host doesn't support non-sriov port. But connectivity on the node would not be established since no agent is running on that host to establish such connectivity.
[IrenaB] I
Having ML2 plugin as neutron backend, will fail to bind the port, in no agent is running on the Host

[ROBERT] If a host supports SRIOV only, and there is an agent running on the host to support SRIOV, would binding succeed in ML2 plugin for the above 'nova boot' request?
[IrenaB] I think by adding the vnic_typem as we plan, Mechanism Driver will bind the port only if it supports vic_type and there is live agent on this host. So it should work

On a hybrid compute node, can we run multiple neutron L2 agents on a single host? It seems possible.

Irena brought up the idea of using host aggregate. This requires creation of a non-SRIOV host aggregate, and use that in the above 'nova boot' command. It should work.

The patch I had introduced a new constraint in the existing PCI passthrough filter.

The consensus seems to be having a better solution in a later release. And for now, people can either use host aggregate or resort to their own means.

Let's keep the discussion going on this.

Thanks,
Robert





On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Hi Folks,

Based on Thursday's discussion and a chat with Irena, I took the liberty to add a summary and discussion points for SRIOV on Monday and onwards. Check it out https://wiki.openstack.org/wiki/Meetings/Passthrough. Please feel free to update it. Let's try to finalize it next week. The goal is to determine the BPs that need to get approved, and to start coding.

thanks,
Robert


On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com<mailto:baoli at cisco.com>> wrote:

Sounds great! Let's do it on Thursday.

--Robert

On 1/22/14 12:46 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert, all,
I would suggest not to delay the SR-IOV discussion to the next week.
Let's try to cover the SRIOV side and especially the nova-neutron interaction points and interfaces this Thursday.
Once we have the interaction points well defined, we can run parallel patches to cover the full story.

Thanks a lot,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, January 22, 2014 12:02 AM
To: OpenStack Development Mailing List (not for usage questions)
Subject: [openstack-dev] [nova][neutron] PCI passthrough SRIOV

Hi Folks,

As the debate about PCI flavor versus host aggregate goes on, I'd like to move forward with the SRIOV side of things in the same time. I know that tomorrow's IRC will be focusing on the BP review, and it may well continue into Thursday. Therefore, let's start discussing SRIOV side of things on Monday.

Basically, we need to work out the details on:
        -- regardless it's PCI flavor or host aggregate or something else, how to use it to specify a SRIOV port.
        -- new parameters for -nic
        -- new parameters for neutron net-create/neutron port-create
        -- interface between nova and neutron
        -- nova side of work
        -- neutron side of work

We should start coding ASAP.

Thanks,
Robert


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140128/3d620a13/attachment.html>
"
NOVA 1402728 - CG,msg15170,<52E852D0.9090105@windriver.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

On 01/28/2014 10:55 AM, Jani, Nrupal wrote:

> While technically it is possible, we as a team can decide
> about the final recommendationJGiven that VFs are going to be used for
> the high-performance VMs, mixing VMs with virtio & VFs may not be a good
> option.  Initially we can use PF interface for the management traffic
> and/or VF configuration!!

I would expect that it would be fairly common to want to dedicate a VF 
link for high-speed data plane and use a virtio link for control plane 
traffic, health checks, etc.

Chris


"
NOVA 1402728 - CG,msg15030,<1390860256.16278.9.camel@yjiang5-linux1>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

On Mon, 2014-01-27 at 14:58 +0000, Robert Li (baoli) wrote:
> Hi Folks,
> 
> 
> In today's meeting, we discussed a scheduler issue for SRIOV. The
> basic requirement is for coexistence of the following compute nodes in
> a cloud:
>       -- SRIOV only compute nodes
>       -- non-SRIOV only compute nodes
>       -- Compute nodes that can support both SRIOV and non-SRIOV
> ports. Lack of a proper name, let's call them compute nodes with
> hybrid NICs support, or simply hybrid compute nodes.
> 
> 
> I'm not sure if it's practical in having hybrid compute nodes in a
> real cloud. But it may be useful in the lab to bench mark the
> performance differences between SRIOV, non-SRIOV, and coexistence of
> both.
> 
> 
> In a cloud that supports SRIOV in some of the compute nodes, a request
> such as:
> 
> 
>      nova boot ?flavor m1.large ?image <image-uuid> --nic
> net-id=<net-uuid> vm
> 
> 
> doesn't require a SRIOV port. However, it's possible for the nova
> scheduler to place it on a compute node that supports sriov port only.
> Since neutron plugin runs on the controller, port-create would succeed
> unless neutron knows the host doesn't support non-sriov port. But
> connectivity on the node would not be established since no agent is
> running on that host to establish such connectivity. 
> 
> 
> Irena brought up the idea of using host aggregate. This requires
> creation of a non-SRIOV host aggregate, and use that in the above
> 'nova boot' command. It should work.
> 
> 
> The patch I had introduced a new constraint in the existing PCI
> passthrough filter. 
> 
> 
> The consensus seems to be having a better solution in a later release.
> And for now, people can either use host aggregate or resort to their
> own means.
> 
> 
> Let's keep the discussion going on this. 
> 
> 
> Thanks,
> Robert
> 
> 
>  
> 
> 
> 
> 
> 
> 
> On 1/24/14 4:50 PM, ""Robert Li (baoli)"" <baoli at cisco.com> wrote:
> 
> 
>         Hi Folks,
>         
>         
>         Based on Thursday's discussion and a chat with Irena, I took
>         the liberty to add a summary and discussion points for SRIOV
>         on Monday and onwards. Check it
>         out https://wiki.openstack.org/wiki/Meetings/Passthrough.
>         Please feel free to update it. Let's try to finalize it next
>         week. The goal is to determine the BPs that need to get
>         approved, and to start coding. 
>         
>         
>         thanks,
>         Robert
>         
>         
>         
>         
>         On 1/22/14 8:03 AM, ""Robert Li (baoli)"" <baoli at cisco.com>
>         wrote:
>         
>         
>                 Sounds great! Let's do it on Thursday.
>                 
>                 
>                 --Robert
>                 
>                 
>                 On 1/22/14 12:46 AM, ""Irena Berezovsky""
>                 <irenab at mellanox.com> wrote:
>                 
>                 
>                         Hi Robert, all,
>                         
>                         I would suggest not to delay the SR-IOV
>                         discussion to the next week.
>                         
>                         Let?s try to cover the SRIOV side and
>                         especially the nova-neutron interaction points
>                         and interfaces this Thursday.
>                         
>                         Once we have the interaction points well
>                         defined, we can run parallel patches to cover
>                         the full story.
>                         
>                          
>                         
>                         Thanks a lot,
>                         
>                         Irena 
>                         
>                          
>                         
>                         From: Robert Li (baoli)
>                         [mailto:baoli at cisco.com] 
>                         Sent: Wednesday, January 22, 2014 12:02 AM
>                         To: OpenStack Development Mailing List (not
>                         for usage questions)
>                         Subject: [openstack-dev] [nova][neutron] PCI
>                         passthrough SRIOV
>                         
>                         
>                          
>                         
>                         Hi Folks,
>                         
>                         
>                          
>                         
>                         
>                         As the debate about PCI flavor versus host
>                         aggregate goes on, I'd like to move forward
>                         with the SRIOV side of things in the same
>                         time. I know that tomorrow's IRC will be
>                         focusing on the BP review, and it may well
>                         continue into Thursday. Therefore, let's start
>                         discussing SRIOV side of things on Monday. 
>                         
>                         
>                          
>                         
>                         
>                         Basically, we need to work out the details on:
>                         
>                         
>                                 -- regardless it's PCI flavor or host
>                         aggregate or something else, how to use it to
>                         specify a SRIOV port. 
>                         
>                         
>                                 -- new parameters for ?nic
>                         
>                         
>                                 -- new parameters for neutron
>                         net-create/neutron port-create
>                         
>                         
>                                 -- interface between nova and neutron
>                         
>                         
>                                 -- nova side of work
>                         
>                         
>                                 -- neutron side of work
>                         
>                         
>                          
>                         
>                         
>                         We should start coding ASAP.
>                         
>                         
>                          
>                         
>                         
>                         Thanks,
>                         
>                         
>                         Robert
>                         
>                         
>                          
>                         
>                         
>                          
>                         
>                         
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

I noticed that you discussed that prefer non-SRIOV compute node to SRIOV
compute node. I think that should be achieved through weight?

Thanks
--jyh


"
NOVA 1402728 - CG,msg15034,<CAPoubz5BuFJ=-YHk9ynzDEMKwfe78WYdbX2YM=BL2DEQgQbcpw@mail.gmail.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

On 27 January 2014 15:58, Robert Li (baoli) <baoli at cisco.com> wrote:

>  Hi Folks,
>
>  In today's meeting, we discussed a scheduler issue for SRIOV. The basic
> requirement is for coexistence of the following compute nodes in a cloud:
>       -- SRIOV only compute nodes
>
      -- non-SRIOV only compute nodes
>       -- Compute nodes that can support both SRIOV and non-SRIOV ports.
> Lack of a proper name, let's call them compute nodes with hybrid NICs
> support, or simply hybrid compute nodes.
>
>  I'm not sure if it's practical in having hybrid compute nodes in a real
> cloud. But it may be useful in the lab to bench mark the performance
> differences between SRIOV, non-SRIOV, and coexistence of both.
>

I think in fact hybrid nodes would be the common case  - there's nothing
wrong with mixing virtual and physical NICs in a VM and it's been the
general case we've been discussing till now.  VMs that *only* support SRIOV
and have no soft switch sound like a complete outlier to me.  I'm assuming
that passthrough devices are a scarce resource and you wouldn't want to
waste them on a low traffic control connection, so you would always have a
softswitch on the host to take care of such cases.

I believe there *is* a use case here when  you have some, but not all,
machines that have SRIOV devices.  They will also have a softswitch of some
sort and are therefore not only 'SRIOV only' in that sense.  But the point
is that if you have a limited SRIOV resource you may want to preserve these
machines for VMs that have SRIOV requirements, and avoid mapping general
VMs with no SRIOV requirements onto them.

You can expand the problem further and avoid loading up machines with
specific PCI devices of any sort if you have a VM that doesn't need a
device of that sort, which comes down to prioritising your machines at
schedule time based on whether they're a good fit for the VM you intend to
schedule.

In any case, as discussed in the meeting, this is an optimisation and not
something we have to solve in the initial release, because:


>
> Irena brought up the idea of using host aggregate. This requires creation
> of a non-SRIOV host aggregate, and use that in the above 'nova boot'
> command. It should work.
>
>
So, while it's not the greatest solution, there's at least a way of
achieving it right now.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140127/6148de3b/attachment.html>
"
NOVA 1402728 - CG,msg15039,<1390862442.16278.13.camel@yjiang5-linux1>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV

--

On Mon, 2014-01-27 at 22:31 +0100, Ian Wells wrote:
> In any case, as discussed in the meeting, this is an optimisation and
> not something we have to solve in the initial release, because:
>   

+1 for this. We should keep it as future enhancement effort.

--jyh


"
NOVA 1402728 - CG,msg06339,<9D25E123B44F4A4291F4B5C13DA94E7773626E18@MTLDAG02.mtl.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network support

--

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register a summit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131016/9f35d055/attachment.html>
"
NOVA 1402728 - CG,msg07005,<1705659276F6A540A34DBC1919579984102793D2@xmb-rcd-x03.cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register a summit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131025/69e2dcb0/attachment.html>
"
NOVA 1402728 - CG,msg07188,<526F4AA8.4050301@intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
	support

--

On 2013?10?27? 15:48, Irena Berezovsky wrote:
>
> Hi Robert,
>
> Thank you very much for sharing the information regarding your 
> efforts. Can you please share your idea of the end to end flow? How do 
> you suggest  to bind Nova and Neutron?
>
> The blueprints you registered make sense. On Nova side, there is a 
> need to bind between requested virtual network and PCI 
> device/interface to be allocated as vNIC.
>
> On the Neutron side, there is a need to  support networking 
> configuration of the vNIC. Neutron should be able to identify the PCI 
> device/macvtap interface in order to apply configuration. I think it 
> makes sense to provide neutron integration via dedicated Modular Layer 
> 2 Mechanism Driver to allow PCI pass-through vNIC support along with 
> other networking technologies.
>
> During the Havana Release, we introduced Mellanox Neutron plugin that 
> enables networking via SRIOV pass-through devices or macvtap interfaces.
>
Hi, Irena & Robert

I'm very intresting on your work Mellanox Neutron plugin, which enable 
SRIOV devices or mactap interfaces. and could you provide more 
infomation about it: bp/patches/current work flow/what is expect from 
nova pci passthourgh.  and then, plus Robert's requements/discuss, i 
know the more detail about what's expected from nova pci, what pci next 
will to be.

in current stats i got:

a) fine classify of devices by auto discovery and request
     1) enable white list specify the address
     2) enable white list append group info like (IN/OUT/... anything)
     3) enable pci request can apppend  more infomation into the extra info
         i need input here, what is it? eventhough pci don't care the 
extra info, but clear is better.
         i.e. Robet's
              . direct pci-passthrough/macvtap
               port profile

b) extra info awawness allocation ('feature pci' by Robert)
<https://launchpad.net/%7Ebaoli>

    1) had API and code level interface to access extra info
    2) Scheduler awawa ness about extra info/or device type so vNIC can 
be differentiated.
    3) boot/interface-attach APIs:  API interface for convertneutron NIC 
info to PCI request. :
from binding:capabilities binding:profile  to
            PCI alias(request)/
            direct pci-passthrough/macvtap  ( is it need store into pci 
device extra info?)
            port profile( is it need store into pci device extra info?)
    4) scheduler enhancement to meet NIC requements

Yongli He at intel

> We want to integrate our solution with PCI pass-through Nova support. 
>  I will be glad to share more details if you are interested.
>
> The PCI pass-through networking support is planned to be discussed 
> during the summit: http://summit.openstack.org/cfp/details/129. I 
> think it's worth to drill down into more detailed proposal and present 
> it during the summit, especially since it impacts both nova and 
> neutron projects.
>
> Would you be interested in collaboration on this effort? Would you be 
> interested to exchange more emails or set an IRC/WebEx meeting during 
> this week before the summit?
>
> Regards,
>
> Irena
>
> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
> *Sent:* Friday, October 25, 2013 11:16 PM
> *To:* prashant.upadhyaya at aricent.com; Irena Berezovsky; 
> yunhong.jiang at intel.com; chris.friesen at windriver.com; yongli.he at intel.com
> *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle 
> Mestery (kmestery); Sandhya Dasu (sadasu)
> *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through 
> network support
>
> Hi Irena,
>
> This is Robert Li from Cisco Systems. Recently, I was tasked to 
> investigate such support for Cisco's systems that support VM-FEX, 
> which is a SRIOV technology supporting 802-1Qbh. I was able to bring 
> up nova instances with SRIOV interfaces, and establish networking in 
> between the instances that employes the SRIOV interfaces. Certainly, 
> this was accomplished with hacking and some manual intervention. Based 
> on this experience and my study with the two existing nova 
> pci-passthrough blueprints that have been implemented and committed 
> into Havana 
> (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt), 
>  I registered a couple of blueprints (one on Nova side, the other on 
> the Neutron side):
>
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>
> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
>
> in order to address SRIOV support in openstack.
>
> Please take a look at them and see if they make sense, and let me know 
> any comments and questions. We can also discuss this in the summit, I 
> suppose.
>
> I noticed that there is another thread on this topic, so copy those 
> folks  from that thread as well.
>
> thanks,
>
> Robert
>
> On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com 
> <mailto:irenab at mellanox.com>> wrote:
>
>     Hi,
>
>     As one of the next steps for PCI pass-through I would like to
>     discuss is the support for PCI pass-through vNIC.
>
>     While nova takes care of PCI pass-through device resources
>      management and VIF settings, neutron should manage their
>     networking configuration.
>
>     I would like to register asummit proposal to discuss the support
>     for PCI pass-through networking.
>
>     I am not sure what would be the right topic to discuss the PCI
>     pass-through networking, since it involve both nova and neutron.
>
>     There is already a session registered by Yongli on nova topic to
>     discuss the PCI pass-through next steps.
>
>     I think PCI pass-through networking is quite a big topic and it
>     worth to have a separate discussion.
>
>     Is there any other people who are interested to discuss it and
>     share their thoughts and experience?
>
>     Regards,
>
>     Irena
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/9ddbe9fe/attachment-0001.html>
"
NOVA 1402728 - CG,msg07206,<1705659276F6A540A34DBC19195799841027A7A4@xmb-rcd-x03.cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the ?nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the ?nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the ?nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it?s worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131028/ba1ae583/attachment.html>
"
NOVA 1402728 - CG,msg07187,<526F48FD.6070301@intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
	support

--

On 2013?10?29? 03:22, Robert Li (baoli) wrote:
> Hi Irena,
>
> Thank you very much for your comments. See inline.
>
> --Robert
>
> On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com 
> <mailto:irenab at mellanox.com>> wrote:
>
>     Hi Robert,
>
>     Thank you very much for sharing the information regarding your
>     efforts. Can you please share your idea of the end to end flow?
>     How do you suggest  to bind Nova and Neutron?
>
>
> The end to end flow is actually encompassed in the blueprints in a 
> nutshell. I will reiterate it in below. The binding between Nova and 
> Neutron occurs with the neutron v2 API that nova invokes in order to 
> provision the neutron services. The vif driver is responsible for 
> plugging in an instance onto the networking setup that neutron has 
> created on the host.
>
> Normally, one will invoke ""nova boot"" api with the ?nic options to 
> specify the nic with which the instance will be connected to the 
> network. It currently allows net-id, fixed ip and/or port-id to be 
> specified for the option. However, it doesn't allow one to specify 
> special networking requirements for the instance. Thanks to the nova 
> pci-passthrough work, one can specify PCI passthrough device(s) in the 
> nova flavor. But it doesn't provide means to tie up these PCI devices 
> in the case of ethernet adpators with networking services. Therefore 
> the idea is actually simple as indicated by the blueprint titles, to 
> provide means to tie up SRIOV devices with neutron services. A work 
> flow would roughly look like this for 'nova boot':
>
>       -- Specifies networking requirements in the ?nic option. 
> Specifically for SRIOV, allow the following to be specified in 
> addition to the existing required information:
>                . PCI alias
>                . direct pci-passthrough/macvtap
>                . port profileid that is compliant with 802.1Qbh
>         The above information is optional. In the absence of them, the 
> existing behavior remains.
>
>      -- if special networking requirements exist, Nova api creates PCI 
> requests in the nova instance type for scheduling purpose
>
>      -- Nova scheduler schedules the instance based on the requested 
> flavor plus the PCI requests that are created for networking.
>
>      -- Nova compute invokes neutron services with PCI passthrough 
> information if any
>
>      --  Neutron performs its normal operations based on the request, 
> such as allocating a port, assigning ip addresses, etc. Specific to 
> SRIOV, it should validate the information such as profileid, and 
> stores them in its db. It's also possible to associate a port 
> profileid with a neutron network so that port profileid becomes 
> optional in the ?nic option. Neutron returns  nova the port 
> information, especially for PCI passthrough related information in the 
> port binding object. Currently, the port binding object contains the 
> following information:
>           binding:vif_type
>           binding:host_id
>           binding:profile
>           binding:capabilities
(openstack bonce stop me to sent to so many people at one time, so i 
remove cc & to, hope every one can see this)

i heard of some nic passthrough solution in summary, and you metioned 
this, in high level of implement the NIC passthrough there is:
           hardware VEB (Virtual ethernet Switches )
           the nic need external switch like 802.1qbg

so question is:
           where is the diffrent type infomation?
           does  802.1qbg need know which port the PF connected to?
>
>     -- nova constructs the domain xml and plug in the instance by 
> calling the vif driver. The vif driver can build up the interface xml 
> based on the port binding information.
>
>
>
>     The blueprints you registered make sense. On Nova side, there is a
>     need to bind between requested virtual network and PCI
>     device/interface to be allocated as vNIC.
>
>     On the Neutron side, there is a need to  support networking
>     configuration of the vNIC. Neutron should be able to identify the
>     PCI device/macvtap interface in order to apply configuration. I
>     think it makes sense to provide neutron integration via dedicated
>     Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC
>     support along with other networking technologies.
>
>
> I haven't sorted through this yet. A neutron port could be associated 
> with a PCI device or not, which is a common feature, IMHO. However, a 
> ML2 driver may be needed specific to a particular SRIOV technology.
>
>     During the Havana Release, we introduced Mellanox Neutron plugin
>     that enables networking via SRIOV pass-through devices or macvtap
>     interfaces.
>
>     We want to integrate our solution with PCI pass-through Nova
>     support.  I will be glad to share more details if you are interested.
>
>
> Good to know that you already have a SRIOV implementation. I found out 
> some information online about the mlnx plugin, but need more time to 
> get to know it better. And certainly I'm interested in knowing its 
> details.
>
>     The PCI pass-through networking support is planned to be discussed
>     during the summit: http://summit.openstack.org/cfp/details/129. I
>     think it?s worth to drill down into more detailed proposal and
>     present it during the summit, especially since it impacts both
>     nova and neutron projects.
>
> I agree. Maybe we can steal some time in that discussion.
>
>     Would you be interested in collaboration on this effort? Would you
>     be interested to exchange more emails or set an IRC/WebEx meeting
>     during this week before the summit?
>
>
> Sure. If folks want to discuss it before the summit, we can schedule a 
> webex later this week. Or otherwise, we can continue the discussion 
> with email.
>
>     Regards,
>
>     Irena
>
>     *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>     *Sent:* Friday, October 25, 2013 11:16 PM
>     *To:* prashant.upadhyaya at aricent.com
>     <mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky;
>     yunhong.jiang at intel.com <mailto:yunhong.jiang at intel.com>;
>     chris.friesen at windriver.com <mailto:chris.friesen at windriver.com>;
>     yongli.he at intel.com <mailto:yongli.he at intel.com>
>     *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen);
>     Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
>     *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
>     network support
>
>     Hi Irena,
>
>     This is Robert Li from Cisco Systems. Recently, I was tasked to
>     investigate such support for Cisco's systems that support VM-FEX,
>     which is a SRIOV technology supporting 802-1Qbh. I was able to
>     bring up nova instances with SRIOV interfaces, and establish
>     networking in between the instances that employes the SRIOV
>     interfaces. Certainly, this was accomplished with hacking and some
>     manual intervention. Based on this experience and my study with
>     the two existing nova pci-passthrough blueprints that have been
>     implemented and committed into Havana
>     (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
>     https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),
>      I registered a couple of blueprints (one on Nova side, the other
>     on the Neutron side):
>
>     https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>
>     https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
>
>     in order to address SRIOV support in openstack.
>
>     Please take a look at them and see if they make sense, and let me
>     know any comments and questions. We can also discuss this in the
>     summit, I suppose.
>
>     I noticed that there is another thread on this topic, so copy
>     those folks  from that thread as well.
>
>     thanks,
>
>     Robert
>
>     On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com
>     <mailto:irenab at mellanox.com>> wrote:
>
>         Hi,
>
>         As one of the next steps for PCI pass-through I would like to
>         discuss is the support for PCI pass-through vNIC.
>
>         While nova takes care of PCI pass-through device resources
>          management and VIF settings, neutron should manage their
>         networking configuration.
>
>         I would like to register asummit proposal to discuss the
>         support for PCI pass-through networking.
>
>         I am not sure what would be the right topic to discuss the PCI
>         pass-through networking, since it involve both nova and neutron.
>
>         There is already a session registered by Yongli on nova topic
>         to discuss the PCI pass-through next steps.
>
>         I think PCI pass-through networking is quite a big topic and
>         it worth to have a separate discussion.
>
>         Is there any other people who are interested to discuss it and
>         share their thoughts and experience?
>
>         Regards,
>
>         Irena
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/213d8837/attachment.html>
"
NOVA 1402728 - CG,msg07207,<DDCAE26804250545B9934A2056554AA01FBA9084@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting because it's more openstack style and also can keep the minutes clearly.

To your flow, can you give more detailed example. For example, I can consider user specify the instance with -nic option specify a network id, and then how nova device the requirement to the PCI device? I assume the network id should define the switches that the device can connect to , but how is that information translated to the PCI property requirement? Will this translation happen before the nova scheduler make host decision?

Thanks
--jyh

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, October 28, 2013 12:22 PM
To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang, Yunhong; chris.friesen at windriver.com; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the -nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the -nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the -nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it's worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/55aac05c/attachment.html>
"
NOVA 1402728 - CG,msg07210,<9D25E123B44F4A4291F4B5C13DA94E778024CC05@MTLDAG01.mtl.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi Jiang, Robert,
IRC meeting option works for me.
If I understand your question below, you are looking for a way to tie up between requested virtual network(s) and requested PCI device(s). The way we did it in our solution  is to map a provider:physical_network to an interface that represents the Physical Function. Every virtual network is bound to the provider:physical_network, so the PCI device should be allocated based on this mapping.  We can  map a PCI alias to the provider:physical_network.

Another topic to discuss is where the mapping between neutron port and PCI device should be managed. One way to solve it, is to propagate the allocated PCI device details to neutron on port creation.
In case  there is no qbg/qbh support, VF networking configuration should be applied locally on the Host.
The question is when and how to apply networking configuration on the PCI device?
We see the following options:

*         it can be done on port creation.

*         It can be done when nova VIF driver is called for vNIC plugging. This will require to  have all networking configuration available to the VIF driver or send request to the neutron server to obtain it.

*         It can be done by  having a dedicated L2 neutron agent on each Host that scans for allocated PCI devices  and then retrieves networking configuration from the server and configures the device. The agent will be also responsible for managing update requests coming from the neutron server.


For macvtap vNIC type assignment, the networking configuration can be applied by a dedicated L2 neutron agent.

BR,
Irena

From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
Sent: Tuesday, October 29, 2013 9:04 AM

To: Robert Li (baoli); Irena Berezovsky; prashant.upadhyaya at aricent.com; chris.friesen at windriver.com; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network support

Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting because it's more openstack style and also can keep the minutes clearly.

To your flow, can you give more detailed example. For example, I can consider user specify the instance with -nic option specify a network id, and then how nova device the requirement to the PCI device? I assume the network id should define the switches that the device can connect to , but how is that information translated to the PCI property requirement? Will this translation happen before the nova scheduler make host decision?

Thanks
--jyh

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, October 28, 2013 12:22 PM
To: Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the -nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the -nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the -nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it's worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/f31f58b3/attachment.html>
"
NOVA 1402728 - CG,msg07217,<CABib2_o6tVKB+pEk6p-OByYd5ZF23JcMLM0yN37pcX6bHOHNAQ@mail.gmail.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
	support

--

I would love to see a symmetry between Cinder local volumes and
Neutron PCI passthrough VIFs.

Not entirely sure I have that clear in my head right now, but I just
wanted to share the idea:
* describe resource external to nova that is attached to VM in the API
(block device mapping and/or vif references)
* ideally the nova scheduler needs to be aware of the local capacity,
and how that relates to the above information (relates to the cross
service scheduling issues)
* state of the device should be stored by Neutron/Cinder
(attached/detached, capacity, IP, etc), but still exposed to the
""scheduler""
* connection params get given to Nova from Neutron/Cinder
* nova still has the vif driver or volume driver to make the final connection
* the disk should be formatted/expanded, and network info injected in
the same way as before (cloud-init, config drive, DHCP, etc)

John

On 29 October 2013 10:17, Irena Berezovsky <irenab at mellanox.com> wrote:
> Hi Jiang, Robert,
>
> IRC meeting option works for me.
>
> If I understand your question below, you are looking for a way to tie up
> between requested virtual network(s) and requested PCI device(s). The way we
> did it in our solution  is to map a provider:physical_network to an
> interface that represents the Physical Function. Every virtual network is
> bound to the provider:physical_network, so the PCI device should be
> allocated based on this mapping.  We can  map a PCI alias to the
> provider:physical_network.
>
>
>
> Another topic to discuss is where the mapping between neutron port and PCI
> device should be managed. One way to solve it, is to propagate the allocated
> PCI device details to neutron on port creation.
>
> In case  there is no qbg/qbh support, VF networking configuration should be
> applied locally on the Host.
>
> The question is when and how to apply networking configuration on the PCI
> device?
>
> We see the following options:
>
> ?         it can be done on port creation.
>
> ?         It can be done when nova VIF driver is called for vNIC plugging.
> This will require to  have all networking configuration available to the VIF
> driver or send request to the neutron server to obtain it.
>
> ?         It can be done by  having a dedicated L2 neutron agent on each
> Host that scans for allocated PCI devices  and then retrieves networking
> configuration from the server and configures the device. The agent will be
> also responsible for managing update requests coming from the neutron
> server.
>
>
>
> For macvtap vNIC type assignment, the networking configuration can be
> applied by a dedicated L2 neutron agent.
>
>
>
> BR,
>
> Irena
>
>
>
> From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
> Sent: Tuesday, October 29, 2013 9:04 AM
>
>
> To: Robert Li (baoli); Irena Berezovsky; prashant.upadhyaya at aricent.com;
> chris.friesen at windriver.com; He, Yongli; Itzik Brown
>
>
> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery
> (kmestery); Sandhya Dasu (sadasu)
> Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
>
>
>
> Robert, is it possible to have a IRC meeting? I?d prefer to IRC meeting
> because it?s more openstack style and also can keep the minutes clearly.
>
>
>
> To your flow, can you give more detailed example. For example, I can
> consider user specify the instance with ?nic option specify a network id,
> and then how nova device the requirement to the PCI device? I assume the
> network id should define the switches that the device can connect to , but
> how is that information translated to the PCI property requirement? Will
> this translation happen before the nova scheduler make host decision?
>
>
>
> Thanks
>
> --jyh
>
>
>
> From: Robert Li (baoli) [mailto:baoli at cisco.com]
> Sent: Monday, October 28, 2013 12:22 PM
> To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang, Yunhong;
> chris.friesen at windriver.com; He, Yongli; Itzik Brown
> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery
> (kmestery); Sandhya Dasu (sadasu)
> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
>
>
>
> Hi Irena,
>
>
>
> Thank you very much for your comments. See inline.
>
>
>
> --Robert
>
>
>
> On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>
>
>
> Hi Robert,
>
> Thank you very much for sharing the information regarding your efforts. Can
> you please share your idea of the end to end flow? How do you suggest  to
> bind Nova and Neutron?
>
>
>
> The end to end flow is actually encompassed in the blueprints in a nutshell.
> I will reiterate it in below. The binding between Nova and Neutron occurs
> with the neutron v2 API that nova invokes in order to provision the neutron
> services. The vif driver is responsible for plugging in an instance onto the
> networking setup that neutron has created on the host.
>
>
>
> Normally, one will invoke ""nova boot"" api with the ?nic options to specify
> the nic with which the instance will be connected to the network. It
> currently allows net-id, fixed ip and/or port-id to be specified for the
> option. However, it doesn't allow one to specify special networking
> requirements for the instance. Thanks to the nova pci-passthrough work, one
> can specify PCI passthrough device(s) in the nova flavor. But it doesn't
> provide means to tie up these PCI devices in the case of ethernet adpators
> with networking services. Therefore the idea is actually simple as indicated
> by the blueprint titles, to provide means to tie up SRIOV devices with
> neutron services. A work flow would roughly look like this for 'nova boot':
>
>
>
>       -- Specifies networking requirements in the ?nic option. Specifically
> for SRIOV, allow the following to be specified in addition to the existing
> required information:
>
>                . PCI alias
>
>                . direct pci-passthrough/macvtap
>
>                . port profileid that is compliant with 802.1Qbh
>
>
>
>         The above information is optional. In the absence of them, the
> existing behavior remains.
>
>
>
>      -- if special networking requirements exist, Nova api creates PCI
> requests in the nova instance type for scheduling purpose
>
>
>
>      -- Nova scheduler schedules the instance based on the requested flavor
> plus the PCI requests that are created for networking.
>
>
>
>      -- Nova compute invokes neutron services with PCI passthrough
> information if any
>
>
>
>      --  Neutron performs its normal operations based on the request, such
> as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it
> should validate the information such as profileid, and stores them in its
> db. It's also possible to associate a port profileid with a neutron network
> so that port profileid becomes optional in the ?nic option. Neutron returns
> nova the port information, especially for PCI passthrough related
> information in the port binding object. Currently, the port binding object
> contains the following information:
>
>           binding:vif_type
>
>           binding:host_id
>
>           binding:profile
>
>           binding:capabilities
>
>
>
>     -- nova constructs the domain xml and plug in the instance by calling
> the vif driver. The vif driver can build up the interface xml based on the
> port binding information.
>
>
>
>
>
>
>
>
>
> The blueprints you registered make sense. On Nova side, there is a need to
> bind between requested virtual network and PCI device/interface to be
> allocated as vNIC.
>
> On the Neutron side, there is a need to  support networking configuration of
> the vNIC. Neutron should be able to identify the PCI device/macvtap
> interface in order to apply configuration. I think it makes sense to provide
> neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow
> PCI pass-through vNIC support along with other networking technologies.
>
>
>
> I haven't sorted through this yet. A neutron port could be associated with a
> PCI device or not, which is a common feature, IMHO. However, a ML2 driver
> may be needed specific to a particular SRIOV technology.
>
>
>
>
>
> During the Havana Release, we introduced Mellanox Neutron plugin that
> enables networking via SRIOV pass-through devices or macvtap interfaces.
>
> We want to integrate our solution with PCI pass-through Nova support.  I
> will be glad to share more details if you are interested.
>
>
>
>
>
> Good to know that you already have a SRIOV implementation. I found out some
> information online about the mlnx plugin, but need more time to get to know
> it better. And certainly I'm interested in knowing its details.
>
>
>
> The PCI pass-through networking support is planned to be discussed during
> the summit: http://summit.openstack.org/cfp/details/129. I think it?s worth
> to drill down into more detailed proposal and present it during the summit,
> especially since it impacts both nova and neutron projects.
>
>
>
> I agree. Maybe we can steal some time in that discussion.
>
>
>
> Would you be interested in collaboration on this effort? Would you be
> interested to exchange more emails or set an IRC/WebEx meeting during this
> week before the summit?
>
>
>
> Sure. If folks want to discuss it before the summit, we can schedule a webex
> later this week. Or otherwise, we can continue the discussion with email.
>
>
>
>
>
>
>
> Regards,
>
> Irena
>
>
>
> From: Robert Li (baoli) [mailto:baoli at cisco.com]
> Sent: Friday, October 25, 2013 11:16 PM
> To: prashant.upadhyaya at aricent.com; Irena Berezovsky;
> yunhong.jiang at intel.com; chris.friesen at windriver.com; yongli.he at intel.com
> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery
> (kmestery); Sandhya Dasu (sadasu)
> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
>
>
>
> Hi Irena,
>
>
>
> This is Robert Li from Cisco Systems. Recently, I was tasked to investigate
> such support for Cisco's systems that support VM-FEX, which is a SRIOV
> technology supporting 802-1Qbh. I was able to bring up nova instances with
> SRIOV interfaces, and establish networking in between the instances that
> employes the SRIOV interfaces. Certainly, this was accomplished with hacking
> and some manual intervention. Based on this experience and my study with the
> two existing nova pci-passthrough blueprints that have been implemented and
> committed into Havana
> (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
> registered a couple of blueprints (one on Nova side, the other on the
> Neutron side):
>
>
>
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>
> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
>
>
>
> in order to address SRIOV support in openstack.
>
>
>
> Please take a look at them and see if they make sense, and let me know any
> comments and questions. We can also discuss this in the summit, I suppose.
>
>
>
> I noticed that there is another thread on this topic, so copy those folks
> from that thread as well.
>
>
>
> thanks,
>
> Robert
>
>
>
> On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>
>
>
> Hi,
>
> As one of the next steps for PCI pass-through I would like to discuss is the
> support for PCI pass-through vNIC.
>
> While nova takes care of PCI pass-through device resources  management and
> VIF settings, neutron should manage their networking configuration.
>
> I would like to register asummit proposal to discuss the support for PCI
> pass-through networking.
>
> I am not sure what would be the right topic to discuss the PCI pass-through
> networking, since it involve both nova and neutron.
>
> There is already a session registered by Yongli on nova topic to discuss the
> PCI pass-through next steps.
>
> I think PCI pass-through networking is quite a big topic and it worth to
> have a separate discussion.
>
> Is there any other people who are interested to discuss it and share their
> thoughts and experience?
>
>
>
> Regards,
>
> Irena
>
>
>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

"
NOVA 1402728 - CG,msg07241,<1705659276F6A540A34DBC19195799841027C3BB@xmb-rcd-x03.cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi John,

Great to hear from you on Cinder with pcipassthrough. I thought that it
would be coming. I like the idea.

thanks,
Robert

On 10/29/13 6:46 AM, ""John Garbutt"" <john at johngarbutt.com> wrote:

>I would love to see a symmetry between Cinder local volumes and
>Neutron PCI passthrough VIFs.
>
>Not entirely sure I have that clear in my head right now, but I just
>wanted to share the idea:
>* describe resource external to nova that is attached to VM in the API
>(block device mapping and/or vif references)
>* ideally the nova scheduler needs to be aware of the local capacity,
>and how that relates to the above information (relates to the cross
>service scheduling issues)
>* state of the device should be stored by Neutron/Cinder
>(attached/detached, capacity, IP, etc), but still exposed to the
>""scheduler""
>* connection params get given to Nova from Neutron/Cinder
>* nova still has the vif driver or volume driver to make the final
>connection
>* the disk should be formatted/expanded, and network info injected in
>the same way as before (cloud-init, config drive, DHCP, etc)
>
>John
>
>On 29 October 2013 10:17, Irena Berezovsky <irenab at mellanox.com> wrote:
>> Hi Jiang, Robert,
>>
>> IRC meeting option works for me.
>>
>> If I understand your question below, you are looking for a way to tie up
>> between requested virtual network(s) and requested PCI device(s). The
>>way we
>> did it in our solution  is to map a provider:physical_network to an
>> interface that represents the Physical Function. Every virtual network
>>is
>> bound to the provider:physical_network, so the PCI device should be
>> allocated based on this mapping.  We can  map a PCI alias to the
>> provider:physical_network.
>>
>>
>>
>> Another topic to discuss is where the mapping between neutron port and
>>PCI
>> device should be managed. One way to solve it, is to propagate the
>>allocated
>> PCI device details to neutron on port creation.
>>
>> In case  there is no qbg/qbh support, VF networking configuration
>>should be
>> applied locally on the Host.
>>
>> The question is when and how to apply networking configuration on the
>>PCI
>> device?
>>
>> We see the following options:
>>
>> ?         it can be done on port creation.
>>
>> ?         It can be done when nova VIF driver is called for vNIC
>>plugging.
>> This will require to  have all networking configuration available to
>>the VIF
>> driver or send request to the neutron server to obtain it.
>>
>> ?         It can be done by  having a dedicated L2 neutron agent on each
>> Host that scans for allocated PCI devices  and then retrieves networking
>> configuration from the server and configures the device. The agent will
>>be
>> also responsible for managing update requests coming from the neutron
>> server.
>>
>>
>>
>> For macvtap vNIC type assignment, the networking configuration can be
>> applied by a dedicated L2 neutron agent.
>>
>>
>>
>> BR,
>>
>> Irena
>>
>>
>>
>> From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
>> Sent: Tuesday, October 29, 2013 9:04 AM
>>
>>
>> To: Robert Li (baoli); Irena Berezovsky; prashant.upadhyaya at aricent.com;
>> chris.friesen at windriver.com; He, Yongli; Itzik Brown
>>
>>
>> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
>>Mestery
>> (kmestery); Sandhya Dasu (sadasu)
>> Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network
>> support
>>
>>
>>
>> Robert, is it possible to have a IRC meeting? I?d prefer to IRC meeting
>> because it?s more openstack style and also can keep the minutes clearly.
>>
>>
>>
>> To your flow, can you give more detailed example. For example, I can
>> consider user specify the instance with ?nic option specify a network
>>id,
>> and then how nova device the requirement to the PCI device? I assume the
>> network id should define the switches that the device can connect to ,
>>but
>> how is that information translated to the PCI property requirement? Will
>> this translation happen before the nova scheduler make host decision?
>>
>>
>>
>> Thanks
>>
>> --jyh
>>
>>
>>
>> From: Robert Li (baoli) [mailto:baoli at cisco.com]
>> Sent: Monday, October 28, 2013 12:22 PM
>> To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang, Yunhong;
>> chris.friesen at windriver.com; He, Yongli; Itzik Brown
>> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
>>Mestery
>> (kmestery); Sandhya Dasu (sadasu)
>> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
>> support
>>
>>
>>
>> Hi Irena,
>>
>>
>>
>> Thank you very much for your comments. See inline.
>>
>>
>>
>> --Robert
>>
>>
>>
>> On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>
>>
>>
>> Hi Robert,
>>
>> Thank you very much for sharing the information regarding your efforts.
>>Can
>> you please share your idea of the end to end flow? How do you suggest
>>to
>> bind Nova and Neutron?
>>
>>
>>
>> The end to end flow is actually encompassed in the blueprints in a
>>nutshell.
>> I will reiterate it in below. The binding between Nova and Neutron
>>occurs
>> with the neutron v2 API that nova invokes in order to provision the
>>neutron
>> services. The vif driver is responsible for plugging in an instance
>>onto the
>> networking setup that neutron has created on the host.
>>
>>
>>
>> Normally, one will invoke ""nova boot"" api with the ?nic options to
>>specify
>> the nic with which the instance will be connected to the network. It
>> currently allows net-id, fixed ip and/or port-id to be specified for the
>> option. However, it doesn't allow one to specify special networking
>> requirements for the instance. Thanks to the nova pci-passthrough work,
>>one
>> can specify PCI passthrough device(s) in the nova flavor. But it doesn't
>> provide means to tie up these PCI devices in the case of ethernet
>>adpators
>> with networking services. Therefore the idea is actually simple as
>>indicated
>> by the blueprint titles, to provide means to tie up SRIOV devices with
>> neutron services. A work flow would roughly look like this for 'nova
>>boot':
>>
>>
>>
>>       -- Specifies networking requirements in the ?nic option.
>>Specifically
>> for SRIOV, allow the following to be specified in addition to the
>>existing
>> required information:
>>
>>                . PCI alias
>>
>>                . direct pci-passthrough/macvtap
>>
>>                . port profileid that is compliant with 802.1Qbh
>>
>>
>>
>>         The above information is optional. In the absence of them, the
>> existing behavior remains.
>>
>>
>>
>>      -- if special networking requirements exist, Nova api creates PCI
>> requests in the nova instance type for scheduling purpose
>>
>>
>>
>>      -- Nova scheduler schedules the instance based on the requested
>>flavor
>> plus the PCI requests that are created for networking.
>>
>>
>>
>>      -- Nova compute invokes neutron services with PCI passthrough
>> information if any
>>
>>
>>
>>      --  Neutron performs its normal operations based on the request,
>>such
>> as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it
>> should validate the information such as profileid, and stores them in
>>its
>> db. It's also possible to associate a port profileid with a neutron
>>network
>> so that port profileid becomes optional in the ?nic option. Neutron
>>returns
>> nova the port information, especially for PCI passthrough related
>> information in the port binding object. Currently, the port binding
>>object
>> contains the following information:
>>
>>           binding:vif_type
>>
>>           binding:host_id
>>
>>           binding:profile
>>
>>           binding:capabilities
>>
>>
>>
>>     -- nova constructs the domain xml and plug in the instance by
>>calling
>> the vif driver. The vif driver can build up the interface xml based on
>>the
>> port binding information.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> The blueprints you registered make sense. On Nova side, there is a need
>>to
>> bind between requested virtual network and PCI device/interface to be
>> allocated as vNIC.
>>
>> On the Neutron side, there is a need to  support networking
>>configuration of
>> the vNIC. Neutron should be able to identify the PCI device/macvtap
>> interface in order to apply configuration. I think it makes sense to
>>provide
>> neutron integration via dedicated Modular Layer 2 Mechanism Driver to
>>allow
>> PCI pass-through vNIC support along with other networking technologies.
>>
>>
>>
>> I haven't sorted through this yet. A neutron port could be associated
>>with a
>> PCI device or not, which is a common feature, IMHO. However, a ML2
>>driver
>> may be needed specific to a particular SRIOV technology.
>>
>>
>>
>>
>>
>> During the Havana Release, we introduced Mellanox Neutron plugin that
>> enables networking via SRIOV pass-through devices or macvtap interfaces.
>>
>> We want to integrate our solution with PCI pass-through Nova support.  I
>> will be glad to share more details if you are interested.
>>
>>
>>
>>
>>
>> Good to know that you already have a SRIOV implementation. I found out
>>some
>> information online about the mlnx plugin, but need more time to get to
>>know
>> it better. And certainly I'm interested in knowing its details.
>>
>>
>>
>> The PCI pass-through networking support is planned to be discussed
>>during
>> the summit: http://summit.openstack.org/cfp/details/129. I think it?s
>>worth
>> to drill down into more detailed proposal and present it during the
>>summit,
>> especially since it impacts both nova and neutron projects.
>>
>>
>>
>> I agree. Maybe we can steal some time in that discussion.
>>
>>
>>
>> Would you be interested in collaboration on this effort? Would you be
>> interested to exchange more emails or set an IRC/WebEx meeting during
>>this
>> week before the summit?
>>
>>
>>
>> Sure. If folks want to discuss it before the summit, we can schedule a
>>webex
>> later this week. Or otherwise, we can continue the discussion with
>>email.
>>
>>
>>
>>
>>
>>
>>
>> Regards,
>>
>> Irena
>>
>>
>>
>> From: Robert Li (baoli) [mailto:baoli at cisco.com]
>> Sent: Friday, October 25, 2013 11:16 PM
>> To: prashant.upadhyaya at aricent.com; Irena Berezovsky;
>> yunhong.jiang at intel.com; chris.friesen at windriver.com;
>>yongli.he at intel.com
>> Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
>>Mestery
>> (kmestery); Sandhya Dasu (sadasu)
>> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
>> support
>>
>>
>>
>> Hi Irena,
>>
>>
>>
>> This is Robert Li from Cisco Systems. Recently, I was tasked to
>>investigate
>> such support for Cisco's systems that support VM-FEX, which is a SRIOV
>> technology supporting 802-1Qbh. I was able to bring up nova instances
>>with
>> SRIOV interfaces, and establish networking in between the instances that
>> employes the SRIOV interfaces. Certainly, this was accomplished with
>>hacking
>> and some manual intervention. Based on this experience and my study
>>with the
>> two existing nova pci-passthrough blueprints that have been implemented
>>and
>> committed into Havana
>> (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
>> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
>> registered a couple of blueprints (one on Nova side, the other on the
>> Neutron side):
>>
>>
>>
>> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>>
>> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
>>
>>
>>
>> in order to address SRIOV support in openstack.
>>
>>
>>
>> Please take a look at them and see if they make sense, and let me know
>>any
>> comments and questions. We can also discuss this in the summit, I
>>suppose.
>>
>>
>>
>> I noticed that there is another thread on this topic, so copy those
>>folks
>> from that thread as well.
>>
>>
>>
>> thanks,
>>
>> Robert
>>
>>
>>
>> On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>
>>
>>
>> Hi,
>>
>> As one of the next steps for PCI pass-through I would like to discuss
>>is the
>> support for PCI pass-through vNIC.
>>
>> While nova takes care of PCI pass-through device resources  management
>>and
>> VIF settings, neutron should manage their networking configuration.
>>
>> I would like to register asummit proposal to discuss the support for PCI
>> pass-through networking.
>>
>> I am not sure what would be the right topic to discuss the PCI
>>pass-through
>> networking, since it involve both nova and neutron.
>>
>> There is already a session registered by Yongli on nova topic to
>>discuss the
>> PCI pass-through next steps.
>>
>> I think PCI pass-through networking is quite a big topic and it worth to
>> have a separate discussion.
>>
>> Is there any other people who are interested to discuss it and share
>>their
>> thoughts and experience?
>>
>>
>>
>> Regards,
>>
>> Irena
>>
>>
>>
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>


"
NOVA 1402728 - CG,msg07282,<DDCAE26804250545B9934A2056554AA01FBA9D28@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--


> * describe resource external to nova that is attached to VM in the API
> (block device mapping and/or vif references)
> * ideally the nova scheduler needs to be aware of the local capacity,
> and how that relates to the above information (relates to the cross
> service scheduling issues)

I think this possibly a bit different. For volume, it's sure managed by Cinder, but for PCI devices, currently
It ;s managed by nova. So we possibly need nova to translate the information (possibly before nova scheduler).

> * state of the device should be stored by Neutron/Cinder
> (attached/detached, capacity, IP, etc), but still exposed to the
> ""scheduler""

I'm not sure if we can keep the state of the device in Neutron. Currently nova manage all PCI devices.

Thanks
--jyh


> * connection params get given to Nova from Neutron/Cinder
> * nova still has the vif driver or volume driver to make the final connection
> * the disk should be formatted/expanded, and network info injected in
> the same way as before (cloud-init, config drive, DHCP, etc)
> 
> John
> 
> On 29 October 2013 10:17, Irena Berezovsky <irenab at mellanox.com>
> wrote:
> > Hi Jiang, Robert,
> >
> > IRC meeting option works for me.
> >
> > If I understand your question below, you are looking for a way to tie up
> > between requested virtual network(s) and requested PCI device(s). The
> way we
> > did it in our solution  is to map a provider:physical_network to an
> > interface that represents the Physical Function. Every virtual network is
> > bound to the provider:physical_network, so the PCI device should be
> > allocated based on this mapping.  We can  map a PCI alias to the
> > provider:physical_network.
> >
> >
> >
> > Another topic to discuss is where the mapping between neutron port
> and PCI
> > device should be managed. One way to solve it, is to propagate the
> allocated
> > PCI device details to neutron on port creation.
> >
> > In case  there is no qbg/qbh support, VF networking configuration
> should be
> > applied locally on the Host.
> >
> > The question is when and how to apply networking configuration on the
> PCI
> > device?
> >
> > We see the following options:
> >
> > *         it can be done on port creation.
> >
> > *         It can be done when nova VIF driver is called for vNIC
> plugging.
> > This will require to  have all networking configuration available to the
> VIF
> > driver or send request to the neutron server to obtain it.
> >
> > *         It can be done by  having a dedicated L2 neutron agent on
> each
> > Host that scans for allocated PCI devices  and then retrieves networking
> > configuration from the server and configures the device. The agent will
> be
> > also responsible for managing update requests coming from the neutron
> > server.
> >
> >
> >
> > For macvtap vNIC type assignment, the networking configuration can be
> > applied by a dedicated L2 neutron agent.
> >
> >
> >
> > BR,
> >
> > Irena
> >
> >
> >
> > From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
> > Sent: Tuesday, October 29, 2013 9:04 AM
> >
> >
> > To: Robert Li (baoli); Irena Berezovsky;
> prashant.upadhyaya at aricent.com;
> > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> >
> >
> > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> Mestery
> > (kmestery); Sandhya Dasu (sadasu)
> > Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network
> > support
> >
> >
> >
> > Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting
> > because it's more openstack style and also can keep the minutes
> clearly.
> >
> >
> >
> > To your flow, can you give more detailed example. For example, I can
> > consider user specify the instance with -nic option specify a network id,
> > and then how nova device the requirement to the PCI device? I assume
> the
> > network id should define the switches that the device can connect to ,
> but
> > how is that information translated to the PCI property requirement? Will
> > this translation happen before the nova scheduler make host decision?
> >
> >
> >
> > Thanks
> >
> > --jyh
> >
> >
> >
> > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > Sent: Monday, October 28, 2013 12:22 PM
> > To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang, Yunhong;
> > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> Mestery
> > (kmestery); Sandhya Dasu (sadasu)
> > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> > support
> >
> >
> >
> > Hi Irena,
> >
> >
> >
> > Thank you very much for your comments. See inline.
> >
> >
> >
> > --Robert
> >
> >
> >
> > On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com>
> wrote:
> >
> >
> >
> > Hi Robert,
> >
> > Thank you very much for sharing the information regarding your efforts.
> Can
> > you please share your idea of the end to end flow? How do you suggest
> to
> > bind Nova and Neutron?
> >
> >
> >
> > The end to end flow is actually encompassed in the blueprints in a
> nutshell.
> > I will reiterate it in below. The binding between Nova and Neutron
> occurs
> > with the neutron v2 API that nova invokes in order to provision the
> neutron
> > services. The vif driver is responsible for plugging in an instance onto the
> > networking setup that neutron has created on the host.
> >
> >
> >
> > Normally, one will invoke ""nova boot"" api with the -nic options to specify
> > the nic with which the instance will be connected to the network. It
> > currently allows net-id, fixed ip and/or port-id to be specified for the
> > option. However, it doesn't allow one to specify special networking
> > requirements for the instance. Thanks to the nova pci-passthrough work,
> one
> > can specify PCI passthrough device(s) in the nova flavor. But it doesn't
> > provide means to tie up these PCI devices in the case of ethernet
> adpators
> > with networking services. Therefore the idea is actually simple as
> indicated
> > by the blueprint titles, to provide means to tie up SRIOV devices with
> > neutron services. A work flow would roughly look like this for 'nova
> boot':
> >
> >
> >
> >       -- Specifies networking requirements in the -nic option.
> Specifically
> > for SRIOV, allow the following to be specified in addition to the existing
> > required information:
> >
> >                . PCI alias
> >
> >                . direct pci-passthrough/macvtap
> >
> >                . port profileid that is compliant with 802.1Qbh
> >
> >
> >
> >         The above information is optional. In the absence of them, the
> > existing behavior remains.
> >
> >
> >
> >      -- if special networking requirements exist, Nova api creates PCI
> > requests in the nova instance type for scheduling purpose
> >
> >
> >
> >      -- Nova scheduler schedules the instance based on the requested
> flavor
> > plus the PCI requests that are created for networking.
> >
> >
> >
> >      -- Nova compute invokes neutron services with PCI passthrough
> > information if any
> >
> >
> >
> >      --  Neutron performs its normal operations based on the request,
> such
> > as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it
> > should validate the information such as profileid, and stores them in its
> > db. It's also possible to associate a port profileid with a neutron network
> > so that port profileid becomes optional in the -nic option. Neutron
> returns
> > nova the port information, especially for PCI passthrough related
> > information in the port binding object. Currently, the port binding object
> > contains the following information:
> >
> >           binding:vif_type
> >
> >           binding:host_id
> >
> >           binding:profile
> >
> >           binding:capabilities
> >
> >
> >
> >     -- nova constructs the domain xml and plug in the instance by
> calling
> > the vif driver. The vif driver can build up the interface xml based on the
> > port binding information.
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > The blueprints you registered make sense. On Nova side, there is a need
> to
> > bind between requested virtual network and PCI device/interface to be
> > allocated as vNIC.
> >
> > On the Neutron side, there is a need to  support networking
> configuration of
> > the vNIC. Neutron should be able to identify the PCI device/macvtap
> > interface in order to apply configuration. I think it makes sense to
> provide
> > neutron integration via dedicated Modular Layer 2 Mechanism Driver to
> allow
> > PCI pass-through vNIC support along with other networking technologies.
> >
> >
> >
> > I haven't sorted through this yet. A neutron port could be associated
> with a
> > PCI device or not, which is a common feature, IMHO. However, a ML2
> driver
> > may be needed specific to a particular SRIOV technology.
> >
> >
> >
> >
> >
> > During the Havana Release, we introduced Mellanox Neutron plugin that
> > enables networking via SRIOV pass-through devices or macvtap
> interfaces.
> >
> > We want to integrate our solution with PCI pass-through Nova support.
> I
> > will be glad to share more details if you are interested.
> >
> >
> >
> >
> >
> > Good to know that you already have a SRIOV implementation. I found out
> some
> > information online about the mlnx plugin, but need more time to get to
> know
> > it better. And certainly I'm interested in knowing its details.
> >
> >
> >
> > The PCI pass-through networking support is planned to be discussed
> during
> > the summit: http://summit.openstack.org/cfp/details/129. I think it's
> worth
> > to drill down into more detailed proposal and present it during the
> summit,
> > especially since it impacts both nova and neutron projects.
> >
> >
> >
> > I agree. Maybe we can steal some time in that discussion.
> >
> >
> >
> > Would you be interested in collaboration on this effort? Would you be
> > interested to exchange more emails or set an IRC/WebEx meeting during
> this
> > week before the summit?
> >
> >
> >
> > Sure. If folks want to discuss it before the summit, we can schedule a
> webex
> > later this week. Or otherwise, we can continue the discussion with email.
> >
> >
> >
> >
> >
> >
> >
> > Regards,
> >
> > Irena
> >
> >
> >
> > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > Sent: Friday, October 25, 2013 11:16 PM
> > To: prashant.upadhyaya at aricent.com; Irena Berezovsky;
> > yunhong.jiang at intel.com; chris.friesen at windriver.com;
> yongli.he at intel.com
> > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> Mestery
> > (kmestery); Sandhya Dasu (sadasu)
> > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> > support
> >
> >
> >
> > Hi Irena,
> >
> >
> >
> > This is Robert Li from Cisco Systems. Recently, I was tasked to investigate
> > such support for Cisco's systems that support VM-FEX, which is a SRIOV
> > technology supporting 802-1Qbh. I was able to bring up nova instances
> with
> > SRIOV interfaces, and establish networking in between the instances that
> > employes the SRIOV interfaces. Certainly, this was accomplished with
> hacking
> > and some manual intervention. Based on this experience and my study
> with the
> > two existing nova pci-passthrough blueprints that have been
> implemented and
> > committed into Havana
> > (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
> > https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
> > registered a couple of blueprints (one on Nova side, the other on the
> > Neutron side):
> >
> >
> >
> > https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
> >
> > https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
> >
> >
> >
> > in order to address SRIOV support in openstack.
> >
> >
> >
> > Please take a look at them and see if they make sense, and let me know
> any
> > comments and questions. We can also discuss this in the summit, I
> suppose.
> >
> >
> >
> > I noticed that there is another thread on this topic, so copy those folks
> > from that thread as well.
> >
> >
> >
> > thanks,
> >
> > Robert
> >
> >
> >
> > On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com>
> wrote:
> >
> >
> >
> > Hi,
> >
> > As one of the next steps for PCI pass-through I would like to discuss is
> the
> > support for PCI pass-through vNIC.
> >
> > While nova takes care of PCI pass-through device resources
> management and
> > VIF settings, neutron should manage their networking configuration.
> >
> > I would like to register asummit proposal to discuss the support for PCI
> > pass-through networking.
> >
> > I am not sure what would be the right topic to discuss the PCI
> pass-through
> > networking, since it involve both nova and neutron.
> >
> > There is already a session registered by Yongli on nova topic to discuss
> the
> > PCI pass-through next steps.
> >
> > I think PCI pass-through networking is quite a big topic and it worth to
> > have a separate discussion.
> >
> > Is there any other people who are interested to discuss it and share
> their
> > thoughts and experience?
> >
> >
> >
> > Regards,
> >
> > Irena
> >
> >
> >
> >
> > _______________________________________________
> > OpenStack-dev mailing list
> > OpenStack-dev at lists.openstack.org
> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >

"
NOVA 1402728 - CG,msg07329,<20131030032408.GA13645@private.email.ne.jp>,"[openstack-dev] [nova] [neutron] PCI pass-through network
	support

--

Hi Yunhong.

On Tue, Oct 29, 2013 at 08:22:40PM +0000,
""Jiang, Yunhong"" <yunhong.jiang at intel.com> wrote:

> > * describe resource external to nova that is attached to VM in the API
> > (block device mapping and/or vif references)
> > * ideally the nova scheduler needs to be aware of the local capacity,
> > and how that relates to the above information (relates to the cross
> > service scheduling issues)
> 
> I think this possibly a bit different. For volume, it's sure managed by Cinder, but for PCI devices, currently
> It ;s managed by nova. So we possibly need nova to translate the information (possibly before nova scheduler).
> 
> > * state of the device should be stored by Neutron/Cinder
> > (attached/detached, capacity, IP, etc), but still exposed to the
> > ""scheduler""
> 
> I'm not sure if we can keep the state of the device in Neutron. Currently nova manage all PCI devices.

Yes, with the current implementation, nova manages PCI devices and it works.
That's great. It will remain so in Icehouse cycle (maybe also J?).

But how about long term direction?
Neutron should know/manage such network related resources on compute nodes?
The implementation in Nova will be moved into Neutron like what Cinder did?
any opinions/thoughts?
It seems that not so many Neutron developers are interested in PCI
passthrough at the moment, though.

There are use cases for this, I think.
For example, some compute nodes use OVS plugin, another nodes LB plugin.
(Right now it may not possible easily, but it will be with ML2 plugin and
mechanism driver). User wants their VMs to run on nodes with OVS plugin for
some reason(e.g. performance difference).
Such usage would be handled similarly.

Thanks,
---
Isaku Yamahata


> 
> Thanks
> --jyh
> 
> 
> > * connection params get given to Nova from Neutron/Cinder
> > * nova still has the vif driver or volume driver to make the final connection
> > * the disk should be formatted/expanded, and network info injected in
> > the same way as before (cloud-init, config drive, DHCP, etc)
> > 
> > John
> > 
> > On 29 October 2013 10:17, Irena Berezovsky <irenab at mellanox.com>
> > wrote:
> > > Hi Jiang, Robert,
> > >
> > > IRC meeting option works for me.
> > >
> > > If I understand your question below, you are looking for a way to tie up
> > > between requested virtual network(s) and requested PCI device(s). The
> > way we
> > > did it in our solution  is to map a provider:physical_network to an
> > > interface that represents the Physical Function. Every virtual network is
> > > bound to the provider:physical_network, so the PCI device should be
> > > allocated based on this mapping.  We can  map a PCI alias to the
> > > provider:physical_network.
> > >
> > >
> > >
> > > Another topic to discuss is where the mapping between neutron port
> > and PCI
> > > device should be managed. One way to solve it, is to propagate the
> > allocated
> > > PCI device details to neutron on port creation.
> > >
> > > In case  there is no qbg/qbh support, VF networking configuration
> > should be
> > > applied locally on the Host.
> > >
> > > The question is when and how to apply networking configuration on the
> > PCI
> > > device?
> > >
> > > We see the following options:
> > >
> > > *         it can be done on port creation.
> > >
> > > *         It can be done when nova VIF driver is called for vNIC
> > plugging.
> > > This will require to  have all networking configuration available to the
> > VIF
> > > driver or send request to the neutron server to obtain it.
> > >
> > > *         It can be done by  having a dedicated L2 neutron agent on
> > each
> > > Host that scans for allocated PCI devices  and then retrieves networking
> > > configuration from the server and configures the device. The agent will
> > be
> > > also responsible for managing update requests coming from the neutron
> > > server.
> > >
> > >
> > >
> > > For macvtap vNIC type assignment, the networking configuration can be
> > > applied by a dedicated L2 neutron agent.
> > >
> > >
> > >
> > > BR,
> > >
> > > Irena
> > >
> > >
> > >
> > > From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
> > > Sent: Tuesday, October 29, 2013 9:04 AM
> > >
> > >
> > > To: Robert Li (baoli); Irena Berezovsky;
> > prashant.upadhyaya at aricent.com;
> > > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > >
> > >
> > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> > Mestery
> > > (kmestery); Sandhya Dasu (sadasu)
> > > Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network
> > > support
> > >
> > >
> > >
> > > Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting
> > > because it's more openstack style and also can keep the minutes
> > clearly.
> > >
> > >
> > >
> > > To your flow, can you give more detailed example. For example, I can
> > > consider user specify the instance with -nic option specify a network id,
> > > and then how nova device the requirement to the PCI device? I assume
> > the
> > > network id should define the switches that the device can connect to ,
> > but
> > > how is that information translated to the PCI property requirement? Will
> > > this translation happen before the nova scheduler make host decision?
> > >
> > >
> > >
> > > Thanks
> > >
> > > --jyh
> > >
> > >
> > >
> > > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > > Sent: Monday, October 28, 2013 12:22 PM
> > > To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang, Yunhong;
> > > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> > Mestery
> > > (kmestery); Sandhya Dasu (sadasu)
> > > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> > > support
> > >
> > >
> > >
> > > Hi Irena,
> > >
> > >
> > >
> > > Thank you very much for your comments. See inline.
> > >
> > >
> > >
> > > --Robert
> > >
> > >
> > >
> > > On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com>
> > wrote:
> > >
> > >
> > >
> > > Hi Robert,
> > >
> > > Thank you very much for sharing the information regarding your efforts.
> > Can
> > > you please share your idea of the end to end flow? How do you suggest
> > to
> > > bind Nova and Neutron?
> > >
> > >
> > >
> > > The end to end flow is actually encompassed in the blueprints in a
> > nutshell.
> > > I will reiterate it in below. The binding between Nova and Neutron
> > occurs
> > > with the neutron v2 API that nova invokes in order to provision the
> > neutron
> > > services. The vif driver is responsible for plugging in an instance onto the
> > > networking setup that neutron has created on the host.
> > >
> > >
> > >
> > > Normally, one will invoke ""nova boot"" api with the -nic options to specify
> > > the nic with which the instance will be connected to the network. It
> > > currently allows net-id, fixed ip and/or port-id to be specified for the
> > > option. However, it doesn't allow one to specify special networking
> > > requirements for the instance. Thanks to the nova pci-passthrough work,
> > one
> > > can specify PCI passthrough device(s) in the nova flavor. But it doesn't
> > > provide means to tie up these PCI devices in the case of ethernet
> > adpators
> > > with networking services. Therefore the idea is actually simple as
> > indicated
> > > by the blueprint titles, to provide means to tie up SRIOV devices with
> > > neutron services. A work flow would roughly look like this for 'nova
> > boot':
> > >
> > >
> > >
> > >       -- Specifies networking requirements in the -nic option.
> > Specifically
> > > for SRIOV, allow the following to be specified in addition to the existing
> > > required information:
> > >
> > >                . PCI alias
> > >
> > >                . direct pci-passthrough/macvtap
> > >
> > >                . port profileid that is compliant with 802.1Qbh
> > >
> > >
> > >
> > >         The above information is optional. In the absence of them, the
> > > existing behavior remains.
> > >
> > >
> > >
> > >      -- if special networking requirements exist, Nova api creates PCI
> > > requests in the nova instance type for scheduling purpose
> > >
> > >
> > >
> > >      -- Nova scheduler schedules the instance based on the requested
> > flavor
> > > plus the PCI requests that are created for networking.
> > >
> > >
> > >
> > >      -- Nova compute invokes neutron services with PCI passthrough
> > > information if any
> > >
> > >
> > >
> > >      --  Neutron performs its normal operations based on the request,
> > such
> > > as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it
> > > should validate the information such as profileid, and stores them in its
> > > db. It's also possible to associate a port profileid with a neutron network
> > > so that port profileid becomes optional in the -nic option. Neutron
> > returns
> > > nova the port information, especially for PCI passthrough related
> > > information in the port binding object. Currently, the port binding object
> > > contains the following information:
> > >
> > >           binding:vif_type
> > >
> > >           binding:host_id
> > >
> > >           binding:profile
> > >
> > >           binding:capabilities
> > >
> > >
> > >
> > >     -- nova constructs the domain xml and plug in the instance by
> > calling
> > > the vif driver. The vif driver can build up the interface xml based on the
> > > port binding information.
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > The blueprints you registered make sense. On Nova side, there is a need
> > to
> > > bind between requested virtual network and PCI device/interface to be
> > > allocated as vNIC.
> > >
> > > On the Neutron side, there is a need to  support networking
> > configuration of
> > > the vNIC. Neutron should be able to identify the PCI device/macvtap
> > > interface in order to apply configuration. I think it makes sense to
> > provide
> > > neutron integration via dedicated Modular Layer 2 Mechanism Driver to
> > allow
> > > PCI pass-through vNIC support along with other networking technologies.
> > >
> > >
> > >
> > > I haven't sorted through this yet. A neutron port could be associated
> > with a
> > > PCI device or not, which is a common feature, IMHO. However, a ML2
> > driver
> > > may be needed specific to a particular SRIOV technology.
> > >
> > >
> > >
> > >
> > >
> > > During the Havana Release, we introduced Mellanox Neutron plugin that
> > > enables networking via SRIOV pass-through devices or macvtap
> > interfaces.
> > >
> > > We want to integrate our solution with PCI pass-through Nova support.
> > I
> > > will be glad to share more details if you are interested.
> > >
> > >
> > >
> > >
> > >
> > > Good to know that you already have a SRIOV implementation. I found out
> > some
> > > information online about the mlnx plugin, but need more time to get to
> > know
> > > it better. And certainly I'm interested in knowing its details.
> > >
> > >
> > >
> > > The PCI pass-through networking support is planned to be discussed
> > during
> > > the summit: http://summit.openstack.org/cfp/details/129. I think it's
> > worth
> > > to drill down into more detailed proposal and present it during the
> > summit,
> > > especially since it impacts both nova and neutron projects.
> > >
> > >
> > >
> > > I agree. Maybe we can steal some time in that discussion.
> > >
> > >
> > >
> > > Would you be interested in collaboration on this effort? Would you be
> > > interested to exchange more emails or set an IRC/WebEx meeting during
> > this
> > > week before the summit?
> > >
> > >
> > >
> > > Sure. If folks want to discuss it before the summit, we can schedule a
> > webex
> > > later this week. Or otherwise, we can continue the discussion with email.
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > Regards,
> > >
> > > Irena
> > >
> > >
> > >
> > > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > > Sent: Friday, October 25, 2013 11:16 PM
> > > To: prashant.upadhyaya at aricent.com; Irena Berezovsky;
> > > yunhong.jiang at intel.com; chris.friesen at windriver.com;
> > yongli.he at intel.com
> > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> > Mestery
> > > (kmestery); Sandhya Dasu (sadasu)
> > > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> > > support
> > >
> > >
> > >
> > > Hi Irena,
> > >
> > >
> > >
> > > This is Robert Li from Cisco Systems. Recently, I was tasked to investigate
> > > such support for Cisco's systems that support VM-FEX, which is a SRIOV
> > > technology supporting 802-1Qbh. I was able to bring up nova instances
> > with
> > > SRIOV interfaces, and establish networking in between the instances that
> > > employes the SRIOV interfaces. Certainly, this was accomplished with
> > hacking
> > > and some manual intervention. Based on this experience and my study
> > with the
> > > two existing nova pci-passthrough blueprints that have been
> > implemented and
> > > committed into Havana
> > > (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
> > > https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
> > > registered a couple of blueprints (one on Nova side, the other on the
> > > Neutron side):
> > >
> > >
> > >
> > > https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
> > >
> > > https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
> > >
> > >
> > >
> > > in order to address SRIOV support in openstack.
> > >
> > >
> > >
> > > Please take a look at them and see if they make sense, and let me know
> > any
> > > comments and questions. We can also discuss this in the summit, I
> > suppose.
> > >
> > >
> > >
> > > I noticed that there is another thread on this topic, so copy those folks
> > > from that thread as well.
> > >
> > >
> > >
> > > thanks,
> > >
> > > Robert
> > >
> > >
> > >
> > > On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com>
> > wrote:
> > >
> > >
> > >
> > > Hi,
> > >
> > > As one of the next steps for PCI pass-through I would like to discuss is
> > the
> > > support for PCI pass-through vNIC.
> > >
> > > While nova takes care of PCI pass-through device resources
> > management and
> > > VIF settings, neutron should manage their networking configuration.
> > >
> > > I would like to register asummit proposal to discuss the support for PCI
> > > pass-through networking.
> > >
> > > I am not sure what would be the right topic to discuss the PCI
> > pass-through
> > > networking, since it involve both nova and neutron.
> > >
> > > There is already a session registered by Yongli on nova topic to discuss
> > the
> > > PCI pass-through next steps.
> > >
> > > I think PCI pass-through networking is quite a big topic and it worth to
> > > have a separate discussion.
> > >
> > > Is there any other people who are interested to discuss it and share
> > their
> > > thoughts and experience?
> > >
> > >
> > >
> > > Regards,
> > >
> > > Irena
> > >
> > >
> > >
> > >
> > > _______________________________________________
> > > OpenStack-dev mailing list
> > > OpenStack-dev at lists.openstack.org
> > > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> > >
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

-- 
Isaku Yamahata <isaku.yamahata at gmail.com>

"
NOVA 1402728 - CG,msg07331,<DDCAE26804250545B9934A2056554AA01FBAA1D2@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through
	network	support

--



> -----Original Message-----
> From: Isaku Yamahata [mailto:isaku.yamahata at gmail.com]
> Sent: Tuesday, October 29, 2013 8:24 PM
> To: OpenStack Development Mailing List (not for usage questions)
> Cc: isaku.yamahata at gmail.com; Itzik Brown
> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
> 
> Hi Yunhong.
> 
> On Tue, Oct 29, 2013 at 08:22:40PM +0000,
> ""Jiang, Yunhong"" <yunhong.jiang at intel.com> wrote:
> 
> > > * describe resource external to nova that is attached to VM in the API
> > > (block device mapping and/or vif references)
> > > * ideally the nova scheduler needs to be aware of the local capacity,
> > > and how that relates to the above information (relates to the cross
> > > service scheduling issues)
> >
> > I think this possibly a bit different. For volume, it's sure managed by
> Cinder, but for PCI devices, currently
> > It ;s managed by nova. So we possibly need nova to translate the
> information (possibly before nova scheduler).
> >
> > > * state of the device should be stored by Neutron/Cinder
> > > (attached/detached, capacity, IP, etc), but still exposed to the
> > > ""scheduler""
> >
> > I'm not sure if we can keep the state of the device in Neutron. Currently
> nova manage all PCI devices.
> 
> Yes, with the current implementation, nova manages PCI devices and it
> works.
> That's great. It will remain so in Icehouse cycle (maybe also J?).
> 
> But how about long term direction?
> Neutron should know/manage such network related resources on
> compute nodes?

So you mean the PCI device management will be spited between Nova and Neutron? For example, non-NIC device owned by nova and NIC device owned by neutron?

There have been so many discussion of the scheduler enhancement, like https://etherpad.openstack.org/p/grizzly-split-out-scheduling , so possibly that's the right direction? Let's wait for the summit discussion.

> The implementation in Nova will be moved into Neutron like what Cinder
> did?
> any opinions/thoughts?
> It seems that not so many Neutron developers are interested in PCI
> passthrough at the moment, though.
> 
> There are use cases for this, I think.
> For example, some compute nodes use OVS plugin, another nodes LB
> plugin.
> (Right now it may not possible easily, but it will be with ML2 plugin and
> mechanism driver). User wants their VMs to run on nodes with OVS plugin
> for
> some reason(e.g. performance difference).
> Such usage would be handled similarly.
> 
> Thanks,
> ---
> Isaku Yamahata
> 
> 
> >
> > Thanks
> > --jyh
> >
> >
> > > * connection params get given to Nova from Neutron/Cinder
> > > * nova still has the vif driver or volume driver to make the final
> connection
> > > * the disk should be formatted/expanded, and network info injected in
> > > the same way as before (cloud-init, config drive, DHCP, etc)
> > >
> > > John
> > >
> > > On 29 October 2013 10:17, Irena Berezovsky
> <irenab at mellanox.com>
> > > wrote:
> > > > Hi Jiang, Robert,
> > > >
> > > > IRC meeting option works for me.
> > > >
> > > > If I understand your question below, you are looking for a way to tie
> up
> > > > between requested virtual network(s) and requested PCI device(s).
> The
> > > way we
> > > > did it in our solution  is to map a provider:physical_network to an
> > > > interface that represents the Physical Function. Every virtual
> network is
> > > > bound to the provider:physical_network, so the PCI device should
> be
> > > > allocated based on this mapping.  We can  map a PCI alias to the
> > > > provider:physical_network.
> > > >
> > > >
> > > >
> > > > Another topic to discuss is where the mapping between neutron
> port
> > > and PCI
> > > > device should be managed. One way to solve it, is to propagate the
> > > allocated
> > > > PCI device details to neutron on port creation.
> > > >
> > > > In case  there is no qbg/qbh support, VF networking configuration
> > > should be
> > > > applied locally on the Host.
> > > >
> > > > The question is when and how to apply networking configuration on
> the
> > > PCI
> > > > device?
> > > >
> > > > We see the following options:
> > > >
> > > > *         it can be done on port creation.
> > > >
> > > > *         It can be done when nova VIF driver is called for vNIC
> > > plugging.
> > > > This will require to  have all networking configuration available to
> the
> > > VIF
> > > > driver or send request to the neutron server to obtain it.
> > > >
> > > > *         It can be done by  having a dedicated L2 neutron agent
> on
> > > each
> > > > Host that scans for allocated PCI devices  and then retrieves
> networking
> > > > configuration from the server and configures the device. The agent
> will
> > > be
> > > > also responsible for managing update requests coming from the
> neutron
> > > > server.
> > > >
> > > >
> > > >
> > > > For macvtap vNIC type assignment, the networking configuration can
> be
> > > > applied by a dedicated L2 neutron agent.
> > > >
> > > >
> > > >
> > > > BR,
> > > >
> > > > Irena
> > > >
> > > >
> > > >
> > > > From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
> > > > Sent: Tuesday, October 29, 2013 9:04 AM
> > > >
> > > >
> > > > To: Robert Li (baoli); Irena Berezovsky;
> > > prashant.upadhyaya at aricent.com;
> > > > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > > >
> > > >
> > > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen);
> Kyle
> > > Mestery
> > > > (kmestery); Sandhya Dasu (sadasu)
> > > > Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through
> network
> > > > support
> > > >
> > > >
> > > >
> > > > Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting
> > > > because it's more openstack style and also can keep the minutes
> > > clearly.
> > > >
> > > >
> > > >
> > > > To your flow, can you give more detailed example. For example, I can
> > > > consider user specify the instance with -nic option specify a network
> id,
> > > > and then how nova device the requirement to the PCI device? I
> assume
> > > the
> > > > network id should define the switches that the device can connect
> to ,
> > > but
> > > > how is that information translated to the PCI property requirement?
> Will
> > > > this translation happen before the nova scheduler make host
> decision?
> > > >
> > > >
> > > >
> > > > Thanks
> > > >
> > > > --jyh
> > > >
> > > >
> > > >
> > > > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > > > Sent: Monday, October 28, 2013 12:22 PM
> > > > To: Irena Berezovsky; prashant.upadhyaya at aricent.com; Jiang,
> Yunhong;
> > > > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen);
> Kyle
> > > Mestery
> > > > (kmestery); Sandhya Dasu (sadasu)
> > > > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through
> network
> > > > support
> > > >
> > > >
> > > >
> > > > Hi Irena,
> > > >
> > > >
> > > >
> > > > Thank you very much for your comments. See inline.
> > > >
> > > >
> > > >
> > > > --Robert
> > > >
> > > >
> > > >
> > > > On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com>
> > > wrote:
> > > >
> > > >
> > > >
> > > > Hi Robert,
> > > >
> > > > Thank you very much for sharing the information regarding your
> efforts.
> > > Can
> > > > you please share your idea of the end to end flow? How do you
> suggest
> > > to
> > > > bind Nova and Neutron?
> > > >
> > > >
> > > >
> > > > The end to end flow is actually encompassed in the blueprints in a
> > > nutshell.
> > > > I will reiterate it in below. The binding between Nova and Neutron
> > > occurs
> > > > with the neutron v2 API that nova invokes in order to provision the
> > > neutron
> > > > services. The vif driver is responsible for plugging in an instance onto
> the
> > > > networking setup that neutron has created on the host.
> > > >
> > > >
> > > >
> > > > Normally, one will invoke ""nova boot"" api with the -nic options to
> specify
> > > > the nic with which the instance will be connected to the network. It
> > > > currently allows net-id, fixed ip and/or port-id to be specified for the
> > > > option. However, it doesn't allow one to specify special networking
> > > > requirements for the instance. Thanks to the nova pci-passthrough
> work,
> > > one
> > > > can specify PCI passthrough device(s) in the nova flavor. But it
> doesn't
> > > > provide means to tie up these PCI devices in the case of ethernet
> > > adpators
> > > > with networking services. Therefore the idea is actually simple as
> > > indicated
> > > > by the blueprint titles, to provide means to tie up SRIOV devices with
> > > > neutron services. A work flow would roughly look like this for 'nova
> > > boot':
> > > >
> > > >
> > > >
> > > >       -- Specifies networking requirements in the -nic option.
> > > Specifically
> > > > for SRIOV, allow the following to be specified in addition to the
> existing
> > > > required information:
> > > >
> > > >                . PCI alias
> > > >
> > > >                . direct pci-passthrough/macvtap
> > > >
> > > >                . port profileid that is compliant with 802.1Qbh
> > > >
> > > >
> > > >
> > > >         The above information is optional. In the absence of them,
> the
> > > > existing behavior remains.
> > > >
> > > >
> > > >
> > > >      -- if special networking requirements exist, Nova api creates
> PCI
> > > > requests in the nova instance type for scheduling purpose
> > > >
> > > >
> > > >
> > > >      -- Nova scheduler schedules the instance based on the
> requested
> > > flavor
> > > > plus the PCI requests that are created for networking.
> > > >
> > > >
> > > >
> > > >      -- Nova compute invokes neutron services with PCI
> passthrough
> > > > information if any
> > > >
> > > >
> > > >
> > > >      --  Neutron performs its normal operations based on the
> request,
> > > such
> > > > as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it
> > > > should validate the information such as profileid, and stores them in
> its
> > > > db. It's also possible to associate a port profileid with a neutron
> network
> > > > so that port profileid becomes optional in the -nic option. Neutron
> > > returns
> > > > nova the port information, especially for PCI passthrough related
> > > > information in the port binding object. Currently, the port binding
> object
> > > > contains the following information:
> > > >
> > > >           binding:vif_type
> > > >
> > > >           binding:host_id
> > > >
> > > >           binding:profile
> > > >
> > > >           binding:capabilities
> > > >
> > > >
> > > >
> > > >     -- nova constructs the domain xml and plug in the instance by
> > > calling
> > > > the vif driver. The vif driver can build up the interface xml based on
> the
> > > > port binding information.
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > The blueprints you registered make sense. On Nova side, there is a
> need
> > > to
> > > > bind between requested virtual network and PCI device/interface to
> be
> > > > allocated as vNIC.
> > > >
> > > > On the Neutron side, there is a need to  support networking
> > > configuration of
> > > > the vNIC. Neutron should be able to identify the PCI device/macvtap
> > > > interface in order to apply configuration. I think it makes sense to
> > > provide
> > > > neutron integration via dedicated Modular Layer 2 Mechanism
> Driver to
> > > allow
> > > > PCI pass-through vNIC support along with other networking
> technologies.
> > > >
> > > >
> > > >
> > > > I haven't sorted through this yet. A neutron port could be associated
> > > with a
> > > > PCI device or not, which is a common feature, IMHO. However, a
> ML2
> > > driver
> > > > may be needed specific to a particular SRIOV technology.
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > During the Havana Release, we introduced Mellanox Neutron plugin
> that
> > > > enables networking via SRIOV pass-through devices or macvtap
> > > interfaces.
> > > >
> > > > We want to integrate our solution with PCI pass-through Nova
> support.
> > > I
> > > > will be glad to share more details if you are interested.
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > Good to know that you already have a SRIOV implementation. I found
> out
> > > some
> > > > information online about the mlnx plugin, but need more time to get
> to
> > > know
> > > > it better. And certainly I'm interested in knowing its details.
> > > >
> > > >
> > > >
> > > > The PCI pass-through networking support is planned to be discussed
> > > during
> > > > the summit: http://summit.openstack.org/cfp/details/129. I think it's
> > > worth
> > > > to drill down into more detailed proposal and present it during the
> > > summit,
> > > > especially since it impacts both nova and neutron projects.
> > > >
> > > >
> > > >
> > > > I agree. Maybe we can steal some time in that discussion.
> > > >
> > > >
> > > >
> > > > Would you be interested in collaboration on this effort? Would you
> be
> > > > interested to exchange more emails or set an IRC/WebEx meeting
> during
> > > this
> > > > week before the summit?
> > > >
> > > >
> > > >
> > > > Sure. If folks want to discuss it before the summit, we can schedule a
> > > webex
> > > > later this week. Or otherwise, we can continue the discussion with
> email.
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > Regards,
> > > >
> > > > Irena
> > > >
> > > >
> > > >
> > > > From: Robert Li (baoli) [mailto:baoli at cisco.com]
> > > > Sent: Friday, October 25, 2013 11:16 PM
> > > > To: prashant.upadhyaya at aricent.com; Irena Berezovsky;
> > > > yunhong.jiang at intel.com; chris.friesen at windriver.com;
> > > yongli.he at intel.com
> > > > Cc: OpenStack Development Mailing List; Brian Bowen (brbowen);
> Kyle
> > > Mestery
> > > > (kmestery); Sandhya Dasu (sadasu)
> > > > Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through
> network
> > > > support
> > > >
> > > >
> > > >
> > > > Hi Irena,
> > > >
> > > >
> > > >
> > > > This is Robert Li from Cisco Systems. Recently, I was tasked to
> investigate
> > > > such support for Cisco's systems that support VM-FEX, which is a
> SRIOV
> > > > technology supporting 802-1Qbh. I was able to bring up nova
> instances
> > > with
> > > > SRIOV interfaces, and establish networking in between the instances
> that
> > > > employes the SRIOV interfaces. Certainly, this was accomplished with
> > > hacking
> > > > and some manual intervention. Based on this experience and my
> study
> > > with the
> > > > two existing nova pci-passthrough blueprints that have been
> > > implemented and
> > > > committed into Havana
> > > > (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base
> and
> > > >
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
> > > > registered a couple of blueprints (one on Nova side, the other on the
> > > > Neutron side):
> > > >
> > > >
> > > >
> > > > https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
> > > >
> > > >
> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
> > > >
> > > >
> > > >
> > > > in order to address SRIOV support in openstack.
> > > >
> > > >
> > > >
> > > > Please take a look at them and see if they make sense, and let me
> know
> > > any
> > > > comments and questions. We can also discuss this in the summit, I
> > > suppose.
> > > >
> > > >
> > > >
> > > > I noticed that there is another thread on this topic, so copy those
> folks
> > > > from that thread as well.
> > > >
> > > >
> > > >
> > > > thanks,
> > > >
> > > > Robert
> > > >
> > > >
> > > >
> > > > On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com>
> > > wrote:
> > > >
> > > >
> > > >
> > > > Hi,
> > > >
> > > > As one of the next steps for PCI pass-through I would like to discuss is
> > > the
> > > > support for PCI pass-through vNIC.
> > > >
> > > > While nova takes care of PCI pass-through device resources
> > > management and
> > > > VIF settings, neutron should manage their networking configuration.
> > > >
> > > > I would like to register asummit proposal to discuss the support for
> PCI
> > > > pass-through networking.
> > > >
> > > > I am not sure what would be the right topic to discuss the PCI
> > > pass-through
> > > > networking, since it involve both nova and neutron.
> > > >
> > > > There is already a session registered by Yongli on nova topic to
> discuss
> > > the
> > > > PCI pass-through next steps.
> > > >
> > > > I think PCI pass-through networking is quite a big topic and it worth to
> > > > have a separate discussion.
> > > >
> > > > Is there any other people who are interested to discuss it and share
> > > their
> > > > thoughts and experience?
> > > >
> > > >
> > > >
> > > > Regards,
> > > >
> > > > Irena
> > > >
> > > >
> > > >
> > > >
> > > > _______________________________________________
> > > > OpenStack-dev mailing list
> > > > OpenStack-dev at lists.openstack.org
> > > > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> > > >
> >
> > _______________________________________________
> > OpenStack-dev mailing list
> > OpenStack-dev at lists.openstack.org
> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 
> --
> Isaku Yamahata <isaku.yamahata at gmail.com>
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg07381,<20131030124445.GA25141@private.email.ne.jp>,"[openstack-dev] [nova] [neutron] PCI pass-through
	network	support

--

On Wed, Oct 30, 2013 at 04:14:40AM +0000,
""Jiang, Yunhong"" <yunhong.jiang at intel.com> wrote:

> > But how about long term direction?
> > Neutron should know/manage such network related resources on
> > compute nodes?
> 
> So you mean the PCI device management will be spited between Nova and Neutron? For example, non-NIC device owned by nova and NIC device owned by neutron?

Yes. But I'd like to hear from other Neutron developers.


> There have been so many discussion of the scheduler enhancement, like https://etherpad.openstack.org/p/grizzly-split-out-scheduling , so possibly that's the right direction? Let's wait for the summit discussion.

Interesting. Yeah, I look forward for the summit discussion.
Let's try to involve not only Nova developers, but also other Neutron
developers.

thanks,
-- 
Isaku Yamahata <isaku.yamahata at gmail.com>

"
NOVA 1402728 - CG,msg07238,<1705659276F6A540A34DBC19195799841027C398@xmb-rcd-x03.cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi,

sounds like there are enough interests for an IRC meeting before the summit. Do you guys know how to schedule a #openstack IRC meeting?

thanks,
Robert

On 10/29/13 6:17 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Jiang, Robert,
IRC meeting option works for me.
If I understand your question below, you are looking for a way to tie up between requested virtual network(s) and requested PCI device(s). The way we did it in our solution  is to map a provider:physical_network to an interface that represents the Physical Function. Every virtual network is bound to the provider:physical_network, so the PCI device should be allocated based on this mapping.  We can  map a PCI alias to the provider:physical_network.

Another topic to discuss is where the mapping between neutron port and PCI device should be managed. One way to solve it, is to propagate the allocated PCI device details to neutron on port creation.
In case  there is no qbg/qbh support, VF networking configuration should be applied locally on the Host.
The question is when and how to apply networking configuration on the PCI device?
We see the following options:

?         it can be done on port creation.

?         It can be done when nova VIF driver is called for vNIC plugging. This will require to  have all networking configuration available to the VIF driver or send request to the neutron server to obtain it.

?         It can be done by  having a dedicated L2 neutron agent on each Host that scans for allocated PCI devices  and then retrieves networking configuration from the server and configures the device. The agent will be also responsible for managing update requests coming from the neutron server.


For macvtap vNIC type assignment, the networking configuration can be applied by a dedicated L2 neutron agent.

BR,
Irena

From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
Sent: Tuesday, October 29, 2013 9:04 AM

To: Robert Li (baoli); Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network support

Robert, is it possible to have a IRC meeting? I?d prefer to IRC meeting because it?s more openstack style and also can keep the minutes clearly.

To your flow, can you give more detailed example. For example, I can consider user specify the instance with ?nic option specify a network id, and then how nova device the requirement to the PCI device? I assume the network id should define the switches that the device can connect to , but how is that information translated to the PCI property requirement? Will this translation happen before the nova scheduler make host decision?

Thanks
--jyh

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, October 28, 2013 12:22 PM
To: Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the ?nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the ?nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the ?nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it?s worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/708617fc/attachment-0001.html>
"
NOVA 1402728 - CG,msg07289,<DDCAE26804250545B9934A2056554AA01FBA9CF3@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Your explanation of the virtual network and physical network is quite clear and should work well. We need change nova code to achieve it, including get the physical network for the virtual network, passing the physical network requirement to the filter properties etc.

For your port method, so you mean we are sure to passing network id to 'nova boot' and nova will create the port during VM boot, am I right?  Also, how can nova knows that it need allocate the PCI device for the port? I'd suppose that in SR-IOV NIC environment, user don't need specify the PCI requirement. Instead, the PCI requirement should come from the network configuration and image property. Or you think user still need passing flavor with pci request?

--jyh


From: Irena Berezovsky [mailto:irenab at mellanox.com]
Sent: Tuesday, October 29, 2013 3:17 AM
To: Jiang, Yunhong; Robert Li (baoli); prashant.upadhyaya at aricent.com; chris.friesen at windriver.com; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Jiang, Robert,
IRC meeting option works for me.
If I understand your question below, you are looking for a way to tie up between requested virtual network(s) and requested PCI device(s). The way we did it in our solution  is to map a provider:physical_network to an interface that represents the Physical Function. Every virtual network is bound to the provider:physical_network, so the PCI device should be allocated based on this mapping.  We can  map a PCI alias to the provider:physical_network.

Another topic to discuss is where the mapping between neutron port and PCI device should be managed. One way to solve it, is to propagate the allocated PCI device details to neutron on port creation.
In case  there is no qbg/qbh support, VF networking configuration should be applied locally on the Host.
The question is when and how to apply networking configuration on the PCI device?
We see the following options:

*         it can be done on port creation.

*         It can be done when nova VIF driver is called for vNIC plugging. This will require to  have all networking configuration available to the VIF driver or send request to the neutron server to obtain it.

*         It can be done by  having a dedicated L2 neutron agent on each Host that scans for allocated PCI devices  and then retrieves networking configuration from the server and configures the device. The agent will be also responsible for managing update requests coming from the neutron server.


For macvtap vNIC type assignment, the networking configuration can be applied by a dedicated L2 neutron agent.

BR,
Irena

From: Jiang, Yunhong [mailto:yunhong.jiang at intel.com]
Sent: Tuesday, October 29, 2013 9:04 AM

To: Robert Li (baoli); Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: RE: [openstack-dev] [nova] [neutron] PCI pass-through network support

Robert, is it possible to have a IRC meeting? I'd prefer to IRC meeting because it's more openstack style and also can keep the minutes clearly.

To your flow, can you give more detailed example. For example, I can consider user specify the instance with -nic option specify a network id, and then how nova device the requirement to the PCI device? I assume the network id should define the switches that the device can connect to , but how is that information translated to the PCI property requirement? Will this translation happen before the nova scheduler make host decision?

Thanks
--jyh

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, October 28, 2013 12:22 PM
To: Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the -nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the -nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the -nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it's worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/3cd9b1d3/attachment.html>
"
NOVA 1402728 - CG,msg07240,<1705659276F6A540A34DBC19195799841027C380@xmb-rcd-x03.cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
 support

--

Hi Yunhong,

I haven't looked at Mellanox in much detail. I think that we'll get more details from Irena down the road. Regarding your question, I can only answer based on my experience with Cisco's VM-FEX. In a nutshell:
     -- a vNIC is connected to an external switch. Once the host is booted up, all the PFs and VFs provisioned on the vNIC will be created, as well as all the corresponding ethernet interfaces .
     -- As far as Neutron is concerned, a neutron port can be associated with a VF. One way to do so is to specify this requirement in the ?nic option, providing information such as:
               . PCI alias (this is the same alias as defined in your nova blueprints)
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh
     -- similar to how you translate the nova flavor with PCI requirements to PCI requests for scheduling purpose, Nova API (the nova api component) can translate the above to PCI requests for scheduling purpose. I can give more detail later on this.

Regarding your last question, since the vNIC is already connected with the external switch, the vNIC driver will be responsible for communicating the port profile to the external switch. As you have already known, libvirt provides several ways to specify a VM to be booted up with SRIOV. For example, in the following interface definition:


  <interface type='hostdev' managed='yes'>
      <source>
        <address type='pci' domain='0' bus='0x09' slot='0x0' function='0x01'/>
      </source>
      <mac address='01:23:45:67:89:ab' />
      <virtualport type='802.1Qbh'>
        <parameters profileid='my-port-profile' />
      </virtualport>
    </interface>


The SRIOV VF (bus 0x09, VF 0x01) will be allocated, and the port profile 'my-port-profile' will be used to provision this VF. Libvirt will be responsible for invoking the vNIC driver to configure this VF with the port profile my-port-porfile. The driver will talk to the external switch using the 802.1qbh standards to complete the VF's configuration and binding with the VM.


Now that nova PCI passthrough is responsible for discovering/scheduling/allocating a VF, the rest of the puzzle is to associate this PCI device with the feature that's going to use it, and the feature will be responsible for configuring it. You can also see from the above example, in one implementation of SRIOV, the feature (in this case neutron) may not need to do much in terms of working with the external switch, the work is actually done by libvirt behind the scene.


Now the questions are:

        -- how the port profile gets defined/managed

        -- how the port profile gets associated with a neutron network

The first question will be specific to the particular product, and therefore a particular neutron plugin has to mange that.

There may be several approaches to address the second question. For example, in the simplest case, a port profile can be associated with a neutron network. This has some significant drawbacks. Since the port profile defines features for all the ports that use it, the one port profile to one neutron network mapping would mean all the ports on the network will have exactly the same features (for example, QoS characteristics). To make it flexible, the binding of a port profile to a port may be done at the port creation time.


Let me know if the above answered your question.


thanks,

Robert

On 10/29/13 3:03 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>> wrote:

Robert, is it possible to have a IRC meeting? I?d prefer to IRC meeting because it?s more openstack style and also can keep the minutes clearly.

To your flow, can you give more detailed example. For example, I can consider user specify the instance with ?nic option specify a network id, and then how nova device the requirement to the PCI device? I assume the network id should define the switches that the device can connect to , but how is that information translated to the PCI property requirement? Will this translation happen before the nova scheduler make host decision?

Thanks
--jyh

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Monday, October 28, 2013 12:22 PM
To: Irena Berezovsky; prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; He, Yongli; Itzik Brown
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

Thank you very much for your comments. See inline.

--Robert

On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi Robert,
Thank you very much for sharing the information regarding your efforts. Can you please share your idea of the end to end flow? How do you suggest  to bind Nova and Neutron?

The end to end flow is actually encompassed in the blueprints in a nutshell. I will reiterate it in below. The binding between Nova and Neutron occurs with the neutron v2 API that nova invokes in order to provision the neutron services. The vif driver is responsible for plugging in an instance onto the networking setup that neutron has created on the host.

Normally, one will invoke ""nova boot"" api with the ?nic options to specify the nic with which the instance will be connected to the network. It currently allows net-id, fixed ip and/or port-id to be specified for the option. However, it doesn't allow one to specify special networking requirements for the instance. Thanks to the nova pci-passthrough work, one can specify PCI passthrough device(s) in the nova flavor. But it doesn't provide means to tie up these PCI devices in the case of ethernet adpators with networking services. Therefore the idea is actually simple as indicated by the blueprint titles, to provide means to tie up SRIOV devices with neutron services. A work flow would roughly look like this for 'nova boot':

      -- Specifies networking requirements in the ?nic option. Specifically for SRIOV, allow the following to be specified in addition to the existing required information:
               . PCI alias
               . direct pci-passthrough/macvtap
               . port profileid that is compliant with 802.1Qbh

        The above information is optional. In the absence of them, the existing behavior remains.

     -- if special networking requirements exist, Nova api creates PCI requests in the nova instance type for scheduling purpose

     -- Nova scheduler schedules the instance based on the requested flavor plus the PCI requests that are created for networking.

     -- Nova compute invokes neutron services with PCI passthrough information if any

     --  Neutron performs its normal operations based on the request, such as allocating a port, assigning ip addresses, etc. Specific to SRIOV, it should validate the information such as profileid, and stores them in its db. It's also possible to associate a port profileid with a neutron network so that port profileid becomes optional in the ?nic option. Neutron returns  nova the port information, especially for PCI passthrough related information in the port binding object. Currently, the port binding object contains the following information:
          binding:vif_type
          binding:host_id
          binding:profile
          binding:capabilities

    -- nova constructs the domain xml and plug in the instance by calling the vif driver. The vif driver can build up the interface xml based on the port binding information.




The blueprints you registered make sense. On Nova side, there is a need to bind between requested virtual network and PCI device/interface to be allocated as vNIC.
On the Neutron side, there is a need to  support networking configuration of the vNIC. Neutron should be able to identify the PCI device/macvtap interface in order to apply configuration. I think it makes sense to provide neutron integration via dedicated Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC support along with other networking technologies.

I haven't sorted through this yet. A neutron port could be associated with a PCI device or not, which is a common feature, IMHO. However, a ML2 driver may be needed specific to a particular SRIOV technology.


During the Havana Release, we introduced Mellanox Neutron plugin that enables networking via SRIOV pass-through devices or macvtap interfaces.
We want to integrate our solution with PCI pass-through Nova support.  I will be glad to share more details if you are interested.


Good to know that you already have a SRIOV implementation. I found out some information online about the mlnx plugin, but need more time to get to know it better. And certainly I'm interested in knowing its details.

The PCI pass-through networking support is planned to be discussed during the summit: http://summit.openstack.org/cfp/details/129. I think it?s worth to drill down into more detailed proposal and present it during the summit, especially since it impacts both nova and neutron projects.

I agree. Maybe we can steal some time in that discussion.

Would you be interested in collaboration on this effort? Would you be interested to exchange more emails or set an IRC/WebEx meeting during this week before the summit?

Sure. If folks want to discuss it before the summit, we can schedule a webex later this week. Or otherwise, we can continue the discussion with email.



Regards,
Irena

From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Friday, October 25, 2013 11:16 PM
To: prashant.upadhyaya at aricent.com<mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky; yunhong.jiang at intel.com<mailto:yunhong.jiang at intel.com>; chris.friesen at windriver.com<mailto:chris.friesen at windriver.com>; yongli.he at intel.com<mailto:yongli.he at intel.com>
Cc: OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network support

Hi Irena,

This is Robert Li from Cisco Systems. Recently, I was tasked to investigate such support for Cisco's systems that support VM-FEX, which is a SRIOV technology supporting 802-1Qbh. I was able to bring up nova instances with SRIOV interfaces, and establish networking in between the instances that employes the SRIOV interfaces. Certainly, this was accomplished with hacking and some manual intervention. Based on this experience and my study with the two existing nova pci-passthrough blueprints that have been implemented and committed into Havana (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I registered a couple of blueprints (one on Nova side, the other on the Neutron side):

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov

in order to address SRIOV support in openstack.

Please take a look at them and see if they make sense, and let me know any comments and questions. We can also discuss this in the summit, I suppose.

I noticed that there is another thread on this topic, so copy those folks  from that thread as well.

thanks,
Robert

On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com<mailto:irenab at mellanox.com>> wrote:

Hi,
As one of the next steps for PCI pass-through I would like to discuss is the support for PCI pass-through vNIC.
While nova takes care of PCI pass-through device resources  management and VIF settings, neutron should manage their networking configuration.
I would like to register asummit proposal to discuss the support for PCI pass-through networking.
I am not sure what would be the right topic to discuss the PCI pass-through networking, since it involve both nova and neutron.
There is already a session registered by Yongli on nova topic to discuss the PCI pass-through next steps.
I think PCI pass-through networking is quite a big topic and it worth to have a separate discussion.
Is there any other people who are interested to discuss it and share their thoughts and experience?

Regards,
Irena

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131029/ebc194e2/attachment.html>
"
NOVA 1402728 - CG,msg07247,<526FCFE4.5010003@cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through network
	support

--

Lots of great info and discussion going on here.

One additional thing I would like to mention is regarding PF and VF usage.

Normally VFs will be assigned to instances, and the PF will either not be
used at all, or maybe some agent in the host of the compute node might have
access to the PF for something (management?).

There is a neutron design track around the development of ""service VMs"".
These are dedicated instances that run neutron services like routers,
firewalls, etc. It is plausible that a service VM would like to use PCI
passthrough and get the entire PF. This would allow it to have complete
control over a physical link, which I think will be wanted in some cases.

-- 
Henry

On Tue, Oct 29, at 10:23 am, Irena Berezovsky <irenab at mellanox.com> wrote:

> Hi,
> 
> I would like to share some details regarding the support provided by
> Mellanox plugin. It enables networking via SRIOV pass-through devices or
> macvtap interfaces.  It plugin is available here:
> https://github.com/openstack/neutron/tree/master/neutron/plugins/mlnx.
> 
> To support either PCI pass-through device and macvtap interface type of
> vNICs, we set neutron port profile:vnic_type according to the required VIF
> type and then use the created port to ?nova boot? the VM.
> 
> To  overcome the missing scheduler awareness for PCI devices which was not
> part of the Havana release yet, we
> 
> have an additional service (embedded switch Daemon) that runs on each
> compute node.  
> 
> This service manages the SRIOV resources allocation,  answers vNICs
> discovery queries and applies VLAN/MAC configuration using standard Linux
> APIs (code is here: https://github.com/mellanox-openstack/mellanox-eswitchd
> ).  The embedded switch Daemon serves as a glue layer between VIF Driver and
> Neutron Agent.
> 
> In the Icehouse Release when SRIOV resources allocation is already part of
> the Nova, we plan to eliminate the need in embedded switch daemon service.
> So what is left to figure out is how to tie up between neutron port and PCI
> device and invoke networking configuration.
> 
>  
> 
> In our case what we have is actually the Hardware VEB that is not programmed
> via either 802.1Qbg or 802.1Qbh, but configured locally by Neutron Agent. We
> also support both Ethernet and InfiniBand physical network L2 technology.
> This means that we apply different configuration commands  to set
> configuration on VF.
> 
>  
> 
> I guess what we have to figure out is how to support the generic case for
> the PCI device networking support, for HW VEB, 802.1Qbg and 802.1Qbh cases.
> 
>  
> 
> BR,
> 
> Irena
> 
>  
> 
> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
> *Sent:* Tuesday, October 29, 2013 3:31 PM
> *To:* Jiang, Yunhong; Irena Berezovsky; prashant.upadhyaya at aricent.com;
> chris.friesen at windriver.com; He, Yongli; Itzik Brown
> *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
> Mestery (kmestery); Sandhya Dasu (sadasu)
> *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through network support
> 
>  
> 
> Hi Yunhong,
> 
>  
> 
> I haven't looked at Mellanox in much detail. I think that we'll get more
> details from Irena down the road. Regarding your question, I can only answer
> based on my experience with Cisco's VM-FEX. In a nutshell:
> 
>      -- a vNIC is connected to an external switch. Once the host is booted
> up, all the PFs and VFs provisioned on the vNIC will be created, as well as
> all the corresponding ethernet interfaces . 
> 
>      -- As far as Neutron is concerned, a neutron port can be associated
> with a VF. One way to do so is to specify this requirement in the ?nic
> option, providing information such as:
> 
>                . PCI alias (this is the same alias as defined in your nova
> blueprints)
> 
>                . direct pci-passthrough/macvtap
> 
>                . port profileid that is compliant with 802.1Qbh
> 
>      -- similar to how you translate the nova flavor with PCI requirements
> to PCI requests for scheduling purpose, Nova API (the nova api component)
> can translate the above to PCI requests for scheduling purpose. I can give
> more detail later on this. 
> 
>  
> 
> Regarding your last question, since the vNIC is already connected with the
> external switch, the vNIC driver will be responsible for communicating the
> port profile to the external switch. As you have already known, libvirt
> provides several ways to specify a VM to be booted up with SRIOV. For
> example, in the following interface definition: 
> 
>       
> 
>   *<interface type='hostdev' managed='yes'>*
> 
> *      <source>*
> 
> *        <address type='pci' domain='0' bus='0x09' slot='0x0' function='0x01'/>*
> 
> *      </source>*
> 
> *      <mac address='01:23:45:67:89:ab' />*
> 
> *      <virtualport type='802.1Qbh'>*
> 
> *        <parameters profileid='my-port-profile' />*
> 
> *      </virtualport>*
> 
> *    </interface>*
> 
>  
> 
> The SRIOV VF (bus 0x09, VF 0x01) will be allocated, and the port profile 'my-port-profile' will be used to provision this VF. Libvirt will be responsible for invoking the vNIC driver to configure this VF with the port profile my-port-porfile. The driver will talk to the external switch using the 802.1qbh standards to complete the VF's configuration and binding with the VM.
> 
>  
> 
> Now that nova PCI passthrough is responsible for discovering/scheduling/allocating a VF, the rest of the puzzle is to associate this PCI device with the feature that's going to use it, and the feature will be responsible for configuring it. You can also see from the above example, in one implementation of SRIOV, the feature (in this case neutron) may not need to do much in terms of working with the external switch, the work is actually done by libvirt behind the scene. 
> 
>  
> 
> Now the questions are:
> 
>         -- how the port profile gets defined/managed
> 
>         -- how the port profile gets associated with a neutron network
> 
> The first question will be specific to the particular product, and therefore a particular neutron plugin has to mange that. 
> 
> There may be several approaches to address the second question. For example, in the simplest case, a port profile can be associated with a neutron network. This has some significant drawbacks. Since the port profile defines features for all the ports that use it, the one port profile to one neutron network mapping would mean all the ports on the network will have exactly the same features (for example, QoS characteristics). To make it flexible, the binding of a port profile to a port may be done at the port creation time.
> 
>  
> 
> Let me know if the above answered your question.
> 
>  
> 
> thanks,
> 
> Robert
> 
>  
> 
> On 10/29/13 3:03 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com
> <mailto:yunhong.jiang at intel.com>> wrote:
> 
>  
> 
>     Robert, is it possible to have a IRC meeting? I?d prefer to IRC meeting
>     because it?s more openstack style and also can keep the minutes clearly.
> 
>      
> 
>     To your flow, can you give more detailed example. For example, I can
>     consider user specify the instance with ?nic option specify a network
>     id, and then how nova device the requirement to the PCI device? I assume
>     the network id should define the switches that the device can connect to
>     , but how is that information translated to the PCI property
>     requirement? Will this translation happen before the nova scheduler make
>     host decision?
> 
>      
> 
>     Thanks
> 
>     --jyh
> 
>      
> 
>     *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>     *Sent:* Monday, October 28, 2013 12:22 PM
>     *To:* Irena Berezovsky; prashant.upadhyaya at aricent.com
>     <mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong;
>     chris.friesen at windriver.com <mailto:chris.friesen at windriver.com>; He,
>     Yongli; Itzik Brown
>     *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen); Kyle
>     Mestery (kmestery); Sandhya Dasu (sadasu)
>     *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through network
>     support
> 
>      
> 
>     Hi Irena,
> 
>      
> 
>     Thank you very much for your comments. See inline. 
> 
>      
> 
>     --Robert
> 
>      
> 
>     On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com
>     <mailto:irenab at mellanox.com>> wrote:
> 
>      
> 
>         Hi Robert,
> 
>         Thank you very much for sharing the information regarding your
>         efforts. Can you please share your idea of the end to end flow? How
>         do you suggest  to bind Nova and Neutron?
> 
>      
> 
>     The end to end flow is actually encompassed in the blueprints in a
>     nutshell. I will reiterate it in below. The binding between Nova and
>     Neutron occurs with the neutron v2 API that nova invokes in order to
>     provision the neutron services. The vif driver is responsible for
>     plugging in an instance onto the networking setup that neutron has
>     created on the host.
> 
>      
> 
>     Normally, one will invoke ""nova boot"" api with the ?nic options to
>     specify the nic with which the instance will be connected to the
>     network. It currently allows net-id, fixed ip and/or port-id to be
>     specified for the option. However, it doesn't allow one to specify
>     special networking requirements for the instance. Thanks to the nova
>     pci-passthrough work, one can specify PCI passthrough device(s) in the
>     nova flavor. But it doesn't provide means to tie up these PCI devices in
>     the case of ethernet adpators with networking services. Therefore the
>     idea is actually simple as indicated by the blueprint titles, to provide
>     means to tie up SRIOV devices with neutron services. A work flow would
>     roughly look like this for 'nova boot':
> 
>      
> 
>           -- Specifies networking requirements in the ?nic option.
>     Specifically for SRIOV, allow the following to be specified in addition
>     to the existing required information:
> 
>                    . PCI alias
> 
>                    . direct pci-passthrough/macvtap
> 
>                    . port profileid that is compliant with 802.1Qbh
> 
>              
> 
>             The above information is optional. In the absence of them, the
>     existing behavior remains.
> 
>      
> 
>          -- if special networking requirements exist, Nova api creates PCI
>     requests in the nova instance type for scheduling purpose
> 
>      
> 
>          -- Nova scheduler schedules the instance based on the requested
>     flavor plus the PCI requests that are created for networking.
> 
>      
> 
>          -- Nova compute invokes neutron services with PCI passthrough
>     information if any 
> 
>      
> 
>          --  Neutron performs its normal operations based on the request,
>     such as allocating a port, assigning ip addresses, etc. Specific to
>     SRIOV, it should validate the information such as profileid, and stores
>     them in its db. It's also possible to associate a port profileid with a
>     neutron network so that port profileid becomes optional in the ?nic
>     option. Neutron returns  nova the port information, especially for PCI
>     passthrough related information in the port binding object. Currently,
>     the port binding object contains the following information:
> 
>               binding:vif_type
> 
>               binding:host_id
> 
>               binding:profile
> 
>               binding:capabilities
> 
>      
> 
>         -- nova constructs the domain xml and plug in the instance by
>     calling the vif driver. The vif driver can build up the interface xml
>     based on the port binding information. 
> 
>      
> 
>      
> 
>      
> 
>          
> 
>         The blueprints you registered make sense. On Nova side, there is a
>         need to bind between requested virtual network and PCI
>         device/interface to be allocated as vNIC.
> 
>         On the Neutron side, there is a need to  support networking
>         configuration of the vNIC. Neutron should be able to identify the
>         PCI device/macvtap interface in order to apply configuration. I
>         think it makes sense to provide neutron integration via dedicated
>         Modular Layer 2 Mechanism Driver to allow PCI pass-through vNIC
>         support along with other networking technologies.
> 
>      
> 
>     I haven't sorted through this yet. A neutron port could be associated
>     with a PCI device or not, which is a common feature, IMHO. However, a
>     ML2 driver may be needed specific to a particular SRIOV technology. 
> 
>      
> 
>          
> 
>         During the Havana Release, we introduced Mellanox Neutron plugin
>         that enables networking via SRIOV pass-through devices or macvtap
>         interfaces.
> 
>         We want to integrate our solution with PCI pass-through Nova
>         support.  I will be glad to share more details if you are interested.
> 
>          
> 
>      
> 
>     Good to know that you already have a SRIOV implementation. I found out
>     some information online about the mlnx plugin, but need more time to get
>     to know it better. And certainly I'm interested in knowing its details.
> 
>      
> 
>         The PCI pass-through networking support is planned to be discussed
>         during the summit: http://summit.openstack.org/cfp/details/129. I
>         think it?s worth to drill down into more detailed proposal and
>         present it during the summit, especially since it impacts both nova
>         and neutron projects.
> 
>          
> 
>     I agree. Maybe we can steal some time in that discussion.
> 
>      
> 
>         Would you be interested in collaboration on this effort? Would you
>         be interested to exchange more emails or set an IRC/WebEx meeting
>         during this week before the summit?
> 
>      
> 
>     Sure. If folks want to discuss it before the summit, we can schedule a
>     webex later this week. Or otherwise, we can continue the discussion with
>     email.
> 
>      
> 
>          
> 
>          
> 
>         Regards,
> 
>         Irena
> 
>          
> 
>         *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>         *Sent:* Friday, October 25, 2013 11:16 PM
>         *To:* prashant.upadhyaya at aricent.com
>         <mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky;
>         yunhong.jiang at intel.com <mailto:yunhong.jiang at intel.com>;
>         chris.friesen at windriver.com <mailto:chris.friesen at windriver.com>;
>         yongli.he at intel.com <mailto:yongli.he at intel.com>
>         *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen);
>         Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
>         *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
>         network support
> 
>          
> 
>         Hi Irena,
> 
>          
> 
>         This is Robert Li from Cisco Systems. Recently, I was tasked to
>         investigate such support for Cisco's systems that support VM-FEX,
>         which is a SRIOV technology supporting 802-1Qbh. I was able to bring
>         up nova instances with SRIOV interfaces, and establish networking in
>         between the instances that employes the SRIOV interfaces. Certainly,
>         this was accomplished with hacking and some manual intervention.
>         Based on this experience and my study with the two existing nova
>         pci-passthrough blueprints that have been implemented and committed
>         into Havana
>         (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
>         https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
>         registered a couple of blueprints (one on Nova side, the other on
>         the Neutron side):
> 
>             
> 
>         https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
> 
>         https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
> 
>          
> 
>         in order to address SRIOV support in openstack. 
> 
>          
> 
>         Please take a look at them and see if they make sense, and let me
>         know any comments and questions. We can also discuss this in the
>         summit, I suppose.
> 
>          
> 
>         I noticed that there is another thread on this topic, so copy those
>         folks  from that thread as well.
> 
>          
> 
>         thanks,
> 
>         Robert
> 
>          
> 
>         On 10/16/13 4:32 PM, ""Irena Berezovsky"" <irenab at mellanox.com
>         <mailto:irenab at mellanox.com>> wrote:
> 
>          
> 
>             Hi,
> 
>             As one of the next steps for PCI pass-through I would like to
>             discuss is the support for PCI pass-through vNIC.
> 
>             While nova takes care of PCI pass-through device resources
>              management and VIF settings, neutron should manage their
>             networking configuration.
> 
>             I would like to register asummit proposal to discuss the support
>             for PCI pass-through networking.
> 
>             I am not sure what would be the right topic to discuss the PCI
>             pass-through networking, since it involve both nova and neutron.  
> 
>             There is already a session registered by Yongli on nova topic to
>             discuss the PCI pass-through next steps.
> 
>             I think PCI pass-through networking is quite a big topic and it
>             worth to have a separate discussion.
> 
>             Is there any other people who are interested to discuss it and
>             share their thoughts and experience?
> 
>              
> 
>             Regards,
> 
>             Irena
> 
>              
> 
> 
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 

"
NOVA 1402728 - CG,msg07290,<DDCAE26804250545B9934A2056554AA01FBA9D68@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI pass-through
	network	support

--

Henry,why do you think the ""service VM"" need the entire PF instead of a VF? I think the SR-IOV NIC should provide QoS and performance isolation.

As to assign entire PCI device to a guest, that should be ok since usually PF and VF has different device ID, the tricky thing is, at least for some PCI devices, you can't configure that some NIC will have SR-IOV enabled while others not.

Thanks
--jyh

> -----Original Message-----
> From: Henry Gessau [mailto:gessau at cisco.com]
> Sent: Tuesday, October 29, 2013 8:10 AM
> To: OpenStack Development Mailing List (not for usage questions)
> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
> 
> Lots of great info and discussion going on here.
> 
> One additional thing I would like to mention is regarding PF and VF usage.
> 
> Normally VFs will be assigned to instances, and the PF will either not be
> used at all, or maybe some agent in the host of the compute node might
> have
> access to the PF for something (management?).
> 
> There is a neutron design track around the development of ""service VMs"".
> These are dedicated instances that run neutron services like routers,
> firewalls, etc. It is plausible that a service VM would like to use PCI
> passthrough and get the entire PF. This would allow it to have complete
> control over a physical link, which I think will be wanted in some cases.
> 
> --
> Henry
> 
> On Tue, Oct 29, at 10:23 am, Irena Berezovsky <irenab at mellanox.com>
> wrote:
> 
> > Hi,
> >
> > I would like to share some details regarding the support provided by
> > Mellanox plugin. It enables networking via SRIOV pass-through devices
> or
> > macvtap interfaces.  It plugin is available here:
> >
> https://github.com/openstack/neutron/tree/master/neutron/plugins/mln
> x.
> >
> > To support either PCI pass-through device and macvtap interface type of
> > vNICs, we set neutron port profile:vnic_type according to the required
> VIF
> > type and then use the created port to 'nova boot' the VM.
> >
> > To  overcome the missing scheduler awareness for PCI devices which
> was not
> > part of the Havana release yet, we
> >
> > have an additional service (embedded switch Daemon) that runs on each
> > compute node.
> >
> > This service manages the SRIOV resources allocation,  answers vNICs
> > discovery queries and applies VLAN/MAC configuration using standard
> Linux
> > APIs (code is here:
> https://github.com/mellanox-openstack/mellanox-eswitchd
> > ).  The embedded switch Daemon serves as a glue layer between VIF
> Driver and
> > Neutron Agent.
> >
> > In the Icehouse Release when SRIOV resources allocation is already part
> of
> > the Nova, we plan to eliminate the need in embedded switch daemon
> service.
> > So what is left to figure out is how to tie up between neutron port and
> PCI
> > device and invoke networking configuration.
> >
> >
> >
> > In our case what we have is actually the Hardware VEB that is not
> programmed
> > via either 802.1Qbg or 802.1Qbh, but configured locally by Neutron
> Agent. We
> > also support both Ethernet and InfiniBand physical network L2
> technology.
> > This means that we apply different configuration commands  to set
> > configuration on VF.
> >
> >
> >
> > I guess what we have to figure out is how to support the generic case for
> > the PCI device networking support, for HW VEB, 802.1Qbg and
> 802.1Qbh cases.
> >
> >
> >
> > BR,
> >
> > Irena
> >
> >
> >
> > *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
> > *Sent:* Tuesday, October 29, 2013 3:31 PM
> > *To:* Jiang, Yunhong; Irena Berezovsky;
> prashant.upadhyaya at aricent.com;
> > chris.friesen at windriver.com; He, Yongli; Itzik Brown
> > *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen);
> Kyle
> > Mestery (kmestery); Sandhya Dasu (sadasu)
> > *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
> network support
> >
> >
> >
> > Hi Yunhong,
> >
> >
> >
> > I haven't looked at Mellanox in much detail. I think that we'll get more
> > details from Irena down the road. Regarding your question, I can only
> answer
> > based on my experience with Cisco's VM-FEX. In a nutshell:
> >
> >      -- a vNIC is connected to an external switch. Once the host is
> booted
> > up, all the PFs and VFs provisioned on the vNIC will be created, as well as
> > all the corresponding ethernet interfaces .
> >
> >      -- As far as Neutron is concerned, a neutron port can be
> associated
> > with a VF. One way to do so is to specify this requirement in the -nic
> > option, providing information such as:
> >
> >                . PCI alias (this is the same alias as defined in your nova
> > blueprints)
> >
> >                . direct pci-passthrough/macvtap
> >
> >                . port profileid that is compliant with 802.1Qbh
> >
> >      -- similar to how you translate the nova flavor with PCI
> requirements
> > to PCI requests for scheduling purpose, Nova API (the nova api
> component)
> > can translate the above to PCI requests for scheduling purpose. I can
> give
> > more detail later on this.
> >
> >
> >
> > Regarding your last question, since the vNIC is already connected with
> the
> > external switch, the vNIC driver will be responsible for communicating
> the
> > port profile to the external switch. As you have already known, libvirt
> > provides several ways to specify a VM to be booted up with SRIOV. For
> > example, in the following interface definition:
> >
> >
> >
> >   *<interface type='hostdev' managed='yes'>*
> >
> > *      <source>*
> >
> > *        <address type='pci' domain='0' bus='0x09' slot='0x0'
> function='0x01'/>*
> >
> > *      </source>*
> >
> > *      <mac address='01:23:45:67:89:ab' />*
> >
> > *      <virtualport type='802.1Qbh'>*
> >
> > *        <parameters profileid='my-port-profile' />*
> >
> > *      </virtualport>*
> >
> > *    </interface>*
> >
> >
> >
> > The SRIOV VF (bus 0x09, VF 0x01) will be allocated, and the port profile
> 'my-port-profile' will be used to provision this VF. Libvirt will be
> responsible for invoking the vNIC driver to configure this VF with the port
> profile my-port-porfile. The driver will talk to the external switch using the
> 802.1qbh standards to complete the VF's configuration and binding with
> the VM.
> >
> >
> >
> > Now that nova PCI passthrough is responsible for
> discovering/scheduling/allocating a VF, the rest of the puzzle is to associate
> this PCI device with the feature that's going to use it, and the feature will
> be responsible for configuring it. You can also see from the above example,
> in one implementation of SRIOV, the feature (in this case neutron) may not
> need to do much in terms of working with the external switch, the work is
> actually done by libvirt behind the scene.
> >
> >
> >
> > Now the questions are:
> >
> >         -- how the port profile gets defined/managed
> >
> >         -- how the port profile gets associated with a neutron network
> >
> > The first question will be specific to the particular product, and
> therefore a particular neutron plugin has to mange that.
> >
> > There may be several approaches to address the second question. For
> example, in the simplest case, a port profile can be associated with a
> neutron network. This has some significant drawbacks. Since the port
> profile defines features for all the ports that use it, the one port profile to
> one neutron network mapping would mean all the ports on the network
> will have exactly the same features (for example, QoS characteristics). To
> make it flexible, the binding of a port profile to a port may be done at the
> port creation time.
> >
> >
> >
> > Let me know if the above answered your question.
> >
> >
> >
> > thanks,
> >
> > Robert
> >
> >
> >
> > On 10/29/13 3:03 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com
> > <mailto:yunhong.jiang at intel.com>> wrote:
> >
> >
> >
> >     Robert, is it possible to have a IRC meeting? I'd prefer to IRC
> meeting
> >     because it's more openstack style and also can keep the minutes
> clearly.
> >
> >
> >
> >     To your flow, can you give more detailed example. For example, I
> can
> >     consider user specify the instance with -nic option specify a
> network
> >     id, and then how nova device the requirement to the PCI device? I
> assume
> >     the network id should define the switches that the device can
> connect to
> >     , but how is that information translated to the PCI property
> >     requirement? Will this translation happen before the nova
> scheduler make
> >     host decision?
> >
> >
> >
> >     Thanks
> >
> >     --jyh
> >
> >
> >
> >     *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
> >     *Sent:* Monday, October 28, 2013 12:22 PM
> >     *To:* Irena Berezovsky; prashant.upadhyaya at aricent.com
> >     <mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong;
> >     chris.friesen at windriver.com
> <mailto:chris.friesen at windriver.com>; He,
> >     Yongli; Itzik Brown
> >     *Cc:* OpenStack Development Mailing List; Brian Bowen
> (brbowen); Kyle
> >     Mestery (kmestery); Sandhya Dasu (sadasu)
> >     *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
> network
> >     support
> >
> >
> >
> >     Hi Irena,
> >
> >
> >
> >     Thank you very much for your comments. See inline.
> >
> >
> >
> >     --Robert
> >
> >
> >
> >     On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com
> >     <mailto:irenab at mellanox.com>> wrote:
> >
> >
> >
> >         Hi Robert,
> >
> >         Thank you very much for sharing the information regarding
> your
> >         efforts. Can you please share your idea of the end to end flow?
> How
> >         do you suggest  to bind Nova and Neutron?
> >
> >
> >
> >     The end to end flow is actually encompassed in the blueprints in a
> >     nutshell. I will reiterate it in below. The binding between Nova and
> >     Neutron occurs with the neutron v2 API that nova invokes in order
> to
> >     provision the neutron services. The vif driver is responsible for
> >     plugging in an instance onto the networking setup that neutron has
> >     created on the host.
> >
> >
> >
> >     Normally, one will invoke ""nova boot"" api with the -nic options to
> >     specify the nic with which the instance will be connected to the
> >     network. It currently allows net-id, fixed ip and/or port-id to be
> >     specified for the option. However, it doesn't allow one to specify
> >     special networking requirements for the instance. Thanks to the
> nova
> >     pci-passthrough work, one can specify PCI passthrough device(s) in
> the
> >     nova flavor. But it doesn't provide means to tie up these PCI devices
> in
> >     the case of ethernet adpators with networking services. Therefore
> the
> >     idea is actually simple as indicated by the blueprint titles, to provide
> >     means to tie up SRIOV devices with neutron services. A work flow
> would
> >     roughly look like this for 'nova boot':
> >
> >
> >
> >           -- Specifies networking requirements in the -nic option.
> >     Specifically for SRIOV, allow the following to be specified in addition
> >     to the existing required information:
> >
> >                    . PCI alias
> >
> >                    . direct pci-passthrough/macvtap
> >
> >                    . port profileid that is compliant with 802.1Qbh
> >
> >
> >
> >             The above information is optional. In the absence of them,
> the
> >     existing behavior remains.
> >
> >
> >
> >          -- if special networking requirements exist, Nova api creates
> PCI
> >     requests in the nova instance type for scheduling purpose
> >
> >
> >
> >          -- Nova scheduler schedules the instance based on the
> requested
> >     flavor plus the PCI requests that are created for networking.
> >
> >
> >
> >          -- Nova compute invokes neutron services with PCI
> passthrough
> >     information if any
> >
> >
> >
> >          --  Neutron performs its normal operations based on the
> request,
> >     such as allocating a port, assigning ip addresses, etc. Specific to
> >     SRIOV, it should validate the information such as profileid, and
> stores
> >     them in its db. It's also possible to associate a port profileid with a
> >     neutron network so that port profileid becomes optional in the
> -nic
> >     option. Neutron returns  nova the port information, especially for
> PCI
> >     passthrough related information in the port binding object.
> Currently,
> >     the port binding object contains the following information:
> >
> >               binding:vif_type
> >
> >               binding:host_id
> >
> >               binding:profile
> >
> >               binding:capabilities
> >
> >
> >
> >         -- nova constructs the domain xml and plug in the instance by
> >     calling the vif driver. The vif driver can build up the interface xml
> >     based on the port binding information.
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >         The blueprints you registered make sense. On Nova side, there
> is a
> >         need to bind between requested virtual network and PCI
> >         device/interface to be allocated as vNIC.
> >
> >         On the Neutron side, there is a need to  support networking
> >         configuration of the vNIC. Neutron should be able to identify
> the
> >         PCI device/macvtap interface in order to apply configuration. I
> >         think it makes sense to provide neutron integration via
> dedicated
> >         Modular Layer 2 Mechanism Driver to allow PCI pass-through
> vNIC
> >         support along with other networking technologies.
> >
> >
> >
> >     I haven't sorted through this yet. A neutron port could be
> associated
> >     with a PCI device or not, which is a common feature, IMHO.
> However, a
> >     ML2 driver may be needed specific to a particular SRIOV
> technology.
> >
> >
> >
> >
> >
> >         During the Havana Release, we introduced Mellanox Neutron
> plugin
> >         that enables networking via SRIOV pass-through devices or
> macvtap
> >         interfaces.
> >
> >         We want to integrate our solution with PCI pass-through Nova
> >         support.  I will be glad to share more details if you are
> interested.
> >
> >
> >
> >
> >
> >     Good to know that you already have a SRIOV implementation. I
> found out
> >     some information online about the mlnx plugin, but need more
> time to get
> >     to know it better. And certainly I'm interested in knowing its details.
> >
> >
> >
> >         The PCI pass-through networking support is planned to be
> discussed
> >         during the summit:
> http://summit.openstack.org/cfp/details/129. I
> >         think it's worth to drill down into more detailed proposal and
> >         present it during the summit, especially since it impacts both
> nova
> >         and neutron projects.
> >
> >
> >
> >     I agree. Maybe we can steal some time in that discussion.
> >
> >
> >
> >         Would you be interested in collaboration on this effort? Would
> you
> >         be interested to exchange more emails or set an IRC/WebEx
> meeting
> >         during this week before the summit?
> >
> >
> >
> >     Sure. If folks want to discuss it before the summit, we can schedule
> a
> >     webex later this week. Or otherwise, we can continue the
> discussion with
> >     email.
> >
> >
> >
> >
> >
> >
> >
> >         Regards,
> >
> >         Irena
> >
> >
> >
> >         *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
> >         *Sent:* Friday, October 25, 2013 11:16 PM
> >         *To:* prashant.upadhyaya at aricent.com
> >         <mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky;
> >         yunhong.jiang at intel.com <mailto:yunhong.jiang at intel.com>;
> >         chris.friesen at windriver.com
> <mailto:chris.friesen at windriver.com>;
> >         yongli.he at intel.com <mailto:yongli.he at intel.com>
> >         *Cc:* OpenStack Development Mailing List; Brian Bowen
> (brbowen);
> >         Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
> >         *Subject:* Re: [openstack-dev] [nova] [neutron] PCI
> pass-through
> >         network support
> >
> >
> >
> >         Hi Irena,
> >
> >
> >
> >         This is Robert Li from Cisco Systems. Recently, I was tasked to
> >         investigate such support for Cisco's systems that support
> VM-FEX,
> >         which is a SRIOV technology supporting 802-1Qbh. I was able to
> bring
> >         up nova instances with SRIOV interfaces, and establish
> networking in
> >         between the instances that employes the SRIOV interfaces.
> Certainly,
> >         this was accomplished with hacking and some manual
> intervention.
> >         Based on this experience and my study with the two existing
> nova
> >         pci-passthrough blueprints that have been implemented and
> committed
> >         into Havana
> >
> (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
> >
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
> >         registered a couple of blueprints (one on Nova side, the other
> on
> >         the Neutron side):
> >
> >
> >
> >
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
> >
> >
> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
> >
> >
> >
> >         in order to address SRIOV support in openstack.
> >
> >
> >
> >         Please take a look at them and see if they make sense, and let
> me
> >         know any comments and questions. We can also discuss this in
> the
> >         summit, I suppose.
> >
> >
> >
> >         I noticed that there is another thread on this topic, so copy
> those
> >         folks  from that thread as well.
> >
> >
> >
> >         thanks,
> >
> >         Robert
> >
> >
> >
> >         On 10/16/13 4:32 PM, ""Irena Berezovsky""
> <irenab at mellanox.com
> >         <mailto:irenab at mellanox.com>> wrote:
> >
> >
> >
> >             Hi,
> >
> >             As one of the next steps for PCI pass-through I would like
> to
> >             discuss is the support for PCI pass-through vNIC.
> >
> >             While nova takes care of PCI pass-through device
> resources
> >              management and VIF settings, neutron should manage
> their
> >             networking configuration.
> >
> >             I would like to register asummit proposal to discuss the
> support
> >             for PCI pass-through networking.
> >
> >             I am not sure what would be the right topic to discuss the
> PCI
> >             pass-through networking, since it involve both nova and
> neutron.
> >
> >             There is already a session registered by Yongli on nova
> topic to
> >             discuss the PCI pass-through next steps.
> >
> >             I think PCI pass-through networking is quite a big topic and
> it
> >             worth to have a separate discussion.
> >
> >             Is there any other people who are interested to discuss it
> and
> >             share their thoughts and experience?
> >
> >
> >
> >             Regards,
> >
> >             Irena
> >
> >
> >
> >
> >
> > _______________________________________________
> > OpenStack-dev mailing list
> > OpenStack-dev at lists.openstack.org
> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg07301,<52702751.40005@cisco.com>,"[openstack-dev] [nova] [neutron] PCI pass-through
	network	support

--

On Tue, Oct 29, at 4:31 pm, Jiang, Yunhong <yunhong.jiang at intel.com> wrote:

> Henry,why do you think the ""service VM"" need the entire PF instead of a
> VF? I think the SR-IOV NIC should provide QoS and performance isolation.

I was speculating. I just thought it might be a good idea to leave open the
possibility of assigning a PF to a VM if the need arises.

Neutron service VMs are a new thing. I will be following the discussions and
there is a summit session for them. It remains to be seen if there is any
desire/need for full PF ownership of NICs. But if a service VM owns the PF
and has the right NIC driver it could do some advanced features with it.

> As to assign entire PCI device to a guest, that should be ok since
> usually PF and VF has different device ID, the tricky thing is, at least
> for some PCI devices, you can't configure that some NIC will have SR-IOV
> enabled while others not.

Thanks for the warning. :) Perhaps the cloud admin might plug in an extra
NIC in just a few nodes (one or two per rack, maybe) for the purpose of
running service VMs there. Again, just speculating. I don't know how hard it
is to manage non-homogenous nodes.

> 
> Thanks
> --jyh
> 
>> -----Original Message-----
>> From: Henry Gessau [mailto:gessau at cisco.com]
>> Sent: Tuesday, October 29, 2013 8:10 AM
>> To: OpenStack Development Mailing List (not for usage questions)
>> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
>> support
>> 
>> Lots of great info and discussion going on here.
>> 
>> One additional thing I would like to mention is regarding PF and VF usage.
>> 
>> Normally VFs will be assigned to instances, and the PF will either not be
>> used at all, or maybe some agent in the host of the compute node might
>> have
>> access to the PF for something (management?).
>> 
>> There is a neutron design track around the development of ""service VMs"".
>> These are dedicated instances that run neutron services like routers,
>> firewalls, etc. It is plausible that a service VM would like to use PCI
>> passthrough and get the entire PF. This would allow it to have complete
>> control over a physical link, which I think will be wanted in some cases.
>> 
>> --
>> Henry
>> 
>> On Tue, Oct 29, at 10:23 am, Irena Berezovsky <irenab at mellanox.com>
>> wrote:
>> 
>> > Hi,
>> >
>> > I would like to share some details regarding the support provided by
>> > Mellanox plugin. It enables networking via SRIOV pass-through devices
>> or
>> > macvtap interfaces.  It plugin is available here:
>> >
>> https://github.com/openstack/neutron/tree/master/neutron/plugins/mln
>> x.
>> >
>> > To support either PCI pass-through device and macvtap interface type of
>> > vNICs, we set neutron port profile:vnic_type according to the required
>> VIF
>> > type and then use the created port to 'nova boot' the VM.
>> >
>> > To  overcome the missing scheduler awareness for PCI devices which
>> was not
>> > part of the Havana release yet, we
>> >
>> > have an additional service (embedded switch Daemon) that runs on each
>> > compute node.
>> >
>> > This service manages the SRIOV resources allocation,  answers vNICs
>> > discovery queries and applies VLAN/MAC configuration using standard
>> Linux
>> > APIs (code is here:
>> https://github.com/mellanox-openstack/mellanox-eswitchd
>> > ).  The embedded switch Daemon serves as a glue layer between VIF
>> Driver and
>> > Neutron Agent.
>> >
>> > In the Icehouse Release when SRIOV resources allocation is already part
>> of
>> > the Nova, we plan to eliminate the need in embedded switch daemon
>> service.
>> > So what is left to figure out is how to tie up between neutron port and
>> PCI
>> > device and invoke networking configuration.
>> >
>> >
>> >
>> > In our case what we have is actually the Hardware VEB that is not
>> programmed
>> > via either 802.1Qbg or 802.1Qbh, but configured locally by Neutron
>> Agent. We
>> > also support both Ethernet and InfiniBand physical network L2
>> technology.
>> > This means that we apply different configuration commands  to set
>> > configuration on VF.
>> >
>> >
>> >
>> > I guess what we have to figure out is how to support the generic case for
>> > the PCI device networking support, for HW VEB, 802.1Qbg and
>> 802.1Qbh cases.
>> >
>> >
>> >
>> > BR,
>> >
>> > Irena
>> >
>> >
>> >
>> > *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>> > *Sent:* Tuesday, October 29, 2013 3:31 PM
>> > *To:* Jiang, Yunhong; Irena Berezovsky;
>> prashant.upadhyaya at aricent.com;
>> > chris.friesen at windriver.com; He, Yongli; Itzik Brown
>> > *Cc:* OpenStack Development Mailing List; Brian Bowen (brbowen);
>> Kyle
>> > Mestery (kmestery); Sandhya Dasu (sadasu)
>> > *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
>> network support
>> >
>> >
>> >
>> > Hi Yunhong,
>> >
>> >
>> >
>> > I haven't looked at Mellanox in much detail. I think that we'll get more
>> > details from Irena down the road. Regarding your question, I can only
>> answer
>> > based on my experience with Cisco's VM-FEX. In a nutshell:
>> >
>> >      -- a vNIC is connected to an external switch. Once the host is
>> booted
>> > up, all the PFs and VFs provisioned on the vNIC will be created, as well as
>> > all the corresponding ethernet interfaces .
>> >
>> >      -- As far as Neutron is concerned, a neutron port can be
>> associated
>> > with a VF. One way to do so is to specify this requirement in the -nic
>> > option, providing information such as:
>> >
>> >                . PCI alias (this is the same alias as defined in your nova
>> > blueprints)
>> >
>> >                . direct pci-passthrough/macvtap
>> >
>> >                . port profileid that is compliant with 802.1Qbh
>> >
>> >      -- similar to how you translate the nova flavor with PCI
>> requirements
>> > to PCI requests for scheduling purpose, Nova API (the nova api
>> component)
>> > can translate the above to PCI requests for scheduling purpose. I can
>> give
>> > more detail later on this.
>> >
>> >
>> >
>> > Regarding your last question, since the vNIC is already connected with
>> the
>> > external switch, the vNIC driver will be responsible for communicating
>> the
>> > port profile to the external switch. As you have already known, libvirt
>> > provides several ways to specify a VM to be booted up with SRIOV. For
>> > example, in the following interface definition:
>> >
>> >
>> >
>> >   *<interface type='hostdev' managed='yes'>*
>> >
>> > *      <source>*
>> >
>> > *        <address type='pci' domain='0' bus='0x09' slot='0x0'
>> function='0x01'/>*
>> >
>> > *      </source>*
>> >
>> > *      <mac address='01:23:45:67:89:ab' />*
>> >
>> > *      <virtualport type='802.1Qbh'>*
>> >
>> > *        <parameters profileid='my-port-profile' />*
>> >
>> > *      </virtualport>*
>> >
>> > *    </interface>*
>> >
>> >
>> >
>> > The SRIOV VF (bus 0x09, VF 0x01) will be allocated, and the port profile
>> 'my-port-profile' will be used to provision this VF. Libvirt will be
>> responsible for invoking the vNIC driver to configure this VF with the port
>> profile my-port-porfile. The driver will talk to the external switch using the
>> 802.1qbh standards to complete the VF's configuration and binding with
>> the VM.
>> >
>> >
>> >
>> > Now that nova PCI passthrough is responsible for
>> discovering/scheduling/allocating a VF, the rest of the puzzle is to associate
>> this PCI device with the feature that's going to use it, and the feature will
>> be responsible for configuring it. You can also see from the above example,
>> in one implementation of SRIOV, the feature (in this case neutron) may not
>> need to do much in terms of working with the external switch, the work is
>> actually done by libvirt behind the scene.
>> >
>> >
>> >
>> > Now the questions are:
>> >
>> >         -- how the port profile gets defined/managed
>> >
>> >         -- how the port profile gets associated with a neutron network
>> >
>> > The first question will be specific to the particular product, and
>> therefore a particular neutron plugin has to mange that.
>> >
>> > There may be several approaches to address the second question. For
>> example, in the simplest case, a port profile can be associated with a
>> neutron network. This has some significant drawbacks. Since the port
>> profile defines features for all the ports that use it, the one port profile to
>> one neutron network mapping would mean all the ports on the network
>> will have exactly the same features (for example, QoS characteristics). To
>> make it flexible, the binding of a port profile to a port may be done at the
>> port creation time.
>> >
>> >
>> >
>> > Let me know if the above answered your question.
>> >
>> >
>> >
>> > thanks,
>> >
>> > Robert
>> >
>> >
>> >
>> > On 10/29/13 3:03 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com
>> > <mailto:yunhong.jiang at intel.com>> wrote:
>> >
>> >
>> >
>> >     Robert, is it possible to have a IRC meeting? I'd prefer to IRC
>> meeting
>> >     because it's more openstack style and also can keep the minutes
>> clearly.
>> >
>> >
>> >
>> >     To your flow, can you give more detailed example. For example, I
>> can
>> >     consider user specify the instance with -nic option specify a
>> network
>> >     id, and then how nova device the requirement to the PCI device? I
>> assume
>> >     the network id should define the switches that the device can
>> connect to
>> >     , but how is that information translated to the PCI property
>> >     requirement? Will this translation happen before the nova
>> scheduler make
>> >     host decision?
>> >
>> >
>> >
>> >     Thanks
>> >
>> >     --jyh
>> >
>> >
>> >
>> >     *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>> >     *Sent:* Monday, October 28, 2013 12:22 PM
>> >     *To:* Irena Berezovsky; prashant.upadhyaya at aricent.com
>> >     <mailto:prashant.upadhyaya at aricent.com>; Jiang, Yunhong;
>> >     chris.friesen at windriver.com
>> <mailto:chris.friesen at windriver.com>; He,
>> >     Yongli; Itzik Brown
>> >     *Cc:* OpenStack Development Mailing List; Brian Bowen
>> (brbowen); Kyle
>> >     Mestery (kmestery); Sandhya Dasu (sadasu)
>> >     *Subject:* Re: [openstack-dev] [nova] [neutron] PCI pass-through
>> network
>> >     support
>> >
>> >
>> >
>> >     Hi Irena,
>> >
>> >
>> >
>> >     Thank you very much for your comments. See inline.
>> >
>> >
>> >
>> >     --Robert
>> >
>> >
>> >
>> >     On 10/27/13 3:48 AM, ""Irena Berezovsky"" <irenab at mellanox.com
>> >     <mailto:irenab at mellanox.com>> wrote:
>> >
>> >
>> >
>> >         Hi Robert,
>> >
>> >         Thank you very much for sharing the information regarding
>> your
>> >         efforts. Can you please share your idea of the end to end flow?
>> How
>> >         do you suggest  to bind Nova and Neutron?
>> >
>> >
>> >
>> >     The end to end flow is actually encompassed in the blueprints in a
>> >     nutshell. I will reiterate it in below. The binding between Nova and
>> >     Neutron occurs with the neutron v2 API that nova invokes in order
>> to
>> >     provision the neutron services. The vif driver is responsible for
>> >     plugging in an instance onto the networking setup that neutron has
>> >     created on the host.
>> >
>> >
>> >
>> >     Normally, one will invoke ""nova boot"" api with the -nic options to
>> >     specify the nic with which the instance will be connected to the
>> >     network. It currently allows net-id, fixed ip and/or port-id to be
>> >     specified for the option. However, it doesn't allow one to specify
>> >     special networking requirements for the instance. Thanks to the
>> nova
>> >     pci-passthrough work, one can specify PCI passthrough device(s) in
>> the
>> >     nova flavor. But it doesn't provide means to tie up these PCI devices
>> in
>> >     the case of ethernet adpators with networking services. Therefore
>> the
>> >     idea is actually simple as indicated by the blueprint titles, to provide
>> >     means to tie up SRIOV devices with neutron services. A work flow
>> would
>> >     roughly look like this for 'nova boot':
>> >
>> >
>> >
>> >           -- Specifies networking requirements in the -nic option.
>> >     Specifically for SRIOV, allow the following to be specified in addition
>> >     to the existing required information:
>> >
>> >                    . PCI alias
>> >
>> >                    . direct pci-passthrough/macvtap
>> >
>> >                    . port profileid that is compliant with 802.1Qbh
>> >
>> >
>> >
>> >             The above information is optional. In the absence of them,
>> the
>> >     existing behavior remains.
>> >
>> >
>> >
>> >          -- if special networking requirements exist, Nova api creates
>> PCI
>> >     requests in the nova instance type for scheduling purpose
>> >
>> >
>> >
>> >          -- Nova scheduler schedules the instance based on the
>> requested
>> >     flavor plus the PCI requests that are created for networking.
>> >
>> >
>> >
>> >          -- Nova compute invokes neutron services with PCI
>> passthrough
>> >     information if any
>> >
>> >
>> >
>> >          --  Neutron performs its normal operations based on the
>> request,
>> >     such as allocating a port, assigning ip addresses, etc. Specific to
>> >     SRIOV, it should validate the information such as profileid, and
>> stores
>> >     them in its db. It's also possible to associate a port profileid with a
>> >     neutron network so that port profileid becomes optional in the
>> -nic
>> >     option. Neutron returns  nova the port information, especially for
>> PCI
>> >     passthrough related information in the port binding object.
>> Currently,
>> >     the port binding object contains the following information:
>> >
>> >               binding:vif_type
>> >
>> >               binding:host_id
>> >
>> >               binding:profile
>> >
>> >               binding:capabilities
>> >
>> >
>> >
>> >         -- nova constructs the domain xml and plug in the instance by
>> >     calling the vif driver. The vif driver can build up the interface xml
>> >     based on the port binding information.
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >         The blueprints you registered make sense. On Nova side, there
>> is a
>> >         need to bind between requested virtual network and PCI
>> >         device/interface to be allocated as vNIC.
>> >
>> >         On the Neutron side, there is a need to  support networking
>> >         configuration of the vNIC. Neutron should be able to identify
>> the
>> >         PCI device/macvtap interface in order to apply configuration. I
>> >         think it makes sense to provide neutron integration via
>> dedicated
>> >         Modular Layer 2 Mechanism Driver to allow PCI pass-through
>> vNIC
>> >         support along with other networking technologies.
>> >
>> >
>> >
>> >     I haven't sorted through this yet. A neutron port could be
>> associated
>> >     with a PCI device or not, which is a common feature, IMHO.
>> However, a
>> >     ML2 driver may be needed specific to a particular SRIOV
>> technology.
>> >
>> >
>> >
>> >
>> >
>> >         During the Havana Release, we introduced Mellanox Neutron
>> plugin
>> >         that enables networking via SRIOV pass-through devices or
>> macvtap
>> >         interfaces.
>> >
>> >         We want to integrate our solution with PCI pass-through Nova
>> >         support.  I will be glad to share more details if you are
>> interested.
>> >
>> >
>> >
>> >
>> >
>> >     Good to know that you already have a SRIOV implementation. I
>> found out
>> >     some information online about the mlnx plugin, but need more
>> time to get
>> >     to know it better. And certainly I'm interested in knowing its details.
>> >
>> >
>> >
>> >         The PCI pass-through networking support is planned to be
>> discussed
>> >         during the summit:
>> http://summit.openstack.org/cfp/details/129. I
>> >         think it's worth to drill down into more detailed proposal and
>> >         present it during the summit, especially since it impacts both
>> nova
>> >         and neutron projects.
>> >
>> >
>> >
>> >     I agree. Maybe we can steal some time in that discussion.
>> >
>> >
>> >
>> >         Would you be interested in collaboration on this effort? Would
>> you
>> >         be interested to exchange more emails or set an IRC/WebEx
>> meeting
>> >         during this week before the summit?
>> >
>> >
>> >
>> >     Sure. If folks want to discuss it before the summit, we can schedule
>> a
>> >     webex later this week. Or otherwise, we can continue the
>> discussion with
>> >     email.
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >         Regards,
>> >
>> >         Irena
>> >
>> >
>> >
>> >         *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>> >         *Sent:* Friday, October 25, 2013 11:16 PM
>> >         *To:* prashant.upadhyaya at aricent.com
>> >         <mailto:prashant.upadhyaya at aricent.com>; Irena Berezovsky;
>> >         yunhong.jiang at intel.com <mailto:yunhong.jiang at intel.com>;
>> >         chris.friesen at windriver.com
>> <mailto:chris.friesen at windriver.com>;
>> >         yongli.he at intel.com <mailto:yongli.he at intel.com>
>> >         *Cc:* OpenStack Development Mailing List; Brian Bowen
>> (brbowen);
>> >         Kyle Mestery (kmestery); Sandhya Dasu (sadasu)
>> >         *Subject:* Re: [openstack-dev] [nova] [neutron] PCI
>> pass-through
>> >         network support
>> >
>> >
>> >
>> >         Hi Irena,
>> >
>> >
>> >
>> >         This is Robert Li from Cisco Systems. Recently, I was tasked to
>> >         investigate such support for Cisco's systems that support
>> VM-FEX,
>> >         which is a SRIOV technology supporting 802-1Qbh. I was able to
>> bring
>> >         up nova instances with SRIOV interfaces, and establish
>> networking in
>> >         between the instances that employes the SRIOV interfaces.
>> Certainly,
>> >         this was accomplished with hacking and some manual
>> intervention.
>> >         Based on this experience and my study with the two existing
>> nova
>> >         pci-passthrough blueprints that have been implemented and
>> committed
>> >         into Havana
>> >
>> (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base and
>> >
>> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-libvirt),  I
>> >         registered a couple of blueprints (one on Nova side, the other
>> on
>> >         the Neutron side):
>> >
>> >
>> >
>> >
>> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>> >
>> >
>> https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov
>> >
>> >
>> >
>> >         in order to address SRIOV support in openstack.
>> >
>> >
>> >
>> >         Please take a look at them and see if they make sense, and let
>> me
>> >         know any comments and questions. We can also discuss this in
>> the
>> >         summit, I suppose.
>> >
>> >
>> >
>> >         I noticed that there is another thread on this topic, so copy
>> those
>> >         folks  from that thread as well.
>> >
>> >
>> >
>> >         thanks,
>> >
>> >         Robert
>> >
>> >
>> >
>> >         On 10/16/13 4:32 PM, ""Irena Berezovsky""
>> <irenab at mellanox.com
>> >         <mailto:irenab at mellanox.com>> wrote:
>> >
>> >
>> >
>> >             Hi,
>> >
>> >             As one of the next steps for PCI pass-through I would like
>> to
>> >             discuss is the support for PCI pass-through vNIC.
>> >
>> >             While nova takes care of PCI pass-through device
>> resources
>> >              management and VIF settings, neutron should manage
>> their
>> >             networking configuration.
>> >
>> >             I would like to register asummit proposal to discuss the
>> support
>> >             for PCI pass-through networking.
>> >
>> >             I am not sure what would be the right topic to discuss the
>> PCI
>> >             pass-through networking, since it involve both nova and
>> neutron.
>> >
>> >             There is already a session registered by Yongli on nova
>> topic to
>> >             discuss the PCI pass-through next steps.
>> >
>> >             I think PCI pass-through networking is quite a big topic and
>> it
>> >             worth to have a separate discussion.
>> >
>> >             Is there any other people who are interested to discuss it
>> and
>> >             share their thoughts and experience?
>> >
>> >
>> >
>> >             Regards,
>> >
>> >             Irena
>> >
>> >
>> >
>> >
>> >
>> > _______________________________________________
>> > OpenStack-dev mailing list
>> > OpenStack-dev at lists.openstack.org
>> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>> >
>> 
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 

"
NOVA 1402728 - CG,msg07303,<52702895.40508@windriver.com>,"[openstack-dev] [nova] [neutron] PCI pass-through
	network	support

--

On 10/29/2013 03:23 PM, Henry Gessau wrote:
> On Tue, Oct 29, at 4:31 pm, Jiang, Yunhong <yunhong.jiang at intel.com> wrote:

>> As to assign entire PCI device to a guest, that should be ok since
>> usually PF and VF has different device ID, the tricky thing is, at least
>> for some PCI devices, you can't configure that some NIC will have SR-IOV
>> enabled while others not.
>
> Thanks for the warning. :) Perhaps the cloud admin might plug in an extra
> NIC in just a few nodes (one or two per rack, maybe) for the purpose of
> running service VMs there. Again, just speculating. I don't know how hard it
> is to manage non-homogenous nodes.

Perhaps those nodes could be identified using a host-aggregate with 
suitable metadata?

Chris


"
NOVA 1402728 - CG,msg07311,<DDCAE26804250545B9934A2056554AA01FBA9F0B@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] [nova] [neutron] PCI
	pass-through	network	support

--



> -----Original Message-----
> From: Henry Gessau [mailto:gessau at cisco.com]
> Sent: Tuesday, October 29, 2013 2:23 PM
> To: OpenStack Development Mailing List (not for usage questions)
> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
> support
> 
> On Tue, Oct 29, at 4:31 pm, Jiang, Yunhong <yunhong.jiang at intel.com>
> wrote:
> 
> > Henry,why do you think the ""service VM"" need the entire PF instead of a
> > VF? I think the SR-IOV NIC should provide QoS and performance
> isolation.
> 
> I was speculating. I just thought it might be a good idea to leave open the
> possibility of assigning a PF to a VM if the need arises.
> 
> Neutron service VMs are a new thing. I will be following the discussions
> and
> there is a summit session for them. It remains to be seen if there is any
> desire/need for full PF ownership of NICs. But if a service VM owns the PF
> and has the right NIC driver it could do some advanced features with it.
> 
At least in current PCI implementation, if a device has no SR-IOV enabled, then that device will be exposed and can be assigned (is this your so-called PF?). If a device has SR-IOV enabled, then only VF be exposed and the PF is hidden from resource tracker. The reason is, when SR-IOV enabled, the PF is mostly used to configure and management the VFs, and it will be security issue to expose the PF to a guest.

I'm not sure if you are talking about the PF, are you talking about the PF w/ or w/o SR-IOV enabled. 

I totally agree that assign a PCI NIC to service VM have a lot of benefit from both performance and isolation point of view.

Thanks
--jyh

"
NOVA 1402728 - CG,msg07318,<527044F7.8040800@cisco.com>,"[openstack-dev] [nova] [neutron] PCI
	pass-through	network	support

--

On Tue, Oct 29, at 5:52 pm, Jiang, Yunhong <yunhong.jiang at intel.com> wrote:

>> -----Original Message-----
>> From: Henry Gessau [mailto:gessau at cisco.com]
>> Sent: Tuesday, October 29, 2013 2:23 PM
>> To: OpenStack Development Mailing List (not for usage questions)
>> Subject: Re: [openstack-dev] [nova] [neutron] PCI pass-through network
>> support
>> 
>> On Tue, Oct 29, at 4:31 pm, Jiang, Yunhong <yunhong.jiang at intel.com>
>> wrote:
>> 
>> > Henry,why do you think the ""service VM"" need the entire PF instead of a
>> > VF? I think the SR-IOV NIC should provide QoS and performance
>> isolation.
>> 
>> I was speculating. I just thought it might be a good idea to leave open the
>> possibility of assigning a PF to a VM if the need arises.
>> 
>> Neutron service VMs are a new thing. I will be following the discussions
>> and
>> there is a summit session for them. It remains to be seen if there is any
>> desire/need for full PF ownership of NICs. But if a service VM owns the PF
>> and has the right NIC driver it could do some advanced features with it.
>> 
> At least in current PCI implementation, if a device has no SR-IOV
> enabled, then that device will be exposed and can be assigned (is this
> your so-called PF?).

Apologies, this was not clear to me until now. Thanks. I am not aware of a
use-case for a service VM needing to control VFs. So you are right, I should
not have talked about PF but rather just the entire NIC device in
passthrough mode, no SR-IOV needed.

So the admin will need to know: Put a NIC in SR-IOV mode if it is to be used
by multiple VMs. Put a NIC in single device passthrough mode if it is to be
used by one service VM.

> If a device has SR-IOV enabled, then only VF be
> exposed and the PF is hidden from resource tracker. The reason is, when
> SR-IOV enabled, the PF is mostly used to configure and management the
> VFs, and it will be security issue to expose the PF to a guest.

Thanks for bringing up the security issue. If a physical network interface
is connected in a special way to some switch/router with the intention being
for it to be used only by a service VM, then close attention must be paid to
security. The device owner might get some low-level network access that can
be misused.

> I'm not sure if you are talking about the PF, are you talking about the
> PF w/ or w/o SR-IOV enabled.
> 
> I totally agree that assign a PCI NIC to service VM have a lot of benefit
> from both performance and isolation point of view.
> 
> Thanks
> --jyh
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 

"
NOVA 1402728 - CG,msg53501,<HKXPR01MB24644116F967E29E410E5A88BCF0@HKXPR01MB246.apcprd01.prod.exchangelabs.com>,"[openstack-dev] PCI pass-through SRIOV

--

Hi,

Am deploying controller-compute openstack setup , in controller I configured openvswitch bridges and in computed node I configured PCI nic supported SRIOV capability and while am spawning VM am getting following error as in attached file:

I followed the link for setting up the sriov https://wiki.openstack.org/wiki/SR-IOV-Passthrough-For-Networking and http://www.qlogic.com/solutions/Documents/UsersGuide_OpenStack_SR-IOV.pdf

Could any one help me regarding this issue


Thanks and Regards,
Raghavendrachari kamsali,
Embedded Computing and Power,
Hyderabad, AndhraPradesh , India


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150523/b410ed37/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sriov_n_compute.txt
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150523/b410ed37/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: openstack_setup_details.txt
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150523/b410ed37/attachment-0001.txt>
"
NOVA 1402728 - CG,msg16161,<CAAfiWQoFn6=dvubgovjO_G5X3N+j8yi9-8v+o6sNeiNHftxfGA@mail.gmail.com>,"[openstack-dev] How to write a new neutron L2 plugin using ML2
	framework?

--

Hi,

SRIOV is under implementation in nova and neutron. Did you have a look to :
https://wiki.openstack.org/wiki/PCI_passthrough_SRIOV_support
https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile
https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov


On Mon, Feb 10, 2014 at 7:27 AM, Isaku Yamahata
<isaku.yamahata at gmail.com> wrote:
> On Sat, Feb 08, 2014 at 03:49:46AM +0000,
> ""Yang, Yi Y"" <yi.y.yang at intel.com> wrote:
>
>> Hi, All
>
> Hi.
>
>
>> I want to write a new neutron L2 plugin using ML2 framework, I noticed openvswitch and linxubridge have been ported into ML2 framework, but it seems many code is removed compared to standalone L2 plugin, I guess some code has been written into a common library. Now I want to write a L2 plugin to enable switch for a SR-IOV 10g NIC, I think I need to write as follows:
>

having such a feature would be awesome : did you fill a BP for that?

>
>> 1. a new mechanism driver neutron/plugins/ml2/drivers/mech_XXX.py, but from source code, it seems nothing to do.

You mean, you want to use AgentMechanismDriverBase directly? this is
an abstract class du to check_segment_for_agent method.

>
> This requires to define how your plugin utilize network.
> If multi tenant network is wanted, what/how technology will be used.
> The common one is VLAN or tunneling(GRE, VXLAN).
> This depends on what feature your NIC supports.
>

>> 2. a new agent neutron/plugins/XXX/ XXX_neutron_plugin.py

I don't know if this would be mandatory. May be you can just add
necessary informations with extend_port_dict while your MD bind the
port, as proposed by this patch :
https://review.openstack.org/#/c/69783/

Nova will then configure the port correctly. The only need for an
agent would be to populate the agent DB with supported segment types,
so that during bind_port, the MD find an appropriate segment (with
check_segment_for_agent).

>>
>> After this, an issue it how to let neutron know it and load it by default or by configuration. Debugging is also an issue, nobody can write code correctly once :-),  does neutron have any good debugging way for a newbie?
>
> LOG.debug and debug middle ware.
> If there are any other better way, I'd also like to know.
>
> thanks,
>
>> I'm very eager to be able to get your help and sincerely thank you in advance.
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
> --
> Isaku Yamahata <isaku.yamahata at gmail.com>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg16230,<CAAfiWQpHTxW9_N3YDHc9zHcYrhjK5-w1SbUf0EQKg7ZqbashJA@mail.gmail.com>,"[openstack-dev] How to write a new neutron L2 plugin using ML2
	framework?

--

Hi,

mellanox is also developing a ML2 driver :
https://blueprints.launchpad.net/neutron/+spec/mlnx-ml2-support

The Havana release is already out, and we are currently working for
IceHouse. But the code for IceHouse should be under review before feb.
18th. So it would be difficult to have your code included in IceHouse.
I think you'd better targeting Juno, the next release.

On Tue, Feb 11, 2014 at 7:40 AM, Yang, Yi Y <yi.y.yang at intel.com> wrote:
> Thank you for your detailed info, but I want to implement this in Havana release, mlnx is a good reference, what I want to implement on Intel NIC is similar to mlnx, but it is a standalone plugin and didn't use ML2 framework, I want to use ML2 framework, I think nova has supported SR-IOV in Havana, so I just need to implement Neutron part, I hope you can provide some guide about this. BTW, We can't afford to wait Icehouse release.
>
> -----Original Message-----
> From: Irena Berezovsky [mailto:irenab at mellanox.com]
> Sent: Monday, February 10, 2014 8:11 PM
> To: OpenStack Development Mailing List (not for usage questions)
> Cc: Yang, Yi Y
> Subject: RE: [openstack-dev] How to write a new neutron L2 plugin using ML2 framework?
>
> Hi,
> As stated below, we are already having this work both in nova and neuron.
> Please take a look at the following discussions:
> https://wiki.openstack.org/wiki/Meetings#PCI_Passthrough_Meeting
>
> For neutron part there are two different flavors that are coming as part of this effort:
> 1. Cisco SRIOV supporting 802.1QBH - no L2 agent 2. Mellanox Flavor - SRIOV embedded switch (""HW_VEB"") - with L2 agent.
> My guess is that second flavor of SRIOV embedded switch should work for Intel NICs as well.
>
> Please join the PCI pass-through meeting discussions to see that you do not do any redundant work or just follow-up on mailing list.
>
> BR,
> Irena
>
>
> -----Original Message-----
> From: Mathieu Rohon [mailto:mathieu.rohon at gmail.com]
> Sent: Monday, February 10, 2014 1:25 PM
> To: OpenStack Development Mailing List (not for usage questions)
> Subject: Re: [openstack-dev] How to write a new neutron L2 plugin using ML2 framework?
>
> Hi,
>
> SRIOV is under implementation in nova and neutron. Did you have a look to :
> https://wiki.openstack.org/wiki/PCI_passthrough_SRIOV_support
> https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile
> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
> https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
>
>
> On Mon, Feb 10, 2014 at 7:27 AM, Isaku Yamahata <isaku.yamahata at gmail.com> wrote:
>> On Sat, Feb 08, 2014 at 03:49:46AM +0000, ""Yang, Yi Y""
>> <yi.y.yang at intel.com> wrote:
>>
>>> Hi, All
>>
>> Hi.
>>
>>
>>> I want to write a new neutron L2 plugin using ML2 framework, I noticed openvswitch and linxubridge have been ported into ML2 framework, but it seems many code is removed compared to standalone L2 plugin, I guess some code has been written into a common library. Now I want to write a L2 plugin to enable switch for a SR-IOV 10g NIC, I think I need to write as follows:
>>
>
> having such a feature would be awesome : did you fill a BP for that?
>
>>
>>> 1. a new mechanism driver neutron/plugins/ml2/drivers/mech_XXX.py, but from source code, it seems nothing to do.
>
> You mean, you want to use AgentMechanismDriverBase directly? this is an abstract class du to check_segment_for_agent method.
>
>>
>> This requires to define how your plugin utilize network.
>> If multi tenant network is wanted, what/how technology will be used.
>> The common one is VLAN or tunneling(GRE, VXLAN).
>> This depends on what feature your NIC supports.
>>
>
>>> 2. a new agent neutron/plugins/XXX/ XXX_neutron_plugin.py
>
> I don't know if this would be mandatory. May be you can just add necessary informations with extend_port_dict while your MD bind the port, as proposed by this patch :
> https://review.openstack.org/#/c/69783/
>
> Nova will then configure the port correctly. The only need for an agent would be to populate the agent DB with supported segment types, so that during bind_port, the MD find an appropriate segment (with check_segment_for_agent).
>
>>>
>>> After this, an issue it how to let neutron know it and load it by default or by configuration. Debugging is also an issue, nobody can write code correctly once :-),  does neutron have any good debugging way for a newbie?
>>
>> LOG.debug and debug middle ware.
>> If there are any other better way, I'd also like to know.
>>
>> thanks,
>>
>>> I'm very eager to be able to get your help and sincerely thank you in advance.
>>>
>>> _______________________________________________
>>> OpenStack-dev mailing list
>>> OpenStack-dev at lists.openstack.org
>>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>> --
>> Isaku Yamahata <isaku.yamahata at gmail.com>
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg16225,<79BBBFE6CB6C9B488C1A45ACD284F51910CBB823@SHSMSX103.ccr.corp.intel.com>,"[openstack-dev] How to write a new neutron L2 plugin using
	ML2	framework?

--

Thank you for your detailed info, but I want to implement this in Havana release, mlnx is a good reference, what I want to implement on Intel NIC is similar to mlnx, but it is a standalone plugin and didn't use ML2 framework, I want to use ML2 framework, I think nova has supported SR-IOV in Havana, so I just need to implement Neutron part, I hope you can provide some guide about this. BTW, We can't afford to wait Icehouse release.

-----Original Message-----
From: Irena Berezovsky [mailto:irenab at mellanox.com] 
Sent: Monday, February 10, 2014 8:11 PM
To: OpenStack Development Mailing List (not for usage questions)
Cc: Yang, Yi Y
Subject: RE: [openstack-dev] How to write a new neutron L2 plugin using ML2 framework?

Hi,
As stated below, we are already having this work both in nova and neuron.
Please take a look at the following discussions:
https://wiki.openstack.org/wiki/Meetings#PCI_Passthrough_Meeting

For neutron part there are two different flavors that are coming as part of this effort:
1. Cisco SRIOV supporting 802.1QBH - no L2 agent 2. Mellanox Flavor - SRIOV embedded switch (""HW_VEB"") - with L2 agent.
My guess is that second flavor of SRIOV embedded switch should work for Intel NICs as well.

Please join the PCI pass-through meeting discussions to see that you do not do any redundant work or just follow-up on mailing list.

BR,
Irena


-----Original Message-----
From: Mathieu Rohon [mailto:mathieu.rohon at gmail.com]
Sent: Monday, February 10, 2014 1:25 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] How to write a new neutron L2 plugin using ML2 framework?

Hi,

SRIOV is under implementation in nova and neutron. Did you have a look to :
https://wiki.openstack.org/wiki/PCI_passthrough_SRIOV_support
https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile
https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov


On Mon, Feb 10, 2014 at 7:27 AM, Isaku Yamahata <isaku.yamahata at gmail.com> wrote:
> On Sat, Feb 08, 2014 at 03:49:46AM +0000, ""Yang, Yi Y"" 
> <yi.y.yang at intel.com> wrote:
>
>> Hi, All
>
> Hi.
>
>
>> I want to write a new neutron L2 plugin using ML2 framework, I noticed openvswitch and linxubridge have been ported into ML2 framework, but it seems many code is removed compared to standalone L2 plugin, I guess some code has been written into a common library. Now I want to write a L2 plugin to enable switch for a SR-IOV 10g NIC, I think I need to write as follows:
>

having such a feature would be awesome : did you fill a BP for that?

>
>> 1. a new mechanism driver neutron/plugins/ml2/drivers/mech_XXX.py, but from source code, it seems nothing to do.

You mean, you want to use AgentMechanismDriverBase directly? this is an abstract class du to check_segment_for_agent method.

>
> This requires to define how your plugin utilize network.
> If multi tenant network is wanted, what/how technology will be used.
> The common one is VLAN or tunneling(GRE, VXLAN).
> This depends on what feature your NIC supports.
>

>> 2. a new agent neutron/plugins/XXX/ XXX_neutron_plugin.py

I don't know if this would be mandatory. May be you can just add necessary informations with extend_port_dict while your MD bind the port, as proposed by this patch :
https://review.openstack.org/#/c/69783/

Nova will then configure the port correctly. The only need for an agent would be to populate the agent DB with supported segment types, so that during bind_port, the MD find an appropriate segment (with check_segment_for_agent).

>>
>> After this, an issue it how to let neutron know it and load it by default or by configuration. Debugging is also an issue, nobody can write code correctly once :-),  does neutron have any good debugging way for a newbie?
>
> LOG.debug and debug middle ware.
> If there are any other better way, I'd also like to know.
>
> thanks,
>
>> I'm very eager to be able to get your help and sincerely thank you in advance.
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
> --
> Isaku Yamahata <isaku.yamahata at gmail.com>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

_______________________________________________
OpenStack-dev mailing list
OpenStack-dev at lists.openstack.org
http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg16162,<9D25E123B44F4A4291F4B5C13DA94E7795EAECA3@MTLDAG01.mtl.com>,"[openstack-dev] How to write a new neutron L2 plugin using
	ML2	framework?

--

Hi,
As stated below, we are already having this work both in nova and neuron.
Please take a look at the following discussions:
https://wiki.openstack.org/wiki/Meetings#PCI_Passthrough_Meeting

For neutron part there are two different flavors that are coming as part of this effort:
1. Cisco SRIOV supporting 802.1QBH - no L2 agent
2. Mellanox Flavor - SRIOV embedded switch (""HW_VEB"") - with L2 agent.
My guess is that second flavor of SRIOV embedded switch should work for Intel NICs as well.

Please join the PCI pass-through meeting discussions to see that you do not do any redundant work or just follow-up on mailing list.

BR,
Irena


-----Original Message-----
From: Mathieu Rohon [mailto:mathieu.rohon at gmail.com] 
Sent: Monday, February 10, 2014 1:25 PM
To: OpenStack Development Mailing List (not for usage questions)
Subject: Re: [openstack-dev] How to write a new neutron L2 plugin using ML2 framework?

Hi,

SRIOV is under implementation in nova and neutron. Did you have a look to :
https://wiki.openstack.org/wiki/PCI_passthrough_SRIOV_support
https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile
https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov


On Mon, Feb 10, 2014 at 7:27 AM, Isaku Yamahata <isaku.yamahata at gmail.com> wrote:
> On Sat, Feb 08, 2014 at 03:49:46AM +0000, ""Yang, Yi Y"" 
> <yi.y.yang at intel.com> wrote:
>
>> Hi, All
>
> Hi.
>
>
>> I want to write a new neutron L2 plugin using ML2 framework, I noticed openvswitch and linxubridge have been ported into ML2 framework, but it seems many code is removed compared to standalone L2 plugin, I guess some code has been written into a common library. Now I want to write a L2 plugin to enable switch for a SR-IOV 10g NIC, I think I need to write as follows:
>

having such a feature would be awesome : did you fill a BP for that?

>
>> 1. a new mechanism driver neutron/plugins/ml2/drivers/mech_XXX.py, but from source code, it seems nothing to do.

You mean, you want to use AgentMechanismDriverBase directly? this is an abstract class du to check_segment_for_agent method.

>
> This requires to define how your plugin utilize network.
> If multi tenant network is wanted, what/how technology will be used.
> The common one is VLAN or tunneling(GRE, VXLAN).
> This depends on what feature your NIC supports.
>

>> 2. a new agent neutron/plugins/XXX/ XXX_neutron_plugin.py

I don't know if this would be mandatory. May be you can just add necessary informations with extend_port_dict while your MD bind the port, as proposed by this patch :
https://review.openstack.org/#/c/69783/

Nova will then configure the port correctly. The only need for an agent would be to populate the agent DB with supported segment types, so that during bind_port, the MD find an appropriate segment (with check_segment_for_agent).

>>
>> After this, an issue it how to let neutron know it and load it by default or by configuration. Debugging is also an issue, nobody can write code correctly once :-),  does neutron have any good debugging way for a newbie?
>
> LOG.debug and debug middle ware.
> If there are any other better way, I'd also like to know.
>
> thanks,
>
>> I'm very eager to be able to get your help and sincerely thank you in advance.
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
> --
> Isaku Yamahata <isaku.yamahata at gmail.com>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

_______________________________________________
OpenStack-dev mailing list
OpenStack-dev at lists.openstack.org
http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg34075,<D02DF54F.B4308%baoli@cisco.com>,"[openstack-dev] [nova] requesting an FFE for SRIOV

--

Hi,

The main sr-iov patches have gone through lots of code reviews, manual rebasing, etc. Now we have some critical refactoring work on the existing infra to get it ready. All the code for refactoring and sr-iov is up for review.

https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov

thanks,
Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140904/b77c4ad4/attachment.html>
"
NOVA 1402728 - CG,msg34090,<54088254.4010105@danplanet.com>,"[openstack-dev] [nova] requesting an FFE for SRIOV

--

> The main sr-iov patches have gone through lots of code reviews, manual
> rebasing, etc. Now we have some critical refactoring work on the
> existing infra to get it ready. All the code for refactoring and sr-iov
> is up for review.  

I've been doing a lot of work on this recently, and plan to see it
through if possible.

So, I'll be a sponsor.

In the meeting russellb said he would as well. I think he's tied up
today, so I'm proxying him in here :)

--Dan

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140904/1209622e/attachment.pgp>
"
NOVA 1402728 - CG,msg34099,<54088787.5050600@redhat.com>,"[openstack-dev] [nova] requesting an FFE for SRIOV

--

On 09/04/2014 05:16 PM, Dan Smith wrote:
>> The main sr-iov patches have gone through lots of code reviews, manual
>> rebasing, etc. Now we have some critical refactoring work on the
>> existing infra to get it ready. All the code for refactoring and sr-iov
>> is up for review.  
> 
> I've been doing a lot of work on this recently, and plan to see it
> through if possible.
> 
> So, I'll be a sponsor.
> 
> In the meeting russellb said he would as well. I think he's tied up
> today, so I'm proxying him in here :)
> 
> --Dan
> 

I've already looked at some of this, and some of the work is based on
the work I did for the NUMA blueprint (that Dan contributed to quite a
bit as well) so I'd be happy to make sure this lands too.

N.

> 
> 
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> 


"
NOVA 1402728 - CG,msg34248,<CABib2_qkOgi86dSZwjz1ndPC6PzHnCFDH4A5M5niWxvDoVk07g@mail.gmail.com>,"[openstack-dev] [nova] requesting an FFE for SRIOV

--

In the nova-meeting we agreed this gets a FFE, based on previous
agreements in nova-meetings.

Blueprint is approved for juno-rc1.

Thanks,
John

On 4 September 2014 16:38, Nikola ?ipanov <ndipanov at redhat.com> wrote:
> On 09/04/2014 05:16 PM, Dan Smith wrote:
>>> The main sr-iov patches have gone through lots of code reviews, manual
>>> rebasing, etc. Now we have some critical refactoring work on the
>>> existing infra to get it ready. All the code for refactoring and sr-iov
>>> is up for review.
>>
>> I've been doing a lot of work on this recently, and plan to see it
>> through if possible.
>>
>> So, I'll be a sponsor.
>>
>> In the meeting russellb said he would as well. I think he's tied up
>> today, so I'm proxying him in here :)
>>
>> --Dan
>>
>
> I've already looked at some of this, and some of the work is based on
> the work I did for the NUMA blueprint (that Dan contributed to quite a
> bit as well) so I'd be happy to make sure this lands too.
>
> N.
>
>>
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg34255,<D02F1EFF.B4A3A%baoli@cisco.com>,"[openstack-dev] [nova] requesting an FFE for SRIOV

--

Thanks!!!

?Robert

On 9/5/14, 7:48 AM, ""John Garbutt"" <john at johngarbutt.com> wrote:

>In the nova-meeting we agreed this gets a FFE, based on previous
>agreements in nova-meetings.
>
>Blueprint is approved for juno-rc1.
>
>Thanks,
>John
>
>On 4 September 2014 16:38, Nikola ?ipanov <ndipanov at redhat.com> wrote:
>> On 09/04/2014 05:16 PM, Dan Smith wrote:
>>>> The main sr-iov patches have gone through lots of code reviews, manual
>>>> rebasing, etc. Now we have some critical refactoring work on the
>>>> existing infra to get it ready. All the code for refactoring and
>>>>sr-iov
>>>> is up for review.
>>>
>>> I've been doing a lot of work on this recently, and plan to see it
>>> through if possible.
>>>
>>> So, I'll be a sponsor.
>>>
>>> In the meeting russellb said he would as well. I think he's tied up
>>> today, so I'm proxying him in here :)
>>>
>>> --Dan
>>>
>>
>> I've already looked at some of this, and some of the work is based on
>> the work I did for the NUMA blueprint (that Dan contributed to quite a
>> bit as well) so I'd be happy to make sure this lands too.
>>
>> N.
>>
>>>
>>>
>>> _______________________________________________
>>> OpenStack-dev mailing list
>>> OpenStack-dev at lists.openstack.org
>>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>
>>
>> _______________________________________________
>> OpenStack-dev mailing list
>> OpenStack-dev at lists.openstack.org
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg18311,<CF3BA74E.143F0%sadasu@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi,
    During today's meeting, it was decided that we would re-purpose
Robert's       
https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to
take care of adding a Base class to take care of common processing for
SR-IOV ports.

This class would:

1. Inherits from AgentMechanismDriverBase.
2. Implements bind_port() where the binding:profile would be checked to
see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
3. Also checks to see that port belongs to vendor/product that supports
SR-IOV.
4. This class could be used by MDs that may or may not have a valid L2
agent.
5. Implement validate_port_binding(). This will always return True for Mds
that do not have an L2 agent.

Please let me know if I left out anything.

Thanks,
Sandhya
On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:

>Hi,
>    As a follow up from today's IRC, Irena, are you looking to write the
>below mentioned Base/Mixin class that inherits from
>AgentMechanismDriverBase class? When you mentioned port state, were you
>referring to the validate_port_binding() method?
>
>Pls clarify.
>
>Thanks,
>Sandhya
>
>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi Bob and Irena,
>>   Thanks for the clarification. Irena, I am not opposed to a
>>SriovMechanismDriverBase/Mixin approach, but I want to first figure out
>>how much common functionality there is. Have you already looked at this?
>>
>>Thanks,
>>Sandhya
>>
>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>
>>>Please see inline my understanding
>>>
>>>-----Original Message-----
>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for
>>>usage questions); Irena Berezovsky; Robert Li (baoli); Brian Bowen
>>>(brbowen)
>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>binding of ports
>>>
>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>> Hi,
>>>>      I have a couple of questions for ML2 experts regarding support of
>>>> SR-IOV ports.
>>>
>>>I'll try, but I think these questions might be more about how the
>>>various
>>>SR-IOV implementations will work than about ML2 itself...
>>>
>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2
>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports to
>>>> the host? Will it just be a db update?
>>>
>>>I think whether or not to use an L2 agent depends on the specific SR-IOV
>>>implementation. Some (Mellanox?) might use an L2 agent, while others
>>>(Cisco?) might put information in binding:vif_details that lets the nova
>>>VIF driver take care of setting up the port without an L2 agent.
>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with other
>>>binding:vif_details attributes, VIFDriver should do the VIF pluging
>>>part.
>>>As for required networking configuration is required, it is usually done
>>>either by L2 Agent or external Controller, depends on MD.
>>>
>>>> 
>>>> 2. Also, how do we handle the functionality in mech_agent.py, within
>>>> the SR-IOV context?
>>>
>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent
>>>would
>>>inherit the AgentMechanismDriverBase class if it provides useful
>>>functionality, but any MechanismDriver implementation is free to not use
>>>this base class if its not applicable. I'm not sure if an
>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is being
>>>planned, and how that would relate to AgentMechanismDriverBase.
>>>
>>>[IrenaB] Agree with Bob, and as I stated before I think there is a need
>>>for SriovMechanismDriverBase/Mixin that provides all the generic
>>>functionality and helper methods that are common to SRIOV ports.
>>>-Bob
>>>
>>>> 
>>>> Thanks,
>>>> Sandhya
>>>> 
>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>> Date: Monday, February 3, 2014 3:14 PM
>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky
>>>> <irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li
>>>>(baoli)""
>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>> <rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen
>>>> (brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>> extra hr of discussion today
>>>> 
>>>> Hi,
>>>>     Since, openstack-meeting-alt seems to be in use, baoli and myself
>>>> are moving to openstack-meeting. Hopefully, Bob Kukura & Irena can
>>>> join soon.
>>>> 
>>>> Thanks,
>>>> Sandhya
>>>> 
>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>> Date: Monday, February 3, 2014 1:26 PM
>>>> To: Irena Berezovsky <irenab at mellanox.com
>>>> <mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com
>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>> <mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List (not
>>>>for usage questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>> extra hr of discussion today
>>>> 
>>>> Hi all,
>>>>     Both openstack-meeting and openstack-meeting-alt are available
>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>> 
>>>> Thanks,
>>>> Sandhya
>>>> 
>>>> From: Irena Berezovsky <irenab at mellanox.com
>>>> <mailto:irenab at mellanox.com>>
>>>> Date: Monday, February 3, 2014 12:52 AM
>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, ""Robert
>>>> Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>> <rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""OpenStack
>>>> Development Mailing List (not for usage questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through SRIOV on
>>>> Jan. 30th
>>>> 
>>>> Hi Sandhya,
>>>> 
>>>> Can you please elaborate how do you suggest to extend the below bp for
>>>> SRIOV Ports managed by different Mechanism Driver?
>>>> 
>>>> I am not biased to any specific direction here, just think we need
>>>> common layer for managing SRIOV port at neutron, since there is a
>>>> common pass between nova and neutron.
>>>> 
>>>>  
>>>> 
>>>> BR,
>>>> 
>>>> Irena
>>>> 
>>>>  
>>>> 
>>>>  
>>>> 
>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura; OpenStack
>>>> Development Mailing List (not for usage questions); Brian Bowen
>>>> (brbowen)
>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>> on Jan. 30th
>>>> 
>>>>  
>>>> 
>>>> Hi Irena,
>>>> 
>>>>       I was initially looking
>>>> at 
>>>> 
>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra-por
>>>>t
>>>>-
>>>>info to take care of the extra information required to set up the
>>>>SR-IOV
>>>>port.
>>>> When the scope of the BP was being decided, we had very little info
>>>> about our own design so I didn't give any feedback about SR-IOV ports.
>>>> But, I feel that this is the direction we should be going. Maybe we
>>>> should target this in Juno.
>>>> 
>>>>  
>>>> 
>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet another
>>>> way to take care of extra port config. Let me know what you think.
>>>> 
>>>>  
>>>> 
>>>> Thanks,
>>>> 
>>>> Sandhya
>>>> 
>>>>  
>>>> 
>>>> *From: *Irena Berezovsky <irenab at mellanox.com
>>>> <mailto:irenab at mellanox.com>>
>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>,
>>>> Robert Kukura <rkukura at redhat.com <mailto:rkukura at redhat.com>>,
>>>> Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, ""OpenStack
>>>> Development Mailing List (not for usage questions)""
>>>> <openstack-dev at lists.openstack.org
>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>> on Jan. 30th
>>>> 
>>>>  
>>>> 
>>>> Robert,
>>>> 
>>>> Thank you very much for the summary.
>>>> 
>>>> Please, see inline
>>>> 
>>>>  
>>>> 
>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky;
>>>> OpenStack Development Mailing List (not for usage questions); Brian
>>>> Bowen (brbowen)
>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV on
>>>> Jan. 30th
>>>> 
>>>>  
>>>> 
>>>> Hi,
>>>> 
>>>>  
>>>> 
>>>> We made a lot of progress today. We agreed that:
>>>> 
>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>> 
>>>> -- BPs:
>>>> 
>>>>      * Irena's
>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
>>>> for binding:vnic_type
>>>> 
>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input info
>>>> will be encapsulated in binding:profile
>>>> 
>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV output
>>>> info will be encapsulated in binding:vif_details, which may include
>>>> other information like security parameters. For SRIOV, vlan_id and
>>>> profileid are candidates.
>>>> 
>>>> -- new arguments for port-create will be implicit arguments. Future
>>>> release may make them explicit. New argument: --binding:vnic_type
>>>> {virtio, direct, macvtap}.
>>>> 
>>>> I think that currently we can make do without the profileid as an
>>>> input parameter from the user. The mechanism driver will return a
>>>> profileid in the vif output.
>>>> 
>>>>  
>>>> 
>>>> Please correct any misstatement in above.
>>>> 
>>>>  
>>>> 
>>>> Issues: 
>>>> 
>>>>   -- do we need a common utils/driver for SRIOV generic parts to be
>>>> used by individual Mechanism drivers that support SRIOV? More details
>>>> on what would be included in this sriov utils/driver? I'm thinking
>>>> that a candidate would be the helper functions to interpret the
>>>> pci_slot, which is proposed as a string. Anything else in your mind?
>>>> 
>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and
>>>> persist SRIOV port related attributes/*
>>>> 
>>>>  
>>>> 
>>>>   -- what should mechanism drivers put in binding:vif_details and how
>>>> nova would use this information? as far as I see it from the code, a
>>>> VIF object is created and populated based on information provided by
>>>> neutron (from get network and get port)
>>>> 
>>>>  
>>>> 
>>>> Questions:
>>>> 
>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For regular
>>>> plugins, binding:vnic_type will not be set, I guess. Then would it be
>>>> treated as a virtio type? And if a non-ML2 plugin wants to support
>>>> SRIOV, would it need to  implement vnic-type, binding:profile,
>>>> binding:vif-details for SRIOV itself?
>>>> 
>>>> */[IrenaB] vnic_type will be added as an additional attribute to
>>>> binding extension. For persistency it should be added in
>>>> PortBindingMixin for non ML2. I didn't think to cover it as part of
>>>> ML2 vnic_type bp./*
>>>> 
>>>> */For the rest attributes, need to see what Bob plans./*
>>>> 
>>>>  
>>>> 
>>>>  -- is a neutron agent making decision based on the binding:vif_type?
>>>>  In that case, it makes sense for binding:vnic_type not to be exposed
>>>> to agents.
>>>> 
>>>> */[IrenaB] vnic_type is input parameter that will eventually cause
>>>> certain vif_type to be sent to GenericVIFDriver and create network
>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>> For example, OVS agent will look only for OVS interfaces, so if SRIOV
>>>> interface is created, it won't be discovered by OVS agent./*
>>>> 
>>>>  
>>>> 
>>>> Thanks,
>>>> 
>>>> Robert
>>>> 
>>>
>>
>>
>>_______________________________________________
>>OpenStack-dev mailing list
>>OpenStack-dev at lists.openstack.org
>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1402728 - CG,msg18344,<CF3BC3A4.5A8E1%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Sandhya,

I agree with you except that I think that the class should inherit from
MechanismDriver. I took a crack at it, and here is what I got:

# Copyright (c) 2014 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you
may
#    not use this file except in compliance with the License. You may
obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
the
#    License for the specific language governing permissions and
limitations
#    under the License.

from abc import ABCMeta, abstractmethod

import functools
import six

from neutron.extensions import portbindings
from neutron.openstack.common import log
from neutron.plugins.ml2 import driver_api as api

LOG = log.getLogger(__name__)


DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
                                portbindings.VNIC_MACVTAP]

def check_vnic_type_and_vendor_info(f):
    @functools.wraps(f)
    def wrapper(self, context):
        vnic_type = context.current.get(portbindings.VNIC_TYPE,
                                        portbindings.VNIC_NORMAL)
        if vnic_type not in self.supported_vnic_types:
            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
                        ""vnic_type: %(vnic_type)s""),
                      {'func_name': f.func_name, 'vnic_type': vnic_type})
            return

        if self.supported_pci_vendor_info:
            profile = context.current.get(portbindings.PROFILE, {})
            if not profile:
                LOG.debug(_(""%s: Missing profile in port binding""),
                          f.func_name)
                return
            pci_vendor_info = profile.get('pci_vendor_info')
            if not pci_vendor_info:
                LOG.debug(_(""%s: Missing pci vendor info in profile""),
                          f.func_name)
                return
            if pci_vendor_info not in self.supported_pci_vendor_info:
                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
                            ""info: %(info)s""),
                          {'func_name': f.func_name, 'info':
pci_vendor_info})
                return
        f(self, context)
    return wrapper

@six.add_metaclass(ABCMeta)
class SriovMechanismDriverBase(api.MechanismDriver):
    """"""Base class for drivers that supports SR-IOV

    The SriovMechanismDriverBase provides common code for mechanism
    drivers that supports SR-IOV. Such a driver may or may not require
    an agent to be running on the port's host.

    MechanismDrivers that uses this base class and requires an agent must
    pass the agent type to __init__(), and must implement
    try_to_bind_segment_for_agent() and check_segment_for_agent().

    MechanismDrivers that uses this base class may provide supported vendor
    information, and must provide the supported vnic types.
    """"""
    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
        """"""Initialize base class for SR-IOV capable Mechanism Drivers

        :param agent_type: Constant identifying agent type in agents_db
        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
        :param supported_vnic_types: The binding:vnic_type values we can
bind
        """"""
        self.supported_pci_vendor_info = supported_pci_vendor_info
        self.agent_type = agent_type
        self.supported_vnic_types = supported_vnic_types

    def initialize(self):
        pass

    @check_vnic_type_and_vendor_info
    def bind_port(self, context):
        LOG.debug(_(""Attempting to bind port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive']:
                    for segment in context.network.network_segments:
                        if self.try_to_bind_segment_for_agent(context,
segment,
                                                              agent):
                            LOG.debug(_(""Bound using segment: %s""),
segment)
                            return
                else:
                    LOG.warning(_(""Attempting to bind with dead agent:
%s""),
                                agent)
        else:
            for segment in context.network.network_segments:
                if self.try_to_bind_segment(context, segment):
                    LOG.debug(_(""Bound using segment: %s""), segment)
                    return

    def validate_port_binding(self, context):
        LOG.debug(_(""Validating binding for port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})
        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive'] and self.check_segment_for_agent(
                    context.bound_segment, agent):
                    LOG.debug(_(""Binding valid""))
                    return True
            LOG.warning(_(""Binding invalid for port: %s""), context.current)
            return False
        else:
            return True

    @check_vnic_type_and_vendor_info
    def unbind_port(self, context):
        LOG.debug(_(""Unbinding port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

    @abstractmethod
    def try_to_bind_segment_for_agent(self, context, segment, agent):
        """"""Try to bind with segment for agent.

        :param context: PortContext instance describing the port
        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment has been bound for agent

        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use agent_db data along with built-in
        knowledge of the corresponding agent's capabilities to attempt
        to bind to the specified network segment for the agent.

        If the segment can be bound for the agent, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""

    @abstractmethod
    def check_segment_for_agent(self, segment, agent):
        """"""Check if segment can be bound for agent.

        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment can be bound for agent

        Called inside transaction during validate_port_binding() so
        that derived MechanismDrivers can use agent_db data along with
        built-in knowledge of the corresponding agent's capabilities
        to determine whether or not the specified network segment can
        be bound for the agent.
        """"""

    @abstractmethod
    def try_to_bind_segment(self, segment):
        """"""Check if segment can be bound.

        :param segment: segment dictionary describing segment to bind
        :returns: True iff segment can be bound
        
        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use database data along with built-in
        knowledge to attempt to bind to the specified network segment

        If the segment can be bound, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""



A SRIOV MD would inherit from it and implement
try_to_bind_segment_for_agent() and check_segment_for_agent() and
try_to_bind_segment(). If an agent is required, the first two methods
should have concrete implementation, and the third may only contain
'pass'. Otherwise, the first two are 'pass', and the third needs to be
implemented.

In the inherited class, its __init__ method should set the supported
pci_vendor_info by either hard coding or by configuration (which is
preferred so that code doesn't need to be changed with newly supported
cards).
in the bind segment method, vif_type and vif_details should be set, and
set_binding() called.

methods inherited from MechanismDriver, such as
create_port_precommit()/postcommit(), update_port_precommit()/postcommit
can be decorated with check_vnic_type_and_vendor_info so that they won't
be called if vnic_type and vendor_info don't match.

Let me know what you think.

thanks,
Robert




On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:

>Hi,
>    During today's meeting, it was decided that we would re-purpose
>Robert's       
>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to
>take care of adding a Base class to take care of common processing for
>SR-IOV ports.
>
>This class would:
>
>1. Inherits from AgentMechanismDriverBase.
>2. Implements bind_port() where the binding:profile would be checked to
>see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>3. Also checks to see that port belongs to vendor/product that supports
>SR-IOV.
>4. This class could be used by MDs that may or may not have a valid L2
>agent.
>5. Implement validate_port_binding(). This will always return True for Mds
>that do not have an L2 agent.
>
>Please let me know if I left out anything.
>
>Thanks,
>Sandhya
>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    As a follow up from today's IRC, Irena, are you looking to write the
>>below mentioned Base/Mixin class that inherits from
>>AgentMechanismDriverBase class? When you mentioned port state, were you
>>referring to the validate_port_binding() method?
>>
>>Pls clarify.
>>
>>Thanks,
>>Sandhya
>>
>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi Bob and Irena,
>>>   Thanks for the clarification. Irena, I am not opposed to a
>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure out
>>>how much common functionality there is. Have you already looked at this?
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>
>>>>Please see inline my understanding
>>>>
>>>>-----Original Message-----
>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for
>>>>usage questions); Irena Berezovsky; Robert Li (baoli); Brian Bowen
>>>>(brbowen)
>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>binding of ports
>>>>
>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>> Hi,
>>>>>      I have a couple of questions for ML2 experts regarding support
>>>>>of
>>>>> SR-IOV ports.
>>>>
>>>>I'll try, but I think these questions might be more about how the
>>>>various
>>>>SR-IOV implementations will work than about ML2 itself...
>>>>
>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2
>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports to
>>>>> the host? Will it just be a db update?
>>>>
>>>>I think whether or not to use an L2 agent depends on the specific
>>>>SR-IOV
>>>>implementation. Some (Mellanox?) might use an L2 agent, while others
>>>>(Cisco?) might put information in binding:vif_details that lets the
>>>>nova
>>>>VIF driver take care of setting up the port without an L2 agent.
>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with
>>>>other
>>>>binding:vif_details attributes, VIFDriver should do the VIF pluging
>>>>part.
>>>>As for required networking configuration is required, it is usually
>>>>done
>>>>either by L2 Agent or external Controller, depends on MD.
>>>>
>>>>> 
>>>>> 2. Also, how do we handle the functionality in mech_agent.py, within
>>>>> the SR-IOV context?
>>>>
>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent
>>>>would
>>>>inherit the AgentMechanismDriverBase class if it provides useful
>>>>functionality, but any MechanismDriver implementation is free to not
>>>>use
>>>>this base class if its not applicable. I'm not sure if an
>>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is being
>>>>planned, and how that would relate to AgentMechanismDriverBase.
>>>>
>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a need
>>>>for SriovMechanismDriverBase/Mixin that provides all the generic
>>>>functionality and helper methods that are common to SRIOV ports.
>>>>-Bob
>>>>
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky
>>>>> <irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li
>>>>>(baoli)""
>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>>> <rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen
>>>>> (brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>> extra hr of discussion today
>>>>> 
>>>>> Hi,
>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and myself
>>>>> are moving to openstack-meeting. Hopefully, Bob Kukura & Irena can
>>>>> join soon.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>> To: Irena Berezovsky <irenab at mellanox.com
>>>>> <mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com
>>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>>> <mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List
>>>>>(not
>>>>>for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>> extra hr of discussion today
>>>>> 
>>>>> Hi all,
>>>>>     Both openstack-meeting and openstack-meeting-alt are available
>>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Irena Berezovsky <irenab at mellanox.com
>>>>> <mailto:irenab at mellanox.com>>
>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>,
>>>>>""Robert
>>>>> Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>>> <rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""OpenStack
>>>>> Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>>on
>>>>> Jan. 30th
>>>>> 
>>>>> Hi Sandhya,
>>>>> 
>>>>> Can you please elaborate how do you suggest to extend the below bp
>>>>>for
>>>>> SRIOV Ports managed by different Mechanism Driver?
>>>>> 
>>>>> I am not biased to any specific direction here, just think we need
>>>>> common layer for managing SRIOV port at neutron, since there is a
>>>>> common pass between nova and neutron.
>>>>> 
>>>>>  
>>>>> 
>>>>> BR,
>>>>> 
>>>>> Irena
>>>>> 
>>>>>  
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura; OpenStack
>>>>> Development Mailing List (not for usage questions); Brian Bowen
>>>>> (brbowen)
>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>> on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi Irena,
>>>>> 
>>>>>       I was initially looking
>>>>> at 
>>>>> 
>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra-po
>>>>>r
>>>>>t
>>>>>-
>>>>>info to take care of the extra information required to set up the
>>>>>SR-IOV
>>>>>port.
>>>>> When the scope of the BP was being decided, we had very little info
>>>>> about our own design so I didn't give any feedback about SR-IOV
>>>>>ports.
>>>>> But, I feel that this is the direction we should be going. Maybe we
>>>>> should target this in Juno.
>>>>> 
>>>>>  
>>>>> 
>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet another
>>>>> way to take care of extra port config. Let me know what you think.
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Sandhya
>>>>> 
>>>>>  
>>>>> 
>>>>> *From: *Irena Berezovsky <irenab at mellanox.com
>>>>> <mailto:irenab at mellanox.com>>
>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>,
>>>>> Robert Kukura <rkukura at redhat.com <mailto:rkukura at redhat.com>>,
>>>>> Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, ""OpenStack
>>>>> Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>> on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Robert,
>>>>> 
>>>>> Thank you very much for the summary.
>>>>> 
>>>>> Please, see inline
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky;
>>>>> OpenStack Development Mailing List (not for usage questions); Brian
>>>>> Bowen (brbowen)
>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV on
>>>>> Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi,
>>>>> 
>>>>>  
>>>>> 
>>>>> We made a lot of progress today. We agreed that:
>>>>> 
>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>> 
>>>>> -- BPs:
>>>>> 
>>>>>      * Irena's
>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type
>>>>> for binding:vnic_type
>>>>> 
>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input
>>>>>info
>>>>> will be encapsulated in binding:profile
>>>>> 
>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV
>>>>>output
>>>>> info will be encapsulated in binding:vif_details, which may include
>>>>> other information like security parameters. For SRIOV, vlan_id and
>>>>> profileid are candidates.
>>>>> 
>>>>> -- new arguments for port-create will be implicit arguments. Future
>>>>> release may make them explicit. New argument: --binding:vnic_type
>>>>> {virtio, direct, macvtap}.
>>>>> 
>>>>> I think that currently we can make do without the profileid as an
>>>>> input parameter from the user. The mechanism driver will return a
>>>>> profileid in the vif output.
>>>>> 
>>>>>  
>>>>> 
>>>>> Please correct any misstatement in above.
>>>>> 
>>>>>  
>>>>> 
>>>>> Issues: 
>>>>> 
>>>>>   -- do we need a common utils/driver for SRIOV generic parts to be
>>>>> used by individual Mechanism drivers that support SRIOV? More details
>>>>> on what would be included in this sriov utils/driver? I'm thinking
>>>>> that a candidate would be the helper functions to interpret the
>>>>> pci_slot, which is proposed as a string. Anything else in your mind?
>>>>> 
>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and
>>>>> persist SRIOV port related attributes/*
>>>>> 
>>>>>  
>>>>> 
>>>>>   -- what should mechanism drivers put in binding:vif_details and how
>>>>> nova would use this information? as far as I see it from the code, a
>>>>> VIF object is created and populated based on information provided by
>>>>> neutron (from get network and get port)
>>>>> 
>>>>>  
>>>>> 
>>>>> Questions:
>>>>> 
>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For
>>>>>regular
>>>>> plugins, binding:vnic_type will not be set, I guess. Then would it be
>>>>> treated as a virtio type? And if a non-ML2 plugin wants to support
>>>>> SRIOV, would it need to  implement vnic-type, binding:profile,
>>>>> binding:vif-details for SRIOV itself?
>>>>> 
>>>>> */[IrenaB] vnic_type will be added as an additional attribute to
>>>>> binding extension. For persistency it should be added in
>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part of
>>>>> ML2 vnic_type bp./*
>>>>> 
>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>> 
>>>>>  
>>>>> 
>>>>>  -- is a neutron agent making decision based on the binding:vif_type?
>>>>>  In that case, it makes sense for binding:vnic_type not to be exposed
>>>>> to agents.
>>>>> 
>>>>> */[IrenaB] vnic_type is input parameter that will eventually cause
>>>>> certain vif_type to be sent to GenericVIFDriver and create network
>>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>>> For example, OVS agent will look only for OVS interfaces, so if SRIOV
>>>>> interface is created, it won't be discovered by OVS agent./*
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Robert
>>>>> 
>>>>
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
>>_______________________________________________
>>OpenStack-dev mailing list
>>OpenStack-dev at lists.openstack.org
>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>


"
NOVA 1402728 - CG,msg18354,<9D25E123B44F4A4291F4B5C13DA94E7795ED9B3A@MTLDAG01.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Robert,
Seems to me that many code lines are duplicated following your proposal.
For agent based MDs, I would prefer to inherit from  SimpleAgentMechanismDriverBase and add there verify method for supported_pci_vendor_info. Specific MD will pass the list of supported pci_vendor_info list. The  'try_to_bind_segment_for_agent' method will call 'supported_pci_vendor_info', and if supported continue with binding flow. 
Maybe instead of a decorator method, it should be just an utility method?
I think that the check for supported vnic_type and pci_vendor info support, should be done in order to see if MD should bind the port. If the answer is Yes, no more checks are required.

Coming back to the question I asked earlier, for non-agent MD, how would you deal with updates after port is bound, like 'admin_state_up' changes?
I'll try to push some reference code later today.

BR,
Irena

-----Original Message-----
From: Robert Li (baoli) [mailto:baoli at cisco.com] 
Sent: Wednesday, March 05, 2014 4:46 AM
To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for usage questions); Irena Berezovsky; Robert Kukura; Brian Bowen (brbowen)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV binding of ports

Hi Sandhya,

I agree with you except that I think that the class should inherit from MechanismDriver. I took a crack at it, and here is what I got:

# Copyright (c) 2014 OpenStack Foundation # All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you
may
#    not use this file except in compliance with the License. You may
obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
the
#    License for the specific language governing permissions and
limitations
#    under the License.

from abc import ABCMeta, abstractmethod

import functools
import six

from neutron.extensions import portbindings from neutron.openstack.common import log from neutron.plugins.ml2 import driver_api as api

LOG = log.getLogger(__name__)


DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
                                portbindings.VNIC_MACVTAP]

def check_vnic_type_and_vendor_info(f):
    @functools.wraps(f)
    def wrapper(self, context):
        vnic_type = context.current.get(portbindings.VNIC_TYPE,
                                        portbindings.VNIC_NORMAL)
        if vnic_type not in self.supported_vnic_types:
            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
                        ""vnic_type: %(vnic_type)s""),
                      {'func_name': f.func_name, 'vnic_type': vnic_type})
            return

        if self.supported_pci_vendor_info:
            profile = context.current.get(portbindings.PROFILE, {})
            if not profile:
                LOG.debug(_(""%s: Missing profile in port binding""),
                          f.func_name)
                return
            pci_vendor_info = profile.get('pci_vendor_info')
            if not pci_vendor_info:
                LOG.debug(_(""%s: Missing pci vendor info in profile""),
                          f.func_name)
                return
            if pci_vendor_info not in self.supported_pci_vendor_info:
                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
                            ""info: %(info)s""),
                          {'func_name': f.func_name, 'info':
pci_vendor_info})
                return
        f(self, context)
    return wrapper

@six.add_metaclass(ABCMeta)
class SriovMechanismDriverBase(api.MechanismDriver):
    """"""Base class for drivers that supports SR-IOV

    The SriovMechanismDriverBase provides common code for mechanism
    drivers that supports SR-IOV. Such a driver may or may not require
    an agent to be running on the port's host.

    MechanismDrivers that uses this base class and requires an agent must
    pass the agent type to __init__(), and must implement
    try_to_bind_segment_for_agent() and check_segment_for_agent().

    MechanismDrivers that uses this base class may provide supported vendor
    information, and must provide the supported vnic types.
    """"""
    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
        """"""Initialize base class for SR-IOV capable Mechanism Drivers

        :param agent_type: Constant identifying agent type in agents_db
        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
        :param supported_vnic_types: The binding:vnic_type values we can bind
        """"""
        self.supported_pci_vendor_info = supported_pci_vendor_info
        self.agent_type = agent_type
        self.supported_vnic_types = supported_vnic_types

    def initialize(self):
        pass

    @check_vnic_type_and_vendor_info
    def bind_port(self, context):
        LOG.debug(_(""Attempting to bind port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive']:
                    for segment in context.network.network_segments:
                        if self.try_to_bind_segment_for_agent(context,
segment,
                                                              agent):
                            LOG.debug(_(""Bound using segment: %s""),
segment)
                            return
                else:
                    LOG.warning(_(""Attempting to bind with dead agent:
%s""),
                                agent)
        else:
            for segment in context.network.network_segments:
                if self.try_to_bind_segment(context, segment):
                    LOG.debug(_(""Bound using segment: %s""), segment)
                    return

    def validate_port_binding(self, context):
        LOG.debug(_(""Validating binding for port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})
        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive'] and self.check_segment_for_agent(
                    context.bound_segment, agent):
                    LOG.debug(_(""Binding valid""))
                    return True
            LOG.warning(_(""Binding invalid for port: %s""), context.current)
            return False
        else:
            return True

    @check_vnic_type_and_vendor_info
    def unbind_port(self, context):
        LOG.debug(_(""Unbinding port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

    @abstractmethod
    def try_to_bind_segment_for_agent(self, context, segment, agent):
        """"""Try to bind with segment for agent.

        :param context: PortContext instance describing the port
        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment has been bound for agent

        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use agent_db data along with built-in
        knowledge of the corresponding agent's capabilities to attempt
        to bind to the specified network segment for the agent.

        If the segment can be bound for the agent, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""

    @abstractmethod
    def check_segment_for_agent(self, segment, agent):
        """"""Check if segment can be bound for agent.

        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment can be bound for agent

        Called inside transaction during validate_port_binding() so
        that derived MechanismDrivers can use agent_db data along with
        built-in knowledge of the corresponding agent's capabilities
        to determine whether or not the specified network segment can
        be bound for the agent.
        """"""

    @abstractmethod
    def try_to_bind_segment(self, segment):
        """"""Check if segment can be bound.

        :param segment: segment dictionary describing segment to bind
        :returns: True iff segment can be bound
        
        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use database data along with built-in
        knowledge to attempt to bind to the specified network segment

        If the segment can be bound, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""



A SRIOV MD would inherit from it and implement
try_to_bind_segment_for_agent() and check_segment_for_agent() and try_to_bind_segment(). If an agent is required, the first two methods should have concrete implementation, and the third may only contain 'pass'. Otherwise, the first two are 'pass', and the third needs to be implemented.

In the inherited class, its __init__ method should set the supported pci_vendor_info by either hard coding or by configuration (which is preferred so that code doesn't need to be changed with newly supported cards).
in the bind segment method, vif_type and vif_details should be set, and
set_binding() called.

methods inherited from MechanismDriver, such as create_port_precommit()/postcommit(), update_port_precommit()/postcommit
can be decorated with check_vnic_type_and_vendor_info so that they won't be called if vnic_type and vendor_info don't match.

Let me know what you think.

thanks,
Robert




On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:

>Hi,
>    During today's meeting, it was decided that we would re-purpose
>Robert's       
>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to 
>take care of adding a Base class to take care of common processing for 
>SR-IOV ports.
>
>This class would:
>
>1. Inherits from AgentMechanismDriverBase.
>2. Implements bind_port() where the binding:profile would be checked to 
>see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>3. Also checks to see that port belongs to vendor/product that supports 
>SR-IOV.
>4. This class could be used by MDs that may or may not have a valid L2 
>agent.
>5. Implement validate_port_binding(). This will always return True for 
>Mds that do not have an L2 agent.
>
>Please let me know if I left out anything.
>
>Thanks,
>Sandhya
>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    As a follow up from today's IRC, Irena, are you looking to write 
>>the below mentioned Base/Mixin class that inherits from 
>>AgentMechanismDriverBase class? When you mentioned port state, were 
>>you referring to the validate_port_binding() method?
>>
>>Pls clarify.
>>
>>Thanks,
>>Sandhya
>>
>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi Bob and Irena,
>>>   Thanks for the clarification. Irena, I am not opposed to a 
>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure 
>>>out how much common functionality there is. Have you already looked at this?
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>
>>>>Please see inline my understanding
>>>>
>>>>-----Original Message-----
>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not 
>>>>for usage questions); Irena Berezovsky; Robert Li (baoli); Brian 
>>>>Bowen
>>>>(brbowen)
>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>binding of ports
>>>>
>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>> Hi,
>>>>>      I have a couple of questions for ML2 experts regarding 
>>>>>support of  SR-IOV ports.
>>>>
>>>>I'll try, but I think these questions might be more about how the 
>>>>various SR-IOV implementations will work than about ML2 itself...
>>>>
>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2 
>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports 
>>>>> to the host? Will it just be a db update?
>>>>
>>>>I think whether or not to use an L2 agent depends on the specific 
>>>>SR-IOV implementation. Some (Mellanox?) might use an L2 agent, while 
>>>>others
>>>>(Cisco?) might put information in binding:vif_details that lets the 
>>>>nova VIF driver take care of setting up the port without an L2 
>>>>agent.
>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with 
>>>>other binding:vif_details attributes, VIFDriver should do the VIF 
>>>>pluging part.
>>>>As for required networking configuration is required, it is usually 
>>>>done either by L2 Agent or external Controller, depends on MD.
>>>>
>>>>> 
>>>>> 2. Also, how do we handle the functionality in mech_agent.py, 
>>>>> within the SR-IOV context?
>>>>
>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent 
>>>>would inherit the AgentMechanismDriverBase class if it provides 
>>>>useful functionality, but any MechanismDriver implementation is free 
>>>>to not use this base class if its not applicable. I'm not sure if an 
>>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is 
>>>>being planned, and how that would relate to AgentMechanismDriverBase.
>>>>
>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a 
>>>>need for SriovMechanismDriverBase/Mixin that provides all the 
>>>>generic functionality and helper methods that are common to SRIOV ports.
>>>>-Bob
>>>>
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky  
>>>>><irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li 
>>>>>(baoli)""
>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura  
>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen  
>>>>>(brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV  extra hr of discussion today
>>>>> 
>>>>> Hi,
>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and 
>>>>> myself are moving to openstack-meeting. Hopefully, Bob Kukura & 
>>>>> Irena can join soon.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>> To: Irena Berezovsky <irenab at mellanox.com  
>>>>><mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com  
>>>>><mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com  
>>>>><mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List 
>>>>>(not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV  extra hr of discussion today
>>>>> 
>>>>> Hi all,
>>>>>     Both openstack-meeting and openstack-meeting-alt are available 
>>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Irena Berezovsky <irenab at mellanox.com  
>>>>><mailto:irenab at mellanox.com>>
>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, 
>>>>>""Robert  Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>, 
>>>>>Robert Kukura  <rkukura at redhat.com <mailto:rkukura at redhat.com>>, 
>>>>>""OpenStack  Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV on  Jan. 30th
>>>>> 
>>>>> Hi Sandhya,
>>>>> 
>>>>> Can you please elaborate how do you suggest to extend the below bp 
>>>>>for  SRIOV Ports managed by different Mechanism Driver?
>>>>> 
>>>>> I am not biased to any specific direction here, just think we need 
>>>>> common layer for managing SRIOV port at neutron, since there is a 
>>>>> common pass between nova and neutron.
>>>>> 
>>>>>  
>>>>> 
>>>>> BR,
>>>>> 
>>>>> Irena
>>>>> 
>>>>>  
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura; 
>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>> Brian Bowen
>>>>> (brbowen)
>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>> SRIOV on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi Irena,
>>>>> 
>>>>>       I was initially looking
>>>>> at
>>>>> 
>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra
>>>>>-po
>>>>>r
>>>>>t
>>>>>-
>>>>>info to take care of the extra information required to set up the 
>>>>>SR-IOV port.
>>>>> When the scope of the BP was being decided, we had very little 
>>>>>info  about our own design so I didn't give any feedback about 
>>>>>SR-IOV ports.
>>>>> But, I feel that this is the direction we should be going. Maybe 
>>>>>we  should target this in Juno.
>>>>> 
>>>>>  
>>>>> 
>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet 
>>>>> another way to take care of extra port config. Let me know what you think.
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Sandhya
>>>>> 
>>>>>  
>>>>> 
>>>>> *From: *Irena Berezovsky <irenab at mellanox.com 
>>>>> <mailto:irenab at mellanox.com>>
>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com 
>>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com 
>>>>> <mailto:rkukura at redhat.com>>, Sandhya Dasu <sadasu at cisco.com 
>>>>> <mailto:sadasu at cisco.com>>, ""OpenStack Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>> SRIOV on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Robert,
>>>>> 
>>>>> Thank you very much for the summary.
>>>>> 
>>>>> Please, see inline
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky; 
>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>> Brian Bowen (brbowen)
>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>> on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi,
>>>>> 
>>>>>  
>>>>> 
>>>>> We made a lot of progress today. We agreed that:
>>>>> 
>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>> 
>>>>> -- BPs:
>>>>> 
>>>>>      * Irena's
>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-ty
>>>>> pe
>>>>> for binding:vnic_type
>>>>> 
>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input 
>>>>>info  will be encapsulated in binding:profile
>>>>> 
>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV 
>>>>>output  info will be encapsulated in binding:vif_details, which may 
>>>>>include  other information like security parameters. For SRIOV, 
>>>>>vlan_id and  profileid are candidates.
>>>>> 
>>>>> -- new arguments for port-create will be implicit arguments. 
>>>>> Future release may make them explicit. New argument: 
>>>>> --binding:vnic_type {virtio, direct, macvtap}.
>>>>> 
>>>>> I think that currently we can make do without the profileid as an 
>>>>> input parameter from the user. The mechanism driver will return a 
>>>>> profileid in the vif output.
>>>>> 
>>>>>  
>>>>> 
>>>>> Please correct any misstatement in above.
>>>>> 
>>>>>  
>>>>> 
>>>>> Issues: 
>>>>> 
>>>>>   -- do we need a common utils/driver for SRIOV generic parts to 
>>>>> be used by individual Mechanism drivers that support SRIOV? More 
>>>>> details on what would be included in this sriov utils/driver? I'm 
>>>>> thinking that a candidate would be the helper functions to 
>>>>> interpret the pci_slot, which is proposed as a string. Anything else in your mind?
>>>>> 
>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and 
>>>>> persist SRIOV port related attributes/*
>>>>> 
>>>>>  
>>>>> 
>>>>>   -- what should mechanism drivers put in binding:vif_details and 
>>>>> how nova would use this information? as far as I see it from the 
>>>>> code, a VIF object is created and populated based on information 
>>>>> provided by neutron (from get network and get port)
>>>>> 
>>>>>  
>>>>> 
>>>>> Questions:
>>>>> 
>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For 
>>>>>regular  plugins, binding:vnic_type will not be set, I guess. Then 
>>>>>would it be  treated as a virtio type? And if a non-ML2 plugin 
>>>>>wants to support  SRIOV, would it need to  implement vnic-type, 
>>>>>binding:profile,  binding:vif-details for SRIOV itself?
>>>>> 
>>>>> */[IrenaB] vnic_type will be added as an additional attribute to 
>>>>> binding extension. For persistency it should be added in 
>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part 
>>>>> of
>>>>> ML2 vnic_type bp./*
>>>>> 
>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>> 
>>>>>  
>>>>> 
>>>>>  -- is a neutron agent making decision based on the binding:vif_type?
>>>>>  In that case, it makes sense for binding:vnic_type not to be 
>>>>> exposed to agents.
>>>>> 
>>>>> */[IrenaB] vnic_type is input parameter that will eventually cause 
>>>>> certain vif_type to be sent to GenericVIFDriver and create network 
>>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>>> For example, OVS agent will look only for OVS interfaces, so if 
>>>>> SRIOV interface is created, it won't be discovered by OVS agent./*
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Robert
>>>>> 
>>>>
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
>>_______________________________________________
>>OpenStack-dev mailing list
>>OpenStack-dev at lists.openstack.org
>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>


"
NOVA 1402728 - CG,msg18368,<9D25E123B44F4A4291F4B5C13DA94E7795EDA220@MTLDAG01.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Robert, Sandhya,
I have pushed the reference implementation SriovAgentMechanismDriverBase as part the following WIP:
https://review.openstack.org/#/c/74464/

The code is in mech_agent.py, and very simple code for mech_sriov_nic_switch.py.

Please take a look and review.

BR,
Irena

-----Original Message-----
From: Irena Berezovsky [mailto:irenab at mellanox.com] 
Sent: Wednesday, March 05, 2014 9:04 AM
To: Robert Li (baoli); Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for usage questions); Robert Kukura; Brian Bowen (brbowen)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV binding of ports

Hi Robert,
Seems to me that many code lines are duplicated following your proposal.
For agent based MDs, I would prefer to inherit from  SimpleAgentMechanismDriverBase and add there verify method for supported_pci_vendor_info. Specific MD will pass the list of supported pci_vendor_info list. The  'try_to_bind_segment_for_agent' method will call 'supported_pci_vendor_info', and if supported continue with binding flow. 
Maybe instead of a decorator method, it should be just an utility method?
I think that the check for supported vnic_type and pci_vendor info support, should be done in order to see if MD should bind the port. If the answer is Yes, no more checks are required.

Coming back to the question I asked earlier, for non-agent MD, how would you deal with updates after port is bound, like 'admin_state_up' changes?
I'll try to push some reference code later today.

BR,
Irena

-----Original Message-----
From: Robert Li (baoli) [mailto:baoli at cisco.com]
Sent: Wednesday, March 05, 2014 4:46 AM
To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for usage questions); Irena Berezovsky; Robert Kukura; Brian Bowen (brbowen)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV binding of ports

Hi Sandhya,

I agree with you except that I think that the class should inherit from MechanismDriver. I took a crack at it, and here is what I got:

# Copyright (c) 2014 OpenStack Foundation # All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you
may
#    not use this file except in compliance with the License. You may
obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
the
#    License for the specific language governing permissions and
limitations
#    under the License.

from abc import ABCMeta, abstractmethod

import functools
import six

from neutron.extensions import portbindings from neutron.openstack.common import log from neutron.plugins.ml2 import driver_api as api

LOG = log.getLogger(__name__)


DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
                                portbindings.VNIC_MACVTAP]

def check_vnic_type_and_vendor_info(f):
    @functools.wraps(f)
    def wrapper(self, context):
        vnic_type = context.current.get(portbindings.VNIC_TYPE,
                                        portbindings.VNIC_NORMAL)
        if vnic_type not in self.supported_vnic_types:
            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
                        ""vnic_type: %(vnic_type)s""),
                      {'func_name': f.func_name, 'vnic_type': vnic_type})
            return

        if self.supported_pci_vendor_info:
            profile = context.current.get(portbindings.PROFILE, {})
            if not profile:
                LOG.debug(_(""%s: Missing profile in port binding""),
                          f.func_name)
                return
            pci_vendor_info = profile.get('pci_vendor_info')
            if not pci_vendor_info:
                LOG.debug(_(""%s: Missing pci vendor info in profile""),
                          f.func_name)
                return
            if pci_vendor_info not in self.supported_pci_vendor_info:
                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
                            ""info: %(info)s""),
                          {'func_name': f.func_name, 'info':
pci_vendor_info})
                return
        f(self, context)
    return wrapper

@six.add_metaclass(ABCMeta)
class SriovMechanismDriverBase(api.MechanismDriver):
    """"""Base class for drivers that supports SR-IOV

    The SriovMechanismDriverBase provides common code for mechanism
    drivers that supports SR-IOV. Such a driver may or may not require
    an agent to be running on the port's host.

    MechanismDrivers that uses this base class and requires an agent must
    pass the agent type to __init__(), and must implement
    try_to_bind_segment_for_agent() and check_segment_for_agent().

    MechanismDrivers that uses this base class may provide supported vendor
    information, and must provide the supported vnic types.
    """"""
    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
        """"""Initialize base class for SR-IOV capable Mechanism Drivers

        :param agent_type: Constant identifying agent type in agents_db
        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
        :param supported_vnic_types: The binding:vnic_type values we can bind
        """"""
        self.supported_pci_vendor_info = supported_pci_vendor_info
        self.agent_type = agent_type
        self.supported_vnic_types = supported_vnic_types

    def initialize(self):
        pass

    @check_vnic_type_and_vendor_info
    def bind_port(self, context):
        LOG.debug(_(""Attempting to bind port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive']:
                    for segment in context.network.network_segments:
                        if self.try_to_bind_segment_for_agent(context,
segment,
                                                              agent):
                            LOG.debug(_(""Bound using segment: %s""),
segment)
                            return
                else:
                    LOG.warning(_(""Attempting to bind with dead agent:
%s""),
                                agent)
        else:
            for segment in context.network.network_segments:
                if self.try_to_bind_segment(context, segment):
                    LOG.debug(_(""Bound using segment: %s""), segment)
                    return

    def validate_port_binding(self, context):
        LOG.debug(_(""Validating binding for port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})
        if self.agent_type:
            for agent in context.host_agents(self.agent_type):
                LOG.debug(_(""Checking agent: %s""), agent)
                if agent['alive'] and self.check_segment_for_agent(
                    context.bound_segment, agent):
                    LOG.debug(_(""Binding valid""))
                    return True
            LOG.warning(_(""Binding invalid for port: %s""), context.current)
            return False
        else:
            return True

    @check_vnic_type_and_vendor_info
    def unbind_port(self, context):
        LOG.debug(_(""Unbinding port %(port)s on ""
                    ""network %(network)s""),
                  {'port': context.current['id'],
                   'network': context.network.current['id']})

    @abstractmethod
    def try_to_bind_segment_for_agent(self, context, segment, agent):
        """"""Try to bind with segment for agent.

        :param context: PortContext instance describing the port
        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment has been bound for agent

        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use agent_db data along with built-in
        knowledge of the corresponding agent's capabilities to attempt
        to bind to the specified network segment for the agent.

        If the segment can be bound for the agent, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""

    @abstractmethod
    def check_segment_for_agent(self, segment, agent):
        """"""Check if segment can be bound for agent.

        :param segment: segment dictionary describing segment to bind
        :param agent: agents_db entry describing agent to bind
        :returns: True iff segment can be bound for agent

        Called inside transaction during validate_port_binding() so
        that derived MechanismDrivers can use agent_db data along with
        built-in knowledge of the corresponding agent's capabilities
        to determine whether or not the specified network segment can
        be bound for the agent.
        """"""

    @abstractmethod
    def try_to_bind_segment(self, segment):
        """"""Check if segment can be bound.

        :param segment: segment dictionary describing segment to bind
        :returns: True iff segment can be bound
        
        Called inside transaction during bind_port() so that derived
        MechanismDrivers can use database data along with built-in
        knowledge to attempt to bind to the specified network segment

        If the segment can be bound, this function must
        call context.set_binding() with appropriate values and then
        return True. Otherwise, it must return False.
        """"""



A SRIOV MD would inherit from it and implement
try_to_bind_segment_for_agent() and check_segment_for_agent() and try_to_bind_segment(). If an agent is required, the first two methods should have concrete implementation, and the third may only contain 'pass'. Otherwise, the first two are 'pass', and the third needs to be implemented.

In the inherited class, its __init__ method should set the supported pci_vendor_info by either hard coding or by configuration (which is preferred so that code doesn't need to be changed with newly supported cards).
in the bind segment method, vif_type and vif_details should be set, and
set_binding() called.

methods inherited from MechanismDriver, such as create_port_precommit()/postcommit(), update_port_precommit()/postcommit
can be decorated with check_vnic_type_and_vendor_info so that they won't be called if vnic_type and vendor_info don't match.

Let me know what you think.

thanks,
Robert




On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:

>Hi,
>    During today's meeting, it was decided that we would re-purpose
>Robert's       
>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to 
>take care of adding a Base class to take care of common processing for 
>SR-IOV ports.
>
>This class would:
>
>1. Inherits from AgentMechanismDriverBase.
>2. Implements bind_port() where the binding:profile would be checked to 
>see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>3. Also checks to see that port belongs to vendor/product that supports 
>SR-IOV.
>4. This class could be used by MDs that may or may not have a valid L2 
>agent.
>5. Implement validate_port_binding(). This will always return True for 
>Mds that do not have an L2 agent.
>
>Please let me know if I left out anything.
>
>Thanks,
>Sandhya
>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    As a follow up from today's IRC, Irena, are you looking to write 
>>the below mentioned Base/Mixin class that inherits from 
>>AgentMechanismDriverBase class? When you mentioned port state, were 
>>you referring to the validate_port_binding() method?
>>
>>Pls clarify.
>>
>>Thanks,
>>Sandhya
>>
>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi Bob and Irena,
>>>   Thanks for the clarification. Irena, I am not opposed to a 
>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure 
>>>out how much common functionality there is. Have you already looked at this?
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>
>>>>Please see inline my understanding
>>>>
>>>>-----Original Message-----
>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not 
>>>>for usage questions); Irena Berezovsky; Robert Li (baoli); Brian 
>>>>Bowen
>>>>(brbowen)
>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>binding of ports
>>>>
>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>> Hi,
>>>>>      I have a couple of questions for ML2 experts regarding 
>>>>>support of  SR-IOV ports.
>>>>
>>>>I'll try, but I think these questions might be more about how the 
>>>>various SR-IOV implementations will work than about ML2 itself...
>>>>
>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2 
>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports 
>>>>> to the host? Will it just be a db update?
>>>>
>>>>I think whether or not to use an L2 agent depends on the specific 
>>>>SR-IOV implementation. Some (Mellanox?) might use an L2 agent, while 
>>>>others
>>>>(Cisco?) might put information in binding:vif_details that lets the 
>>>>nova VIF driver take care of setting up the port without an L2 
>>>>agent.
>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with 
>>>>other binding:vif_details attributes, VIFDriver should do the VIF 
>>>>pluging part.
>>>>As for required networking configuration is required, it is usually 
>>>>done either by L2 Agent or external Controller, depends on MD.
>>>>
>>>>> 
>>>>> 2. Also, how do we handle the functionality in mech_agent.py, 
>>>>> within the SR-IOV context?
>>>>
>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent 
>>>>would inherit the AgentMechanismDriverBase class if it provides 
>>>>useful functionality, but any MechanismDriver implementation is free 
>>>>to not use this base class if its not applicable. I'm not sure if an 
>>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is 
>>>>being planned, and how that would relate to AgentMechanismDriverBase.
>>>>
>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a 
>>>>need for SriovMechanismDriverBase/Mixin that provides all the 
>>>>generic functionality and helper methods that are common to SRIOV ports.
>>>>-Bob
>>>>
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky 
>>>>><irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li 
>>>>>(baoli)""
>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura 
>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen 
>>>>>(brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV  extra hr of discussion today
>>>>> 
>>>>> Hi,
>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and 
>>>>> myself are moving to openstack-meeting. Hopefully, Bob Kukura & 
>>>>> Irena can join soon.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>> To: Irena Berezovsky <irenab at mellanox.com 
>>>>><mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com 
>>>>><mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com 
>>>>><mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List 
>>>>>(not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV  extra hr of discussion today
>>>>> 
>>>>> Hi all,
>>>>>     Both openstack-meeting and openstack-meeting-alt are available 
>>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>> 
>>>>> Thanks,
>>>>> Sandhya
>>>>> 
>>>>> From: Irena Berezovsky <irenab at mellanox.com 
>>>>><mailto:irenab at mellanox.com>>
>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, 
>>>>>""Robert  Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>, 
>>>>>Robert Kukura  <rkukura at redhat.com <mailto:rkukura at redhat.com>>, 
>>>>>""OpenStack  Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>SRIOV on  Jan. 30th
>>>>> 
>>>>> Hi Sandhya,
>>>>> 
>>>>> Can you please elaborate how do you suggest to extend the below bp 
>>>>>for  SRIOV Ports managed by different Mechanism Driver?
>>>>> 
>>>>> I am not biased to any specific direction here, just think we need 
>>>>> common layer for managing SRIOV port at neutron, since there is a 
>>>>> common pass between nova and neutron.
>>>>> 
>>>>>  
>>>>> 
>>>>> BR,
>>>>> 
>>>>> Irena
>>>>> 
>>>>>  
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura; 
>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>> Brian Bowen
>>>>> (brbowen)
>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>> SRIOV on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi Irena,
>>>>> 
>>>>>       I was initially looking
>>>>> at
>>>>> 
>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra
>>>>>-po
>>>>>r
>>>>>t
>>>>>-
>>>>>info to take care of the extra information required to set up the 
>>>>>SR-IOV port.
>>>>> When the scope of the BP was being decided, we had very little 
>>>>>info  about our own design so I didn't give any feedback about 
>>>>>SR-IOV ports.
>>>>> But, I feel that this is the direction we should be going. Maybe 
>>>>>we  should target this in Juno.
>>>>> 
>>>>>  
>>>>> 
>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet 
>>>>> another way to take care of extra port config. Let me know what you think.
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Sandhya
>>>>> 
>>>>>  
>>>>> 
>>>>> *From: *Irena Berezovsky <irenab at mellanox.com 
>>>>> <mailto:irenab at mellanox.com>>
>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com 
>>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com 
>>>>> <mailto:rkukura at redhat.com>>, Sandhya Dasu <sadasu at cisco.com 
>>>>> <mailto:sadasu at cisco.com>>, ""OpenStack Development Mailing List (not for usage questions)""
>>>>> <openstack-dev at lists.openstack.org
>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>> SRIOV on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Robert,
>>>>> 
>>>>> Thank you very much for the summary.
>>>>> 
>>>>> Please, see inline
>>>>> 
>>>>>  
>>>>> 
>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky; 
>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>> Brian Bowen (brbowen)
>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>> on Jan. 30th
>>>>> 
>>>>>  
>>>>> 
>>>>> Hi,
>>>>> 
>>>>>  
>>>>> 
>>>>> We made a lot of progress today. We agreed that:
>>>>> 
>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>> 
>>>>> -- BPs:
>>>>> 
>>>>>      * Irena's
>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-ty
>>>>> pe
>>>>> for binding:vnic_type
>>>>> 
>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input 
>>>>>info  will be encapsulated in binding:profile
>>>>> 
>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV 
>>>>>output  info will be encapsulated in binding:vif_details, which may 
>>>>>include  other information like security parameters. For SRIOV, 
>>>>>vlan_id and  profileid are candidates.
>>>>> 
>>>>> -- new arguments for port-create will be implicit arguments. 
>>>>> Future release may make them explicit. New argument: 
>>>>> --binding:vnic_type {virtio, direct, macvtap}.
>>>>> 
>>>>> I think that currently we can make do without the profileid as an 
>>>>> input parameter from the user. The mechanism driver will return a 
>>>>> profileid in the vif output.
>>>>> 
>>>>>  
>>>>> 
>>>>> Please correct any misstatement in above.
>>>>> 
>>>>>  
>>>>> 
>>>>> Issues: 
>>>>> 
>>>>>   -- do we need a common utils/driver for SRIOV generic parts to 
>>>>> be used by individual Mechanism drivers that support SRIOV? More 
>>>>> details on what would be included in this sriov utils/driver? I'm 
>>>>> thinking that a candidate would be the helper functions to 
>>>>> interpret the pci_slot, which is proposed as a string. Anything else in your mind?
>>>>> 
>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and 
>>>>> persist SRIOV port related attributes/*
>>>>> 
>>>>>  
>>>>> 
>>>>>   -- what should mechanism drivers put in binding:vif_details and 
>>>>> how nova would use this information? as far as I see it from the 
>>>>> code, a VIF object is created and populated based on information 
>>>>> provided by neutron (from get network and get port)
>>>>> 
>>>>>  
>>>>> 
>>>>> Questions:
>>>>> 
>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For 
>>>>>regular  plugins, binding:vnic_type will not be set, I guess. Then 
>>>>>would it be  treated as a virtio type? And if a non-ML2 plugin 
>>>>>wants to support  SRIOV, would it need to  implement vnic-type, 
>>>>>binding:profile,  binding:vif-details for SRIOV itself?
>>>>> 
>>>>> */[IrenaB] vnic_type will be added as an additional attribute to 
>>>>> binding extension. For persistency it should be added in 
>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part 
>>>>> of
>>>>> ML2 vnic_type bp./*
>>>>> 
>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>> 
>>>>>  
>>>>> 
>>>>>  -- is a neutron agent making decision based on the binding:vif_type?
>>>>>  In that case, it makes sense for binding:vnic_type not to be 
>>>>> exposed to agents.
>>>>> 
>>>>> */[IrenaB] vnic_type is input parameter that will eventually cause 
>>>>> certain vif_type to be sent to GenericVIFDriver and create network 
>>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>>> For example, OVS agent will look only for OVS interfaces, so if 
>>>>> SRIOV interface is created, it won't be discovered by OVS agent./*
>>>>> 
>>>>>  
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Robert
>>>>> 
>>>>
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
>>_______________________________________________
>>OpenStack-dev mailing list
>>OpenStack-dev at lists.openstack.org
>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>


_______________________________________________
OpenStack-dev mailing list
OpenStack-dev at lists.openstack.org
http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

"
NOVA 1402728 - CG,msg18397,<CF3CB4F0.5B437%baoli@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Irena,

The main reason for me to do it that way is how vif_details should be
setup in our case. Do you need vlan in vif_details? The behavior in the
existing base classes is that the vif_details is set during the driver
init time. In our case, it needs to be setup during bind_port().

thanks,
Robert


On 3/5/14 7:37 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:

>Hi Robert, Sandhya,
>I have pushed the reference implementation SriovAgentMechanismDriverBase
>as part the following WIP:
>https://review.openstack.org/#/c/74464/
>
>The code is in mech_agent.py, and very simple code for
>mech_sriov_nic_switch.py.
>
>Please take a look and review.
>
>BR,
>Irena
>
>-----Original Message-----
>From: Irena Berezovsky [mailto:irenab at mellanox.com]
>Sent: Wednesday, March 05, 2014 9:04 AM
>To: Robert Li (baoli); Sandhya Dasu (sadasu); OpenStack Development
>Mailing List (not for usage questions); Robert Kukura; Brian Bowen
>(brbowen)
>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>binding of ports
>
>Hi Robert,
>Seems to me that many code lines are duplicated following your proposal.
>For agent based MDs, I would prefer to inherit from
>SimpleAgentMechanismDriverBase and add there verify method for
>supported_pci_vendor_info. Specific MD will pass the list of supported
>pci_vendor_info list. The  'try_to_bind_segment_for_agent' method will
>call 'supported_pci_vendor_info', and if supported continue with binding
>flow. 
>Maybe instead of a decorator method, it should be just an utility method?
>I think that the check for supported vnic_type and pci_vendor info
>support, should be done in order to see if MD should bind the port. If
>the answer is Yes, no more checks are required.
>
>Coming back to the question I asked earlier, for non-agent MD, how would
>you deal with updates after port is bound, like 'admin_state_up' changes?
>I'll try to push some reference code later today.
>
>BR,
>Irena
>
>-----Original Message-----
>From: Robert Li (baoli) [mailto:baoli at cisco.com]
>Sent: Wednesday, March 05, 2014 4:46 AM
>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for
>usage questions); Irena Berezovsky; Robert Kukura; Brian Bowen (brbowen)
>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>binding of ports
>
>Hi Sandhya,
>
>I agree with you except that I think that the class should inherit from
>MechanismDriver. I took a crack at it, and here is what I got:
>
># Copyright (c) 2014 OpenStack Foundation # All Rights Reserved.
>#
>#    Licensed under the Apache License, Version 2.0 (the ""License""); you
>may
>#    not use this file except in compliance with the License. You may
>obtain
>#    a copy of the License at
>#
>#         http://www.apache.org/licenses/LICENSE-2.0
>#
>#    Unless required by applicable law or agreed to in writing, software
>#    distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT
>#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
>the
>#    License for the specific language governing permissions and
>limitations
>#    under the License.
>
>from abc import ABCMeta, abstractmethod
>
>import functools
>import six
>
>from neutron.extensions import portbindings from neutron.openstack.common
>import log from neutron.plugins.ml2 import driver_api as api
>
>LOG = log.getLogger(__name__)
>
>
>DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
>                                portbindings.VNIC_MACVTAP]
>
>def check_vnic_type_and_vendor_info(f):
>    @functools.wraps(f)
>    def wrapper(self, context):
>        vnic_type = context.current.get(portbindings.VNIC_TYPE,
>                                        portbindings.VNIC_NORMAL)
>        if vnic_type not in self.supported_vnic_types:
>            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
>                        ""vnic_type: %(vnic_type)s""),
>                      {'func_name': f.func_name, 'vnic_type': vnic_type})
>            return
>
>        if self.supported_pci_vendor_info:
>            profile = context.current.get(portbindings.PROFILE, {})
>            if not profile:
>                LOG.debug(_(""%s: Missing profile in port binding""),
>                          f.func_name)
>                return
>            pci_vendor_info = profile.get('pci_vendor_info')
>            if not pci_vendor_info:
>                LOG.debug(_(""%s: Missing pci vendor info in profile""),
>                          f.func_name)
>                return
>            if pci_vendor_info not in self.supported_pci_vendor_info:
>                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
>                            ""info: %(info)s""),
>                          {'func_name': f.func_name, 'info':
>pci_vendor_info})
>                return
>        f(self, context)
>    return wrapper
>
>@six.add_metaclass(ABCMeta)
>class SriovMechanismDriverBase(api.MechanismDriver):
>    """"""Base class for drivers that supports SR-IOV
>
>    The SriovMechanismDriverBase provides common code for mechanism
>    drivers that supports SR-IOV. Such a driver may or may not require
>    an agent to be running on the port's host.
>
>    MechanismDrivers that uses this base class and requires an agent must
>    pass the agent type to __init__(), and must implement
>    try_to_bind_segment_for_agent() and check_segment_for_agent().
>
>    MechanismDrivers that uses this base class may provide supported
>vendor
>    information, and must provide the supported vnic types.
>    """"""
>    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
>                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
>        """"""Initialize base class for SR-IOV capable Mechanism Drivers
>
>        :param agent_type: Constant identifying agent type in agents_db
>        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
>        :param supported_vnic_types: The binding:vnic_type values we can
>bind
>        """"""
>        self.supported_pci_vendor_info = supported_pci_vendor_info
>        self.agent_type = agent_type
>        self.supported_vnic_types = supported_vnic_types
>
>    def initialize(self):
>        pass
>
>    @check_vnic_type_and_vendor_info
>    def bind_port(self, context):
>        LOG.debug(_(""Attempting to bind port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive']:
>                    for segment in context.network.network_segments:
>                        if self.try_to_bind_segment_for_agent(context,
>segment,
>                                                              agent):
>                            LOG.debug(_(""Bound using segment: %s""),
>segment)
>                            return
>                else:
>                    LOG.warning(_(""Attempting to bind with dead agent:
>%s""),
>                                agent)
>        else:
>            for segment in context.network.network_segments:
>                if self.try_to_bind_segment(context, segment):
>                    LOG.debug(_(""Bound using segment: %s""), segment)
>                    return
>
>    def validate_port_binding(self, context):
>        LOG.debug(_(""Validating binding for port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive'] and self.check_segment_for_agent(
>                    context.bound_segment, agent):
>                    LOG.debug(_(""Binding valid""))
>                    return True
>            LOG.warning(_(""Binding invalid for port: %s""),
>context.current)
>            return False
>        else:
>            return True
>
>    @check_vnic_type_and_vendor_info
>    def unbind_port(self, context):
>        LOG.debug(_(""Unbinding port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>    @abstractmethod
>    def try_to_bind_segment_for_agent(self, context, segment, agent):
>        """"""Try to bind with segment for agent.
>
>        :param context: PortContext instance describing the port
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment has been bound for agent
>
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use agent_db data along with built-in
>        knowledge of the corresponding agent's capabilities to attempt
>        to bind to the specified network segment for the agent.
>
>        If the segment can be bound for the agent, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>    @abstractmethod
>    def check_segment_for_agent(self, segment, agent):
>        """"""Check if segment can be bound for agent.
>
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment can be bound for agent
>
>        Called inside transaction during validate_port_binding() so
>        that derived MechanismDrivers can use agent_db data along with
>        built-in knowledge of the corresponding agent's capabilities
>        to determine whether or not the specified network segment can
>        be bound for the agent.
>        """"""
>
>    @abstractmethod
>    def try_to_bind_segment(self, segment):
>        """"""Check if segment can be bound.
>
>        :param segment: segment dictionary describing segment to bind
>        :returns: True iff segment can be bound
>        
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use database data along with built-in
>        knowledge to attempt to bind to the specified network segment
>
>        If the segment can be bound, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>
>
>A SRIOV MD would inherit from it and implement
>try_to_bind_segment_for_agent() and check_segment_for_agent() and
>try_to_bind_segment(). If an agent is required, the first two methods
>should have concrete implementation, and the third may only contain
>'pass'. Otherwise, the first two are 'pass', and the third needs to be
>implemented.
>
>In the inherited class, its __init__ method should set the supported
>pci_vendor_info by either hard coding or by configuration (which is
>preferred so that code doesn't need to be changed with newly supported
>cards).
>in the bind segment method, vif_type and vif_details should be set, and
>set_binding() called.
>
>methods inherited from MechanismDriver, such as
>create_port_precommit()/postcommit(), update_port_precommit()/postcommit
>can be decorated with check_vnic_type_and_vendor_info so that they won't
>be called if vnic_type and vendor_info don't match.
>
>Let me know what you think.
>
>thanks,
>Robert
>
>
>
>
>On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    During today's meeting, it was decided that we would re-purpose
>>Robert's       
>>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to
>>take care of adding a Base class to take care of common processing for
>>SR-IOV ports.
>>
>>This class would:
>>
>>1. Inherits from AgentMechanismDriverBase.
>>2. Implements bind_port() where the binding:profile would be checked to
>>see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>>3. Also checks to see that port belongs to vendor/product that supports
>>SR-IOV.
>>4. This class could be used by MDs that may or may not have a valid L2
>>agent.
>>5. Implement validate_port_binding(). This will always return True for
>>Mds that do not have an L2 agent.
>>
>>Please let me know if I left out anything.
>>
>>Thanks,
>>Sandhya
>>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi,
>>>    As a follow up from today's IRC, Irena, are you looking to write
>>>the below mentioned Base/Mixin class that inherits from
>>>AgentMechanismDriverBase class? When you mentioned port state, were
>>>you referring to the validate_port_binding() method?
>>>
>>>Pls clarify.
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>>
>>>>Hi Bob and Irena,
>>>>   Thanks for the clarification. Irena, I am not opposed to a
>>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure
>>>>out how much common functionality there is. Have you already looked at
>>>>this?
>>>>
>>>>Thanks,
>>>>Sandhya
>>>>
>>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>>
>>>>>Please see inline my understanding
>>>>>
>>>>>-----Original Message-----
>>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not
>>>>>for usage questions); Irena Berezovsky; Robert Li (baoli); Brian
>>>>>Bowen
>>>>>(brbowen)
>>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>>binding of ports
>>>>>
>>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>>> Hi,
>>>>>>      I have a couple of questions for ML2 experts regarding
>>>>>>support of  SR-IOV ports.
>>>>>
>>>>>I'll try, but I think these questions might be more about how the
>>>>>various SR-IOV implementations will work than about ML2 itself...
>>>>>
>>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2
>>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports
>>>>>> to the host? Will it just be a db update?
>>>>>
>>>>>I think whether or not to use an L2 agent depends on the specific
>>>>>SR-IOV implementation. Some (Mellanox?) might use an L2 agent, while
>>>>>others
>>>>>(Cisco?) might put information in binding:vif_details that lets the
>>>>>nova VIF driver take care of setting up the port without an L2
>>>>>agent.
>>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with
>>>>>other binding:vif_details attributes, VIFDriver should do the VIF
>>>>>pluging part.
>>>>>As for required networking configuration is required, it is usually
>>>>>done either by L2 Agent or external Controller, depends on MD.
>>>>>
>>>>>> 
>>>>>> 2. Also, how do we handle the functionality in mech_agent.py,
>>>>>> within the SR-IOV context?
>>>>>
>>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent
>>>>>would inherit the AgentMechanismDriverBase class if it provides
>>>>>useful functionality, but any MechanismDriver implementation is free
>>>>>to not use this base class if its not applicable. I'm not sure if an
>>>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is
>>>>>being planned, and how that would relate to AgentMechanismDriverBase.
>>>>>
>>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a
>>>>>need for SriovMechanismDriverBase/Mixin that provides all the
>>>>>generic functionality and helper methods that are common to SRIOV
>>>>>ports.
>>>>>-Bob
>>>>>
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky
>>>>>><irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li
>>>>>>(baoli)""
>>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen
>>>>>>(brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi,
>>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and
>>>>>> myself are moving to openstack-meeting. Hopefully, Bob Kukura &
>>>>>> Irena can join soon.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>>> To: Irena Berezovsky <irenab at mellanox.com
>>>>>><mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com
>>>>>><mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>>>><mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List
>>>>>>(not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi all,
>>>>>>     Both openstack-meeting and openstack-meeting-alt are available
>>>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Irena Berezovsky <irenab at mellanox.com
>>>>>><mailto:irenab at mellanox.com>>
>>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>,
>>>>>>""Robert  Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>,
>>>>>>Robert Kukura  <rkukura at redhat.com <mailto:rkukura at redhat.com>>,
>>>>>>""OpenStack  Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV on  Jan. 30th
>>>>>> 
>>>>>> Hi Sandhya,
>>>>>> 
>>>>>> Can you please elaborate how do you suggest to extend the below bp
>>>>>>for  SRIOV Ports managed by different Mechanism Driver?
>>>>>> 
>>>>>> I am not biased to any specific direction here, just think we need
>>>>>> common layer for managing SRIOV port at neutron, since there is a
>>>>>> common pass between nova and neutron.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> BR,
>>>>>> 
>>>>>> Irena
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura;
>>>>>> OpenStack Development Mailing List (not for usage questions);
>>>>>> Brian Bowen
>>>>>> (brbowen)
>>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>> SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi Irena,
>>>>>> 
>>>>>>       I was initially looking
>>>>>> at
>>>>>> 
>>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra
>>>>>>-po
>>>>>>r
>>>>>>t
>>>>>>-
>>>>>>info to take care of the extra information required to set up the
>>>>>>SR-IOV port.
>>>>>> When the scope of the BP was being decided, we had very little
>>>>>>info  about our own design so I didn't give any feedback about
>>>>>>SR-IOV ports.
>>>>>> But, I feel that this is the direction we should be going. Maybe
>>>>>>we  should target this in Juno.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet
>>>>>> another way to take care of extra port config. Let me know what you
>>>>>>think.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Sandhya
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From: *Irena Berezovsky <irenab at mellanox.com
>>>>>> <mailto:irenab at mellanox.com>>
>>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com
>>>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>>>> <mailto:rkukura at redhat.com>>, Sandhya Dasu <sadasu at cisco.com
>>>>>> <mailto:sadasu at cisco.com>>, ""OpenStack Development Mailing List
>>>>>>(not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>> SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Robert,
>>>>>> 
>>>>>> Thank you very much for the summary.
>>>>>> 
>>>>>> Please, see inline
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky;
>>>>>> OpenStack Development Mailing List (not for usage questions);
>>>>>> Brian Bowen (brbowen)
>>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>>> on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> We made a lot of progress today. We agreed that:
>>>>>> 
>>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>>> 
>>>>>> -- BPs:
>>>>>> 
>>>>>>      * Irena's
>>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-ty
>>>>>> pe
>>>>>> for binding:vnic_type
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input
>>>>>>info  will be encapsulated in binding:profile
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV
>>>>>>output  info will be encapsulated in binding:vif_details, which may
>>>>>>include  other information like security parameters. For SRIOV,
>>>>>>vlan_id and  profileid are candidates.
>>>>>> 
>>>>>> -- new arguments for port-create will be implicit arguments.
>>>>>> Future release may make them explicit. New argument:
>>>>>> --binding:vnic_type {virtio, direct, macvtap}.
>>>>>> 
>>>>>> I think that currently we can make do without the profileid as an
>>>>>> input parameter from the user. The mechanism driver will return a
>>>>>> profileid in the vif output.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Please correct any misstatement in above.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Issues: 
>>>>>> 
>>>>>>   -- do we need a common utils/driver for SRIOV generic parts to
>>>>>> be used by individual Mechanism drivers that support SRIOV? More
>>>>>> details on what would be included in this sriov utils/driver? I'm
>>>>>> thinking that a candidate would be the helper functions to
>>>>>> interpret the pci_slot, which is proposed as a string. Anything
>>>>>>else in your mind?
>>>>>> 
>>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and
>>>>>> persist SRIOV port related attributes/*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>   -- what should mechanism drivers put in binding:vif_details and
>>>>>> how nova would use this information? as far as I see it from the
>>>>>> code, a VIF object is created and populated based on information
>>>>>> provided by neutron (from get network and get port)
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Questions:
>>>>>> 
>>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For
>>>>>>regular  plugins, binding:vnic_type will not be set, I guess. Then
>>>>>>would it be  treated as a virtio type? And if a non-ML2 plugin
>>>>>>wants to support  SRIOV, would it need to  implement vnic-type,
>>>>>>binding:profile,  binding:vif-details for SRIOV itself?
>>>>>> 
>>>>>> */[IrenaB] vnic_type will be added as an additional attribute to
>>>>>> binding extension. For persistency it should be added in
>>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part
>>>>>> of
>>>>>> ML2 vnic_type bp./*
>>>>>> 
>>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  -- is a neutron agent making decision based on the
>>>>>>binding:vif_type?
>>>>>>  In that case, it makes sense for binding:vnic_type not to be
>>>>>> exposed to agents.
>>>>>> 
>>>>>> */[IrenaB] vnic_type is input parameter that will eventually cause
>>>>>> certain vif_type to be sent to GenericVIFDriver and create network
>>>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>>>> For example, OVS agent will look only for OVS interfaces, so if
>>>>>> SRIOV interface is created, it won't be discovered by OVS agent./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Robert
>>>>>> 
>>>>>
>>>>
>>>>
>>>>_______________________________________________
>>>>OpenStack-dev mailing list
>>>>OpenStack-dev at lists.openstack.org
>>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1402728 - CG,msg18401,<9D25E123B44F4A4291F4B5C13DA94E7795EDA919@MTLDAG01.mtl.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Robert,
I think what you mentioned can be achieved by calling into specific MD method from
SriovAgentMechanismDriverBase .try_to_bind_segment_for_agent mehod, maybe something like 'get_vif_details' before it calls to context.set_binding.
Would you mind to continue discussion over patch gerrit review https://review.openstack.org/#/c/74464/ ?
I think it will be easier to follow up the comments and decisions.

Thanks,
Irena

-----Original Message-----
From: Robert Li (baoli) [mailto:baoli at cisco.com] 
Sent: Wednesday, March 05, 2014 6:10 PM
To: Irena Berezovsky; OpenStack Development Mailing List (not for usage questions); Sandhya Dasu (sadasu); Robert Kukura; Brian Bowen (brbowen)
Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV binding of ports

Hi Irena,

The main reason for me to do it that way is how vif_details should be setup in our case. Do you need vlan in vif_details? The behavior in the existing base classes is that the vif_details is set during the driver init time. In our case, it needs to be setup during bind_port().

thanks,
Robert


On 3/5/14 7:37 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:

>Hi Robert, Sandhya,
>I have pushed the reference implementation 
>SriovAgentMechanismDriverBase as part the following WIP:
>https://review.openstack.org/#/c/74464/
>
>The code is in mech_agent.py, and very simple code for 
>mech_sriov_nic_switch.py.
>
>Please take a look and review.
>
>BR,
>Irena
>
>-----Original Message-----
>From: Irena Berezovsky [mailto:irenab at mellanox.com]
>Sent: Wednesday, March 05, 2014 9:04 AM
>To: Robert Li (baoli); Sandhya Dasu (sadasu); OpenStack Development 
>Mailing List (not for usage questions); Robert Kukura; Brian Bowen
>(brbowen)
>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>binding of ports
>
>Hi Robert,
>Seems to me that many code lines are duplicated following your proposal.
>For agent based MDs, I would prefer to inherit from 
>SimpleAgentMechanismDriverBase and add there verify method for 
>supported_pci_vendor_info. Specific MD will pass the list of supported 
>pci_vendor_info list. The  'try_to_bind_segment_for_agent' method will 
>call 'supported_pci_vendor_info', and if supported continue with 
>binding flow.
>Maybe instead of a decorator method, it should be just an utility method?
>I think that the check for supported vnic_type and pci_vendor info 
>support, should be done in order to see if MD should bind the port. If 
>the answer is Yes, no more checks are required.
>
>Coming back to the question I asked earlier, for non-agent MD, how 
>would you deal with updates after port is bound, like 'admin_state_up' changes?
>I'll try to push some reference code later today.
>
>BR,
>Irena
>
>-----Original Message-----
>From: Robert Li (baoli) [mailto:baoli at cisco.com]
>Sent: Wednesday, March 05, 2014 4:46 AM
>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for 
>usage questions); Irena Berezovsky; Robert Kukura; Brian Bowen 
>(brbowen)
>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>binding of ports
>
>Hi Sandhya,
>
>I agree with you except that I think that the class should inherit from 
>MechanismDriver. I took a crack at it, and here is what I got:
>
># Copyright (c) 2014 OpenStack Foundation # All Rights Reserved.
>#
>#    Licensed under the Apache License, Version 2.0 (the ""License""); you
>may
>#    not use this file except in compliance with the License. You may
>obtain
>#    a copy of the License at
>#
>#         http://www.apache.org/licenses/LICENSE-2.0
>#
>#    Unless required by applicable law or agreed to in writing, software
>#    distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT
>#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
>the
>#    License for the specific language governing permissions and
>limitations
>#    under the License.
>
>from abc import ABCMeta, abstractmethod
>
>import functools
>import six
>
>from neutron.extensions import portbindings from 
>neutron.openstack.common import log from neutron.plugins.ml2 import 
>driver_api as api
>
>LOG = log.getLogger(__name__)
>
>
>DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
>                                portbindings.VNIC_MACVTAP]
>
>def check_vnic_type_and_vendor_info(f):
>    @functools.wraps(f)
>    def wrapper(self, context):
>        vnic_type = context.current.get(portbindings.VNIC_TYPE,
>                                        portbindings.VNIC_NORMAL)
>        if vnic_type not in self.supported_vnic_types:
>            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
>                        ""vnic_type: %(vnic_type)s""),
>                      {'func_name': f.func_name, 'vnic_type': vnic_type})
>            return
>
>        if self.supported_pci_vendor_info:
>            profile = context.current.get(portbindings.PROFILE, {})
>            if not profile:
>                LOG.debug(_(""%s: Missing profile in port binding""),
>                          f.func_name)
>                return
>            pci_vendor_info = profile.get('pci_vendor_info')
>            if not pci_vendor_info:
>                LOG.debug(_(""%s: Missing pci vendor info in profile""),
>                          f.func_name)
>                return
>            if pci_vendor_info not in self.supported_pci_vendor_info:
>                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
>                            ""info: %(info)s""),
>                          {'func_name': f.func_name, 'info':
>pci_vendor_info})
>                return
>        f(self, context)
>    return wrapper
>
>@six.add_metaclass(ABCMeta)
>class SriovMechanismDriverBase(api.MechanismDriver):
>    """"""Base class for drivers that supports SR-IOV
>
>    The SriovMechanismDriverBase provides common code for mechanism
>    drivers that supports SR-IOV. Such a driver may or may not require
>    an agent to be running on the port's host.
>
>    MechanismDrivers that uses this base class and requires an agent must
>    pass the agent type to __init__(), and must implement
>    try_to_bind_segment_for_agent() and check_segment_for_agent().
>
>    MechanismDrivers that uses this base class may provide supported 
>vendor
>    information, and must provide the supported vnic types.
>    """"""
>    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
>                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
>        """"""Initialize base class for SR-IOV capable Mechanism Drivers
>
>        :param agent_type: Constant identifying agent type in agents_db
>        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
>        :param supported_vnic_types: The binding:vnic_type values we 
>can bind
>        """"""
>        self.supported_pci_vendor_info = supported_pci_vendor_info
>        self.agent_type = agent_type
>        self.supported_vnic_types = supported_vnic_types
>
>    def initialize(self):
>        pass
>
>    @check_vnic_type_and_vendor_info
>    def bind_port(self, context):
>        LOG.debug(_(""Attempting to bind port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive']:
>                    for segment in context.network.network_segments:
>                        if self.try_to_bind_segment_for_agent(context,
>segment,
>                                                              agent):
>                            LOG.debug(_(""Bound using segment: %s""),
>segment)
>                            return
>                else:
>                    LOG.warning(_(""Attempting to bind with dead agent:
>%s""),
>                                agent)
>        else:
>            for segment in context.network.network_segments:
>                if self.try_to_bind_segment(context, segment):
>                    LOG.debug(_(""Bound using segment: %s""), segment)
>                    return
>
>    def validate_port_binding(self, context):
>        LOG.debug(_(""Validating binding for port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive'] and self.check_segment_for_agent(
>                    context.bound_segment, agent):
>                    LOG.debug(_(""Binding valid""))
>                    return True
>            LOG.warning(_(""Binding invalid for port: %s""),
>context.current)
>            return False
>        else:
>            return True
>
>    @check_vnic_type_and_vendor_info
>    def unbind_port(self, context):
>        LOG.debug(_(""Unbinding port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>    @abstractmethod
>    def try_to_bind_segment_for_agent(self, context, segment, agent):
>        """"""Try to bind with segment for agent.
>
>        :param context: PortContext instance describing the port
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment has been bound for agent
>
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use agent_db data along with built-in
>        knowledge of the corresponding agent's capabilities to attempt
>        to bind to the specified network segment for the agent.
>
>        If the segment can be bound for the agent, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>    @abstractmethod
>    def check_segment_for_agent(self, segment, agent):
>        """"""Check if segment can be bound for agent.
>
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment can be bound for agent
>
>        Called inside transaction during validate_port_binding() so
>        that derived MechanismDrivers can use agent_db data along with
>        built-in knowledge of the corresponding agent's capabilities
>        to determine whether or not the specified network segment can
>        be bound for the agent.
>        """"""
>
>    @abstractmethod
>    def try_to_bind_segment(self, segment):
>        """"""Check if segment can be bound.
>
>        :param segment: segment dictionary describing segment to bind
>        :returns: True iff segment can be bound
>        
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use database data along with built-in
>        knowledge to attempt to bind to the specified network segment
>
>        If the segment can be bound, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>
>
>A SRIOV MD would inherit from it and implement
>try_to_bind_segment_for_agent() and check_segment_for_agent() and 
>try_to_bind_segment(). If an agent is required, the first two methods 
>should have concrete implementation, and the third may only contain 
>'pass'. Otherwise, the first two are 'pass', and the third needs to be 
>implemented.
>
>In the inherited class, its __init__ method should set the supported 
>pci_vendor_info by either hard coding or by configuration (which is 
>preferred so that code doesn't need to be changed with newly supported 
>cards).
>in the bind segment method, vif_type and vif_details should be set, and
>set_binding() called.
>
>methods inherited from MechanismDriver, such as 
>create_port_precommit()/postcommit(), 
>update_port_precommit()/postcommit
>can be decorated with check_vnic_type_and_vendor_info so that they 
>won't be called if vnic_type and vendor_info don't match.
>
>Let me know what you think.
>
>thanks,
>Robert
>
>
>
>
>On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    During today's meeting, it was decided that we would re-purpose
>>Robert's       
>>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov 
>>to take care of adding a Base class to take care of common processing 
>>for SR-IOV ports.
>>
>>This class would:
>>
>>1. Inherits from AgentMechanismDriverBase.
>>2. Implements bind_port() where the binding:profile would be checked 
>>to see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>>3. Also checks to see that port belongs to vendor/product that 
>>supports SR-IOV.
>>4. This class could be used by MDs that may or may not have a valid L2 
>>agent.
>>5. Implement validate_port_binding(). This will always return True for 
>>Mds that do not have an L2 agent.
>>
>>Please let me know if I left out anything.
>>
>>Thanks,
>>Sandhya
>>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi,
>>>    As a follow up from today's IRC, Irena, are you looking to write 
>>>the below mentioned Base/Mixin class that inherits from 
>>>AgentMechanismDriverBase class? When you mentioned port state, were 
>>>you referring to the validate_port_binding() method?
>>>
>>>Pls clarify.
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>>
>>>>Hi Bob and Irena,
>>>>   Thanks for the clarification. Irena, I am not opposed to a 
>>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure 
>>>>out how much common functionality there is. Have you already looked 
>>>>at this?
>>>>
>>>>Thanks,
>>>>Sandhya
>>>>
>>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>>
>>>>>Please see inline my understanding
>>>>>
>>>>>-----Original Message-----
>>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not 
>>>>>for usage questions); Irena Berezovsky; Robert Li (baoli); Brian 
>>>>>Bowen
>>>>>(brbowen)
>>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>>binding of ports
>>>>>
>>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>>> Hi,
>>>>>>      I have a couple of questions for ML2 experts regarding 
>>>>>>support of  SR-IOV ports.
>>>>>
>>>>>I'll try, but I think these questions might be more about how the 
>>>>>various SR-IOV implementations will work than about ML2 itself...
>>>>>
>>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2 
>>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports 
>>>>>> to the host? Will it just be a db update?
>>>>>
>>>>>I think whether or not to use an L2 agent depends on the specific 
>>>>>SR-IOV implementation. Some (Mellanox?) might use an L2 agent, 
>>>>>while others
>>>>>(Cisco?) might put information in binding:vif_details that lets the 
>>>>>nova VIF driver take care of setting up the port without an L2 
>>>>>agent.
>>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with 
>>>>>other binding:vif_details attributes, VIFDriver should do the VIF 
>>>>>pluging part.
>>>>>As for required networking configuration is required, it is usually 
>>>>>done either by L2 Agent or external Controller, depends on MD.
>>>>>
>>>>>> 
>>>>>> 2. Also, how do we handle the functionality in mech_agent.py, 
>>>>>> within the SR-IOV context?
>>>>>
>>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent 
>>>>>would inherit the AgentMechanismDriverBase class if it provides 
>>>>>useful functionality, but any MechanismDriver implementation is 
>>>>>free to not use this base class if its not applicable. I'm not sure 
>>>>>if an SriovMechanismDriverBase (or SriovMechanismDriverMixin) class 
>>>>>is being planned, and how that would relate to AgentMechanismDriverBase.
>>>>>
>>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a 
>>>>>need for SriovMechanismDriverBase/Mixin that provides all the 
>>>>>generic functionality and helper methods that are common to SRIOV 
>>>>>ports.
>>>>>-Bob
>>>>>
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky 
>>>>>><irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li 
>>>>>>(baoli)""
>>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura 
>>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen 
>>>>>>(brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi,
>>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and 
>>>>>> myself are moving to openstack-meeting. Hopefully, Bob Kukura & 
>>>>>> Irena can join soon.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage 
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>>> To: Irena Berezovsky <irenab at mellanox.com 
>>>>>><mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" 
>>>>>><baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura 
>>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""OpenStack 
>>>>>>Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi all,
>>>>>>     Both openstack-meeting and openstack-meeting-alt are 
>>>>>> available today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Irena Berezovsky <irenab at mellanox.com 
>>>>>><mailto:irenab at mellanox.com>>
>>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>, 
>>>>>>""Robert  Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>, 
>>>>>>Robert Kukura  <rkukura at redhat.com <mailto:rkukura at redhat.com>>, 
>>>>>>""OpenStack  Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>>SRIOV on  Jan. 30th
>>>>>> 
>>>>>> Hi Sandhya,
>>>>>> 
>>>>>> Can you please elaborate how do you suggest to extend the below 
>>>>>>bp for  SRIOV Ports managed by different Mechanism Driver?
>>>>>> 
>>>>>> I am not biased to any specific direction here, just think we 
>>>>>> need common layer for managing SRIOV port at neutron, since there 
>>>>>> is a common pass between nova and neutron.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> BR,
>>>>>> 
>>>>>> Irena
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura; 
>>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>>> Brian Bowen
>>>>>> (brbowen)
>>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through 
>>>>>> SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi Irena,
>>>>>> 
>>>>>>       I was initially looking
>>>>>> at
>>>>>> 
>>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extr
>>>>>>a
>>>>>>-po
>>>>>>r
>>>>>>t
>>>>>>-
>>>>>>info to take care of the extra information required to set up the 
>>>>>>SR-IOV port.
>>>>>> When the scope of the BP was being decided, we had very little 
>>>>>>info  about our own design so I didn't give any feedback about 
>>>>>>SR-IOV ports.
>>>>>> But, I feel that this is the direction we should be going. Maybe 
>>>>>>we  should target this in Juno.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet  
>>>>>>another way to take care of extra port config. Let me know what 
>>>>>>you think.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Sandhya
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From: *Irena Berezovsky <irenab at mellanox.com  
>>>>>><mailto:irenab at mellanox.com>>
>>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com  
>>>>>><mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com  
>>>>>><mailto:rkukura at redhat.com>>, Sandhya Dasu <sadasu at cisco.com  
>>>>>><mailto:sadasu at cisco.com>>, ""OpenStack Development Mailing List 
>>>>>>(not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through  
>>>>>>SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Robert,
>>>>>> 
>>>>>> Thank you very much for the summary.
>>>>>> 
>>>>>> Please, see inline
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky; 
>>>>>> OpenStack Development Mailing List (not for usage questions); 
>>>>>> Brian Bowen (brbowen)
>>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV 
>>>>>> on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> We made a lot of progress today. We agreed that:
>>>>>> 
>>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>>> 
>>>>>> -- BPs:
>>>>>> 
>>>>>>      * Irena's
>>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-t
>>>>>> y
>>>>>> pe
>>>>>> for binding:vnic_type
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input 
>>>>>>info  will be encapsulated in binding:profile
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV 
>>>>>>output  info will be encapsulated in binding:vif_details, which 
>>>>>>may include  other information like security parameters. For 
>>>>>>SRIOV, vlan_id and  profileid are candidates.
>>>>>> 
>>>>>> -- new arguments for port-create will be implicit arguments.
>>>>>> Future release may make them explicit. New argument:
>>>>>> --binding:vnic_type {virtio, direct, macvtap}.
>>>>>> 
>>>>>> I think that currently we can make do without the profileid as an 
>>>>>> input parameter from the user. The mechanism driver will return a 
>>>>>> profileid in the vif output.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Please correct any misstatement in above.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Issues: 
>>>>>> 
>>>>>>   -- do we need a common utils/driver for SRIOV generic parts to  
>>>>>>be used by individual Mechanism drivers that support SRIOV? More  
>>>>>>details on what would be included in this sriov utils/driver? I'm  
>>>>>>thinking that a candidate would be the helper functions to  
>>>>>>interpret the pci_slot, which is proposed as a string. Anything 
>>>>>>else in your mind?
>>>>>> 
>>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and 
>>>>>> persist SRIOV port related attributes/*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>   -- what should mechanism drivers put in binding:vif_details and 
>>>>>> how nova would use this information? as far as I see it from the 
>>>>>> code, a VIF object is created and populated based on information 
>>>>>> provided by neutron (from get network and get port)
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Questions:
>>>>>> 
>>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For 
>>>>>>regular  plugins, binding:vnic_type will not be set, I guess. Then 
>>>>>>would it be  treated as a virtio type? And if a non-ML2 plugin 
>>>>>>wants to support  SRIOV, would it need to  implement vnic-type, 
>>>>>>binding:profile,  binding:vif-details for SRIOV itself?
>>>>>> 
>>>>>> */[IrenaB] vnic_type will be added as an additional attribute to 
>>>>>> binding extension. For persistency it should be added in 
>>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part 
>>>>>> of
>>>>>> ML2 vnic_type bp./*
>>>>>> 
>>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  -- is a neutron agent making decision based on the 
>>>>>>binding:vif_type?
>>>>>>  In that case, it makes sense for binding:vnic_type not to be  
>>>>>>exposed to agents.
>>>>>> 
>>>>>> */[IrenaB] vnic_type is input parameter that will eventually 
>>>>>> cause certain vif_type to be sent to GenericVIFDriver and create 
>>>>>> network interface. Neutron agents periodically scan for attached interfaces.
>>>>>> For example, OVS agent will look only for OVS interfaces, so if 
>>>>>> SRIOV interface is created, it won't be discovered by OVS 
>>>>>> agent./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Robert
>>>>>> 
>>>>>
>>>>
>>>>
>>>>_______________________________________________
>>>>OpenStack-dev mailing list
>>>>OpenStack-dev at lists.openstack.org
>>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1402728 - CG,msg18377,<CF3CA19E.1447F%sadasu@cisco.com>,"[openstack-dev] [nova][neutron] PCI pass-through SRIOV binding
 of ports

--

Hi Irena,
    My MD has to take care of admin state changes since I have no L2
agent. I think that is what Bob also alluded to. That being said, I am not
doing anything specific to handle admin_state_up/down. The SR-IOV port on
my device is always going to be up, for now atleast.

Thanks,
Sandhya

On 3/5/14 1:56 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:

>Hi Robert,
>Seems to me that many code lines are duplicated following your proposal.
>For agent based MDs, I would prefer to inherit from
>SimpleAgentMechanismDriverBase and add there verify method for
>supported_pci_vendor_info. Specific MD will pass the list of supported
>pci_vendor_info list. The  'try_to_bind_segment_for_agent' method will
>call 'supported_pci_vendor_info', and if supported continue with binding
>flow. 
>Maybe instead of a decorator method, it should be just an utility method?
>I think that the check for supported vnic_type and pci_vendor info
>support, should be done in order to see if MD should bind the port. If
>the answer is Yes, no more checks are required.
>
>Coming back to the question I asked earlier, for non-agent MD, how would
>you deal with updates after port is bound, like 'admin_state_up' changes?
>I'll try to push some reference code later today.
>
>BR,
>Irena
>
>-----Original Message-----
>From: Robert Li (baoli) [mailto:baoli at cisco.com]
>Sent: Wednesday, March 05, 2014 4:46 AM
>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not for
>usage questions); Irena Berezovsky; Robert Kukura; Brian Bowen (brbowen)
>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>binding of ports
>
>Hi Sandhya,
>
>I agree with you except that I think that the class should inherit from
>MechanismDriver. I took a crack at it, and here is what I got:
>
># Copyright (c) 2014 OpenStack Foundation # All Rights Reserved.
>#
>#    Licensed under the Apache License, Version 2.0 (the ""License""); you
>may
>#    not use this file except in compliance with the License. You may
>obtain
>#    a copy of the License at
>#
>#         http://www.apache.org/licenses/LICENSE-2.0
>#
>#    Unless required by applicable law or agreed to in writing, software
>#    distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT
>#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See
>the
>#    License for the specific language governing permissions and
>limitations
>#    under the License.
>
>from abc import ABCMeta, abstractmethod
>
>import functools
>import six
>
>from neutron.extensions import portbindings from neutron.openstack.common
>import log from neutron.plugins.ml2 import driver_api as api
>
>LOG = log.getLogger(__name__)
>
>
>DEFAULT_VNIC_TYPES_SUPPORTED = [portbindings.VNIC_DIRECT,
>                                portbindings.VNIC_MACVTAP]
>
>def check_vnic_type_and_vendor_info(f):
>    @functools.wraps(f)
>    def wrapper(self, context):
>        vnic_type = context.current.get(portbindings.VNIC_TYPE,
>                                        portbindings.VNIC_NORMAL)
>        if vnic_type not in self.supported_vnic_types:
>            LOG.debug(_(""%(func_name)s: skipped due to unsupported ""
>                        ""vnic_type: %(vnic_type)s""),
>                      {'func_name': f.func_name, 'vnic_type': vnic_type})
>            return
>
>        if self.supported_pci_vendor_info:
>            profile = context.current.get(portbindings.PROFILE, {})
>            if not profile:
>                LOG.debug(_(""%s: Missing profile in port binding""),
>                          f.func_name)
>                return
>            pci_vendor_info = profile.get('pci_vendor_info')
>            if not pci_vendor_info:
>                LOG.debug(_(""%s: Missing pci vendor info in profile""),
>                          f.func_name)
>                return
>            if pci_vendor_info not in self.supported_pci_vendor_info:
>                LOG.debug(_(""%(func_name)s: unsupported pci vendor ""
>                            ""info: %(info)s""),
>                          {'func_name': f.func_name, 'info':
>pci_vendor_info})
>                return
>        f(self, context)
>    return wrapper
>
>@six.add_metaclass(ABCMeta)
>class SriovMechanismDriverBase(api.MechanismDriver):
>    """"""Base class for drivers that supports SR-IOV
>
>    The SriovMechanismDriverBase provides common code for mechanism
>    drivers that supports SR-IOV. Such a driver may or may not require
>    an agent to be running on the port's host.
>
>    MechanismDrivers that uses this base class and requires an agent must
>    pass the agent type to __init__(), and must implement
>    try_to_bind_segment_for_agent() and check_segment_for_agent().
>
>    MechanismDrivers that uses this base class may provide supported
>vendor
>    information, and must provide the supported vnic types.
>    """"""
>    def __init__(self, agent_type=None, supported_pci_vendor_info=[],
>                 supported_vnic_types=DEFAULT_VNIC_TYPES_SUPPORTED):
>        """"""Initialize base class for SR-IOV capable Mechanism Drivers
>
>        :param agent_type: Constant identifying agent type in agents_db
>        :param supported_pci_vendor_info: a list of ""vendor_id:product_id""
>        :param supported_vnic_types: The binding:vnic_type values we can
>bind
>        """"""
>        self.supported_pci_vendor_info = supported_pci_vendor_info
>        self.agent_type = agent_type
>        self.supported_vnic_types = supported_vnic_types
>
>    def initialize(self):
>        pass
>
>    @check_vnic_type_and_vendor_info
>    def bind_port(self, context):
>        LOG.debug(_(""Attempting to bind port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive']:
>                    for segment in context.network.network_segments:
>                        if self.try_to_bind_segment_for_agent(context,
>segment,
>                                                              agent):
>                            LOG.debug(_(""Bound using segment: %s""),
>segment)
>                            return
>                else:
>                    LOG.warning(_(""Attempting to bind with dead agent:
>%s""),
>                                agent)
>        else:
>            for segment in context.network.network_segments:
>                if self.try_to_bind_segment(context, segment):
>                    LOG.debug(_(""Bound using segment: %s""), segment)
>                    return
>
>    def validate_port_binding(self, context):
>        LOG.debug(_(""Validating binding for port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>        if self.agent_type:
>            for agent in context.host_agents(self.agent_type):
>                LOG.debug(_(""Checking agent: %s""), agent)
>                if agent['alive'] and self.check_segment_for_agent(
>                    context.bound_segment, agent):
>                    LOG.debug(_(""Binding valid""))
>                    return True
>            LOG.warning(_(""Binding invalid for port: %s""),
>context.current)
>            return False
>        else:
>            return True
>
>    @check_vnic_type_and_vendor_info
>    def unbind_port(self, context):
>        LOG.debug(_(""Unbinding port %(port)s on ""
>                    ""network %(network)s""),
>                  {'port': context.current['id'],
>                   'network': context.network.current['id']})
>
>    @abstractmethod
>    def try_to_bind_segment_for_agent(self, context, segment, agent):
>        """"""Try to bind with segment for agent.
>
>        :param context: PortContext instance describing the port
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment has been bound for agent
>
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use agent_db data along with built-in
>        knowledge of the corresponding agent's capabilities to attempt
>        to bind to the specified network segment for the agent.
>
>        If the segment can be bound for the agent, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>    @abstractmethod
>    def check_segment_for_agent(self, segment, agent):
>        """"""Check if segment can be bound for agent.
>
>        :param segment: segment dictionary describing segment to bind
>        :param agent: agents_db entry describing agent to bind
>        :returns: True iff segment can be bound for agent
>
>        Called inside transaction during validate_port_binding() so
>        that derived MechanismDrivers can use agent_db data along with
>        built-in knowledge of the corresponding agent's capabilities
>        to determine whether or not the specified network segment can
>        be bound for the agent.
>        """"""
>
>    @abstractmethod
>    def try_to_bind_segment(self, segment):
>        """"""Check if segment can be bound.
>
>        :param segment: segment dictionary describing segment to bind
>        :returns: True iff segment can be bound
>        
>        Called inside transaction during bind_port() so that derived
>        MechanismDrivers can use database data along with built-in
>        knowledge to attempt to bind to the specified network segment
>
>        If the segment can be bound, this function must
>        call context.set_binding() with appropriate values and then
>        return True. Otherwise, it must return False.
>        """"""
>
>
>
>A SRIOV MD would inherit from it and implement
>try_to_bind_segment_for_agent() and check_segment_for_agent() and
>try_to_bind_segment(). If an agent is required, the first two methods
>should have concrete implementation, and the third may only contain
>'pass'. Otherwise, the first two are 'pass', and the third needs to be
>implemented.
>
>In the inherited class, its __init__ method should set the supported
>pci_vendor_info by either hard coding or by configuration (which is
>preferred so that code doesn't need to be changed with newly supported
>cards).
>in the bind segment method, vif_type and vif_details should be set, and
>set_binding() called.
>
>methods inherited from MechanismDriver, such as
>create_port_precommit()/postcommit(), update_port_precommit()/postcommit
>can be decorated with check_vnic_type_and_vendor_info so that they won't
>be called if vnic_type and vendor_info don't match.
>
>Let me know what you think.
>
>thanks,
>Robert
>
>
>
>
>On 3/4/14 4:08 PM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>
>>Hi,
>>    During today's meeting, it was decided that we would re-purpose
>>Robert's       
>>https://blueprints.launchpad.net/neutron/+spec/pci-passthrough-sriov to
>>take care of adding a Base class to take care of common processing for
>>SR-IOV ports.
>>
>>This class would:
>>
>>1. Inherits from AgentMechanismDriverBase.
>>2. Implements bind_port() where the binding:profile would be checked to
>>see if the port's vnic_type is VNIC_DIRECT or VNIC_MACVTAP.
>>3. Also checks to see that port belongs to vendor/product that supports
>>SR-IOV.
>>4. This class could be used by MDs that may or may not have a valid L2
>>agent.
>>5. Implement validate_port_binding(). This will always return True for
>>Mds that do not have an L2 agent.
>>
>>Please let me know if I left out anything.
>>
>>Thanks,
>>Sandhya
>>On 2/25/14 9:18 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>
>>>Hi,
>>>    As a follow up from today's IRC, Irena, are you looking to write
>>>the below mentioned Base/Mixin class that inherits from
>>>AgentMechanismDriverBase class? When you mentioned port state, were
>>>you referring to the validate_port_binding() method?
>>>
>>>Pls clarify.
>>>
>>>Thanks,
>>>Sandhya
>>>
>>>On 2/6/14 7:57 AM, ""Sandhya Dasu (sadasu)"" <sadasu at cisco.com> wrote:
>>>
>>>>Hi Bob and Irena,
>>>>   Thanks for the clarification. Irena, I am not opposed to a
>>>>SriovMechanismDriverBase/Mixin approach, but I want to first figure
>>>>out how much common functionality there is. Have you already looked at
>>>>this?
>>>>
>>>>Thanks,
>>>>Sandhya
>>>>
>>>>On 2/5/14 1:58 AM, ""Irena Berezovsky"" <irenab at mellanox.com> wrote:
>>>>
>>>>>Please see inline my understanding
>>>>>
>>>>>-----Original Message-----
>>>>>From: Robert Kukura [mailto:rkukura at redhat.com]
>>>>>Sent: Tuesday, February 04, 2014 11:57 PM
>>>>>To: Sandhya Dasu (sadasu); OpenStack Development Mailing List (not
>>>>>for usage questions); Irena Berezovsky; Robert Li (baoli); Brian
>>>>>Bowen
>>>>>(brbowen)
>>>>>Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>>binding of ports
>>>>>
>>>>>On 02/04/2014 04:35 PM, Sandhya Dasu (sadasu) wrote:
>>>>>> Hi,
>>>>>>      I have a couple of questions for ML2 experts regarding
>>>>>>support of  SR-IOV ports.
>>>>>
>>>>>I'll try, but I think these questions might be more about how the
>>>>>various SR-IOV implementations will work than about ML2 itself...
>>>>>
>>>>>> 1. The SR-IOV ports would not be managed by ova or linuxbridge L2
>>>>>> agents. So, how does a MD for SR-IOV ports bind/unbind its ports
>>>>>> to the host? Will it just be a db update?
>>>>>
>>>>>I think whether or not to use an L2 agent depends on the specific
>>>>>SR-IOV implementation. Some (Mellanox?) might use an L2 agent, while
>>>>>others
>>>>>(Cisco?) might put information in binding:vif_details that lets the
>>>>>nova VIF driver take care of setting up the port without an L2
>>>>>agent.
>>>>>[IrenaB] Based on VIF_Type that MD defines, and going forward with
>>>>>other binding:vif_details attributes, VIFDriver should do the VIF
>>>>>pluging part.
>>>>>As for required networking configuration is required, it is usually
>>>>>done either by L2 Agent or external Controller, depends on MD.
>>>>>
>>>>>> 
>>>>>> 2. Also, how do we handle the functionality in mech_agent.py,
>>>>>> within the SR-IOV context?
>>>>>
>>>>>My guess is that those SR-IOV MechanismDrivers that use an L2 agent
>>>>>would inherit the AgentMechanismDriverBase class if it provides
>>>>>useful functionality, but any MechanismDriver implementation is free
>>>>>to not use this base class if its not applicable. I'm not sure if an
>>>>>SriovMechanismDriverBase (or SriovMechanismDriverMixin) class is
>>>>>being planned, and how that would relate to AgentMechanismDriverBase.
>>>>>
>>>>>[IrenaB] Agree with Bob, and as I stated before I think there is a
>>>>>need for SriovMechanismDriverBase/Mixin that provides all the
>>>>>generic functionality and helper methods that are common to SRIOV
>>>>>ports.
>>>>>-Bob
>>>>>
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 3:14 PM
>>>>>> To: ""OpenStack Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, Irena Berezovsky
>>>>>><irenab at mellanox.com <mailto:irenab at mellanox.com>>, ""Robert Li
>>>>>>(baoli)""
>>>>>> <baoli at cisco.com <mailto:baoli at cisco.com>>, Robert Kukura
>>>>>><rkukura at redhat.com <mailto:rkukura at redhat.com>>, ""Brian Bowen
>>>>>>(brbowen)"" <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi,
>>>>>>     Since, openstack-meeting-alt seems to be in use, baoli and
>>>>>> myself are moving to openstack-meeting. Hopefully, Bob Kukura &
>>>>>> Irena can join soon.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>
>>>>>> Reply-To: ""OpenStack Development Mailing List (not for usage
>>>>>>questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>
>>>>>> Date: Monday, February 3, 2014 1:26 PM
>>>>>> To: Irena Berezovsky <irenab at mellanox.com
>>>>>><mailto:irenab at mellanox.com>>, ""Robert Li (baoli)"" <baoli at cisco.com
>>>>>><mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>>>><mailto:rkukura at redhat.com>>, ""OpenStack Development Mailing List
>>>>>>(not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV  extra hr of discussion today
>>>>>> 
>>>>>> Hi all,
>>>>>>     Both openstack-meeting and openstack-meeting-alt are available
>>>>>> today. Lets meet at UTC 2000 @ openstack-meeting-alt.
>>>>>> 
>>>>>> Thanks,
>>>>>> Sandhya
>>>>>> 
>>>>>> From: Irena Berezovsky <irenab at mellanox.com
>>>>>><mailto:irenab at mellanox.com>>
>>>>>> Date: Monday, February 3, 2014 12:52 AM
>>>>>> To: Sandhya Dasu <sadasu at cisco.com <mailto:sadasu at cisco.com>>,
>>>>>>""Robert  Li (baoli)"" <baoli at cisco.com <mailto:baoli at cisco.com>>,
>>>>>>Robert Kukura  <rkukura at redhat.com <mailto:rkukura at redhat.com>>,
>>>>>>""OpenStack  Development Mailing List (not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> Subject: RE: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>>SRIOV on  Jan. 30th
>>>>>> 
>>>>>> Hi Sandhya,
>>>>>> 
>>>>>> Can you please elaborate how do you suggest to extend the below bp
>>>>>>for  SRIOV Ports managed by different Mechanism Driver?
>>>>>> 
>>>>>> I am not biased to any specific direction here, just think we need
>>>>>> common layer for managing SRIOV port at neutron, since there is a
>>>>>> common pass between nova and neutron.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> BR,
>>>>>> 
>>>>>> Irena
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Sandhya Dasu (sadasu) [mailto:sadasu at cisco.com]
>>>>>> *Sent:* Friday, January 31, 2014 6:46 PM
>>>>>> *To:* Irena Berezovsky; Robert Li (baoli); Robert Kukura;
>>>>>> OpenStack Development Mailing List (not for usage questions);
>>>>>> Brian Bowen
>>>>>> (brbowen)
>>>>>> *Subject:* Re: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>> SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi Irena,
>>>>>> 
>>>>>>       I was initially looking
>>>>>> at
>>>>>> 
>>>>>>https://blueprints.launchpad.net/neutron/+spec/ml2-typedriver-extra
>>>>>>-po
>>>>>>r
>>>>>>t
>>>>>>-
>>>>>>info to take care of the extra information required to set up the
>>>>>>SR-IOV port.
>>>>>> When the scope of the BP was being decided, we had very little
>>>>>>info  about our own design so I didn't give any feedback about
>>>>>>SR-IOV ports.
>>>>>> But, I feel that this is the direction we should be going. Maybe
>>>>>>we  should target this in Juno.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Introducing, */SRIOVPortProfileMixin /*would be creating yet
>>>>>> another way to take care of extra port config. Let me know what you
>>>>>>think.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Sandhya
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From: *Irena Berezovsky <irenab at mellanox.com
>>>>>> <mailto:irenab at mellanox.com>>
>>>>>> *Date: *Thursday, January 30, 2014 4:13 PM
>>>>>> *To: *""Robert Li (baoli)"" <baoli at cisco.com
>>>>>> <mailto:baoli at cisco.com>>, Robert Kukura <rkukura at redhat.com
>>>>>> <mailto:rkukura at redhat.com>>, Sandhya Dasu <sadasu at cisco.com
>>>>>> <mailto:sadasu at cisco.com>>, ""OpenStack Development Mailing List
>>>>>>(not for usage questions)""
>>>>>> <openstack-dev at lists.openstack.org
>>>>>> <mailto:openstack-dev at lists.openstack.org>>, ""Brian Bowen (brbowen)""
>>>>>> <brbowen at cisco.com <mailto:brbowen at cisco.com>>
>>>>>> *Subject: *RE: [openstack-dev] [nova][neutron] PCI pass-through
>>>>>> SRIOV on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Robert,
>>>>>> 
>>>>>> Thank you very much for the summary.
>>>>>> 
>>>>>> Please, see inline
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> *From:*Robert Li (baoli) [mailto:baoli at cisco.com]
>>>>>> *Sent:* Thursday, January 30, 2014 10:45 PM
>>>>>> *To:* Robert Kukura; Sandhya Dasu (sadasu); Irena Berezovsky;
>>>>>> OpenStack Development Mailing List (not for usage questions);
>>>>>> Brian Bowen (brbowen)
>>>>>> *Subject:* [openstack-dev] [nova][neutron] PCI pass-through SRIOV
>>>>>> on Jan. 30th
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> We made a lot of progress today. We agreed that:
>>>>>> 
>>>>>> -- vnic_type will be a top level attribute as binding:vnic_type
>>>>>> 
>>>>>> -- BPs:
>>>>>> 
>>>>>>      * Irena's
>>>>>> https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-ty
>>>>>> pe
>>>>>> for binding:vnic_type
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:profile in ML2. SRIOV input
>>>>>>info  will be encapsulated in binding:profile
>>>>>> 
>>>>>>      * Bob to submit a BP for binding:vif_details in ML2. SRIOV
>>>>>>output  info will be encapsulated in binding:vif_details, which may
>>>>>>include  other information like security parameters. For SRIOV,
>>>>>>vlan_id and  profileid are candidates.
>>>>>> 
>>>>>> -- new arguments for port-create will be implicit arguments.
>>>>>> Future release may make them explicit. New argument:
>>>>>> --binding:vnic_type {virtio, direct, macvtap}.
>>>>>> 
>>>>>> I think that currently we can make do without the profileid as an
>>>>>> input parameter from the user. The mechanism driver will return a
>>>>>> profileid in the vif output.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Please correct any misstatement in above.
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Issues: 
>>>>>> 
>>>>>>   -- do we need a common utils/driver for SRIOV generic parts to
>>>>>> be used by individual Mechanism drivers that support SRIOV? More
>>>>>> details on what would be included in this sriov utils/driver? I'm
>>>>>> thinking that a candidate would be the helper functions to
>>>>>> interpret the pci_slot, which is proposed as a string. Anything
>>>>>>else in your mind?
>>>>>> 
>>>>>> */[IrenaB] I thought on some SRIOVPortProfileMixin to handle and
>>>>>> persist SRIOV port related attributes/*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>   -- what should mechanism drivers put in binding:vif_details and
>>>>>> how nova would use this information? as far as I see it from the
>>>>>> code, a VIF object is created and populated based on information
>>>>>> provided by neutron (from get network and get port)
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Questions:
>>>>>> 
>>>>>>   -- nova needs to work with both ML2 and non-ML2 plugins. For
>>>>>>regular  plugins, binding:vnic_type will not be set, I guess. Then
>>>>>>would it be  treated as a virtio type? And if a non-ML2 plugin
>>>>>>wants to support  SRIOV, would it need to  implement vnic-type,
>>>>>>binding:profile,  binding:vif-details for SRIOV itself?
>>>>>> 
>>>>>> */[IrenaB] vnic_type will be added as an additional attribute to
>>>>>> binding extension. For persistency it should be added in
>>>>>> PortBindingMixin for non ML2. I didn't think to cover it as part
>>>>>> of
>>>>>> ML2 vnic_type bp./*
>>>>>> 
>>>>>> */For the rest attributes, need to see what Bob plans./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>>  -- is a neutron agent making decision based on the
>>>>>>binding:vif_type?
>>>>>>  In that case, it makes sense for binding:vnic_type not to be
>>>>>> exposed to agents.
>>>>>> 
>>>>>> */[IrenaB] vnic_type is input parameter that will eventually cause
>>>>>> certain vif_type to be sent to GenericVIFDriver and create network
>>>>>> interface. Neutron agents periodically scan for attached interfaces.
>>>>>> For example, OVS agent will look only for OVS interfaces, so if
>>>>>> SRIOV interface is created, it won't be discovered by OVS agent./*
>>>>>> 
>>>>>>  
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> Robert
>>>>>> 
>>>>>
>>>>
>>>>
>>>>_______________________________________________
>>>>OpenStack-dev mailing list
>>>>OpenStack-dev at lists.openstack.org
>>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>>
>>>_______________________________________________
>>>OpenStack-dev mailing list
>>>OpenStack-dev at lists.openstack.org
>>>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>


"
NOVA 1402728 - CG,msg26821,<C227399E0701F545B768C425CD0EC3E0B7045844@ENFIRHMBX1.datcon.co.uk>,"[openstack-dev] [NFV] Specific example NFV use case for a data
	plane app

--

Hello all

At Wednesday's meeting I promised to supply specific examples to help 
illustrate the NFV use cases and also show how they map to some of the
blueprints.  Here's my first example - info on our session border
controller, which is a data plane app.  Please let me know if this is 
the sort of example and detail the group are looking for, then I can
add it into the wiki and send out info on the second, a vIMS core. 

Use case example
----------------

Perimeta Session Border Controller, Metaswitch Networks.  Sits on the
edge of a service provider's network and polices SIP and RTP (i.e. VoIP)
control and media traffic passing over the access network between 
end-users and the core network or the trunk network between the core and
another SP.  

Characteristics relevant to NFV/OpenStack
-----------------------------------------

Fast & guaranteed performance:
-	fast = performance of order of several million VoIP packets (~64-220
bytes depending on codec) per second per core (achievable on COTS hardware)
-	guaranteed via SLAs.

Fully HA, with no SPOFs and service continuity over software and hardware
failures.

Elastically scalable by adding/removing instances under the control of the
NFV orchestrator.

Ideally, ability to separate traffic from different customers via VLANs.

Requirements and mapping to blueprints
--------------------------------------

Fast & guaranteed performance - implications for network:

-	the packets per second target -> either SR-IOV or an accelerated
	DPDK-like data plane
	-	maps to the SR-IOV and accelerated vSwitch blueprints:
		-	""SR-IOV Networking Support"" (https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov)
		-	""Open vSwitch to use patch ports"" (https://blueprints.launchpad.net/neutron/+spec/openvswitch-patch-port-use)
		-	""userspace vhost in ovd vif bindings"" (https://blueprints.launchpad.net/nova/+spec/libvirt-ovs-use-usvhost)
		-	""Snabb NFV driver"" (https://blueprints.launchpad.net/neutron/+spec/snabb-nfv-mech-driver)
		-	""VIF_SNABB"" (https://blueprints.launchpad.net/nova/+spec/vif-snabb)

Fast & guaranteed performance - implications for compute:

-	to optimize data rate we need to keep all working data in L3 cache
	-> need to be able to pin cores
	-	""Virt driver pinning guest vCPUs to host pCPUs"" (https://blueprints.launchpad.net/nova/+spec/virt-driver-cpu-pinning)

-	similarly to optimize data rate need to bind to NIC on host CPU's bus
	-	""I/O (PCIe) Based NUMA Scheduling"" (https://blueprints.launchpad.net/nova/+spec/input-output-based-numa-scheduling)

-	to offer guaranteed performance as opposed to 'best efforts' we need
	to control placement of cores, minimise TLB misses and get accurate 
	info about core topology (threads vs. hyperthreads etc.); maps to the
	remaining blueprints on NUMA & vCPU topology:
	-	""Virt driver guest vCPU topology configuration"" (https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology)
	-	""Virt driver guest NUMA node placement & topology"" (https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement)
	-	""Virt driver large page allocation for guest RAM"" (https://blueprints.launchpad.net/nova/+spec/virt-driver-large-pages)

-	may need support to prevent 'noisy neighbours' stealing L3 cache -
	unproven, and no blueprint we're aware of.

HA:
-	requires anti-affinity rules to prevent active/passive being
	instantiated on same host - already supported, so no gap.

Elastic scaling:
-	similarly readily achievable using existing features - no gap.

VLAN trunking:
-	maps straightforwardly to ""VLAN trunking networks for NFV"" (https://blueprints.launchpad.net/neutron/+spec/nfv-vlan-trunks et al).

Other:
-	being able to offer apparent traffic separation (e.g. service
	traffic vs. application management) over single network is also
	useful in some cases
	-	""Support two interfaces from one VM attached to the same network"" (https://blueprints.launchpad.net/nova/+spec/2-if-1-net)

regards

Calum


Calum Loudon 
Director, Architecture
+44 (0)208 366 1177
?
METASWITCH NETWORKS 
THE BRAINS OF THE NEW GLOBAL NETWORK
www.metaswitch.com



"
NOVA 1402728 - CG,msg33893,<D02C8B1F.B33F4%baoli@cisco.com>,"[openstack-dev] [nova][SR-IOV] Please review this patch series:
 replace pci_request storage with proper object usage

--

Hi,

the patch series:
 https://review.openstack.org/#/c/117781/5
https://review.openstack.org/#/c/117895/
https://review.openstack.org/#/c/117839/
 https://review.openstack.org/#/c/118391/

is ready for review. This needs to get in before Juno feature freeze so that the sr-iov patches  can land in Juno. Refer to https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov for all the patches related to SR-IOV.

thanks,
Robert



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140903/66c5ae76/attachment.html>
"
NOVA 1402728 - CG,msg15634,<DDCAE26804250545B9934A2056554AA01FBFC394@ORSMSX108.amr.corp.intel.com>,"[openstack-dev] The simplified blueprint for PCI extra attributes

--

Hi, John and all,
	I updated the blueprint https://blueprints.launchpad.net/nova/+spec/pci-extra-info-icehouse  according to your feedback, to add the backward compatibility/upgrade issue/examples.

	I try to separate this BP with the SR-IOV NIC support as a standalone enhancement, because this requirement is more a generic PCI pass through feature, and will benefit other usage scenario as well.

	And the reasons that I want to finish this BP in I release are:

	a) it's a generic requirement, and push it into I release is helpful to other scenario.
	b) I don't see upgrade issue, and the only thing will be discarded in future is the PCI alias if we all agree to use PCI flavor. But that effort will be small and there is no conclusion to PCI flavor yet.
	c) SR-IOV NIC support is complex, it will be really helpful if we can keep ball rolling and push the all-agreed items forward. 

	Considering the big patch list for I-3 release, I'm not optimistic to merge this in I release, but as said, we should keep the ball rolling and move forward.

Thanks
--jyh

"
NOVA 1402728 - CG,msg15735,<CF16CE2D.403E3%baoli@cisco.com>,"[openstack-dev] The simplified blueprint for PCI extra
 attributes

--

Hi Yunhong,

A couple of questions:
   -- about the pci_information config item in your spec. What is a
device_id? 

   -- in libvirt driver, we need to retrieve the PCI devices allocated for
the requested networks. These PCI devices won't be treated as hostdev
devices in the domain xml, rather as interfaces. Shall it be specified in
your spec how this is going to be supported?

thanks,
Robert

On 2/4/14 1:36 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com> wrote:

>Hi, John and all,
>	I updated the blueprint
>https://blueprints.launchpad.net/nova/+spec/pci-extra-info-icehouse
>according to your feedback, to add the backward compatibility/upgrade
>issue/examples.
>
>	I try to separate this BP with the SR-IOV NIC support as a standalone
>enhancement, because this requirement is more a generic PCI pass through
>feature, and will benefit other usage scenario as well.
>
>	And the reasons that I want to finish this BP in I release are:
>
>	a) it's a generic requirement, and push it into I release is helpful to
>other scenario.
>	b) I don't see upgrade issue, and the only thing will be discarded in
>future is the PCI alias if we all agree to use PCI flavor. But that
>effort will be small and there is no conclusion to PCI flavor yet.
>	c) SR-IOV NIC support is complex, it will be really helpful if we can
>keep ball rolling and push the all-agreed items forward.
>
>	Considering the big patch list for I-3 release, I'm not optimistic to
>merge this in I release, but as said, we should keep the ball rolling and
>move forward.
>
>Thanks
>--jyh
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1402728 - CG,msg15810,<CF17BC24.40B13%baoli@cisco.com>,"[openstack-dev] The simplified blueprint for PCI extra
 attributes and SR-IOV NIC blueprint

--

Hi John and all,

Yunhong's email mentioned about the SR-IOV NIC support BP:
https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov

I'd appreciate your consideration of the approval of both BPs so that we
can have SR-IOV NIC support in Icehouse.

Thanks,
Robert


On 2/4/14 1:36 AM, ""Jiang, Yunhong"" <yunhong.jiang at intel.com> wrote:

>Hi, John and all,
>	I updated the blueprint
>https://blueprints.launchpad.net/nova/+spec/pci-extra-info-icehouse
>according to your feedback, to add the backward compatibility/upgrade
>issue/examples.
>
>	I try to separate this BP with the SR-IOV NIC support as a standalone
>enhancement, because this requirement is more a generic PCI pass through
>feature, and will benefit other usage scenario as well.
>
>	And the reasons that I want to finish this BP in I release are:
>
>	a) it's a generic requirement, and push it into I release is helpful to
>other scenario.
>	b) I don't see upgrade issue, and the only thing will be discarded in
>future is the PCI alias if we all agree to use PCI flavor. But that
>effort will be small and there is no conclusion to PCI flavor yet.
>	c) SR-IOV NIC support is complex, it will be really helpful if we can
>keep ball rolling and push the all-agreed items forward.
>
>	Considering the big patch list for I-3 release, I'm not optimistic to
>merge this in I release, but as said, we should keep the ball rolling and
>move forward.
>
>Thanks
>--jyh
>
>_______________________________________________
>OpenStack-dev mailing list
>OpenStack-dev at lists.openstack.org
>http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev


"
NOVA 1402728 - CG,msg27791,<53AA21E5.6040409@intel.com>,"[openstack-dev] [nova][pci] A couple of questions

--

Hi, Robert, Irenab

does your patches are properly seting up the topic, like 
pci-passthrough-sriov?
all SRIOV patch need this tag i think , help people find this set of 
patch to review.

https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/pci-passthrough-sriov,n,z

Yongli He

"
NOVA 1434855 - CG,msg09183,<CANCY3ed2KTw0xMQa2Bf7xxFMEsLwoo+7=mujmrz4cztjgL7P8w@mail.gmail.com>,"[openstack-dev] [nova] V3 API blueprints

--

Hi,

Some people have expressed interest in helping out with the V3 API related
work during Icehouse. The following are the relevant blueprints based on
ongoing work from Havana and discussions during the summit sessions:

Ongoing blueprints

https://blueprints.launchpad.net/nova/+spec/nova-v3-api (capstone blueprint)
https://blueprints.launchpad.net/nova/+spec/v3-api-cleanup-misc
https://blueprints.launchpad.net/nova/+spec/v3-api-specification
https://blueprints.launchpad.net/python-novaclient/+spec/v3-api
https://blueprints.launchpad.net/tempest/+spec/nova-v3-api-tests
https://blueprints.launchpad.net/nova/+spec/nova-api-validation-fw

New in Icehouse

https://blueprints.launchpad.net/nova/+spec/api-v3-remove-disk-config
https://blueprints.launchpad.net/nova/+spec/v3-api-core
https://blueprints.launchpad.net/nova/+spec/v3-api-admin-actions-split
https://blueprints.launchpad.net/nova/+spec/v3-api-remove-nova-network
https://blueprints.launchpad.net/nova/+spec/v3-api-remove-extensions
https://blueprints.launchpad.net/nova/+spec/v3-api-pecan
https://blueprints.launchpad.net/nova/+spec/v3-api-policy
https://blueprints.launchpad.net/nova/+spec/v3-api-core-policy

There'll be a few people working on these so if you decide to work on
something please add a relevant entry in the work item section before
starting in order to avoid duplication of effort. Please ping me if you
think anything needs further explanation
or you want a bit of a hand getting started on something.

Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131119/316e6b5c/attachment.html>
"
NOVA 1434855 - CG,msg51155,<5537A2FD.2050907@redhat.com>,"[openstack-dev] [nova] Policy rules are killed by the context admin
	check

--

Hi,

By discussing on a specific bug [1], I just discovered that the admin 
context check which was done at the DB level has been moved to the API 
level thanks to the api-policy-v3 blueprint [2]

That behaviour still leads to a bug if the operator wants to change an 
endpoint policy by leaving it end-user but still continues to be denied 
because of that, as it will forbid any non-admin user to call the 
methods (even if authorize() grants the request)

I consequently opened a bug [3] for this but I'm also concerned about 
the backportability of that and why it shouldn't fixed in v2.0 too.

Releasing the check by removing it sounds an acceptable change, as it 
fixes a bug without changing the expected behaviour [4]. The impact of 
the change sounds also minimal with a very precise scope (ie. leave the 
policy rules work as they are expected) [5]

Folks, thoughts ?

-Sylvain

[1] https://bugs.launchpad.net/nova/+bug/1447084
[2] 
https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
[3] https://bugs.launchpad.net/nova/+bug/1447164
[4] 
https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK 
""Fixing a bug so that a request which resulted in an error response 
before is now successful""
[5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy

"
NOVA 1434855 - CG,msg51212,<553826F8.9040904@linux.vnet.ibm.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--



On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
> Hi,
>
> By discussing on a specific bug [1], I just discovered that the admin
> context check which was done at the DB level has been moved to the API
> level thanks to the api-policy-v3 blueprint [2]
>
> That behaviour still leads to a bug if the operator wants to change an
> endpoint policy by leaving it end-user but still continues to be denied
> because of that, as it will forbid any non-admin user to call the
> methods (even if authorize() grants the request)
>
> I consequently opened a bug [3] for this but I'm also concerned about
> the backportability of that and why it shouldn't fixed in v2.0 too.
>
> Releasing the check by removing it sounds an acceptable change, as it
> fixes a bug without changing the expected behaviour [4]. The impact of
> the change sounds also minimal with a very precise scope (ie. leave the
> policy rules work as they are expected) [5]
>
> Folks, thoughts ?
>
> -Sylvain
>
> [1] https://bugs.launchpad.net/nova/+bug/1447084
> [2]
> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
> [3] https://bugs.launchpad.net/nova/+bug/1447164
> [4]
> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
> ""Fixing a bug so that a request which resulted in an error response
> before is now successful""
> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

I don't disagree, see bug 1168488 from way back in grizzly.

The only thing would be we'd have to make sure the default rule is admin 
for any v2 extensions which are enforcing an admin context today.

-- 

Thanks,

Matt Riedemann


"
NOVA 1434855 - CG,msg51213,<CAGnj6auHXehCyr_b5tYUWVVEOFz9eoDMO-GngBiwxpA4Bij_DQ@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

On Wednesday, April 22, 2015, Matt Riedemann <mriedem at linux.vnet.ibm.com>
wrote:

>
>
> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>
>> Hi,
>>
>> By discussing on a specific bug [1], I just discovered that the admin
>> context check which was done at the DB level has been moved to the API
>> level thanks to the api-policy-v3 blueprint [2]
>>
>> That behaviour still leads to a bug if the operator wants to change an
>> endpoint policy by leaving it end-user but still continues to be denied
>> because of that, as it will forbid any non-admin user to call the
>> methods (even if authorize() grants the request)
>>
>> I consequently opened a bug [3] for this but I'm also concerned about
>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>
>> Releasing the check by removing it sounds an acceptable change, as it
>> fixes a bug without changing the expected behaviour [4]. The impact of
>> the change sounds also minimal with a very precise scope (ie. leave the
>> policy rules work as they are expected) [5]
>>
>> Folks, thoughts ?
>>
>> -Sylvain
>>
>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>> [2]
>>
>> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>> [4]
>>
>> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>> ""Fixing a bug so that a request which resulted in an error response
>> before is now successful""
>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>
>> __________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
>> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
> I don't disagree, see bug 1168488 from way back in grizzly.
>
> The only thing would be we'd have to make sure the default rule is admin
> for any v2 extensions which are enforcing an admin context today.
>
>
This sounds like a sane approach.

--Morgan

> --
>
> Thanks,
>
> Matt Riedemann
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150422/8769514e/attachment.html>
"
NOVA 1434855 - CG,msg51240,<CAH7mGatE4Ua_fxmrAHCC6kbFE_uHnvoJ8u5vxVNFnOAQdrcVsg@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com>:

>
>
> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>
>> Hi,
>>
>> By discussing on a specific bug [1], I just discovered that the admin
>> context check which was done at the DB level has been moved to the API
>> level thanks to the api-policy-v3 blueprint [2]
>>
>> That behaviour still leads to a bug if the operator wants to change an
>> endpoint policy by leaving it end-user but still continues to be denied
>> because of that, as it will forbid any non-admin user to call the
>> methods (even if authorize() grants the request)
>>
>> I consequently opened a bug [3] for this but I'm also concerned about
>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>
>> Releasing the check by removing it sounds an acceptable change, as it
>> fixes a bug without changing the expected behaviour [4]. The impact of
>> the change sounds also minimal with a very precise scope (ie. leave the
>> policy rules work as they are expected) [5]
>>
>> Folks, thoughts ?
>>
>> -Sylvain
>>
>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>> [2]
>>
>> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>> [4]
>>
>> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>> ""Fixing a bug so that a request which resulted in an error response
>> before is now successful""
>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>
>> __________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
>> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
> I don't disagree, see bug 1168488 from way back in grizzly.
>
> The only thing would be we'd have to make sure the default rule is admin
> for any v2 extensions which are enforcing an admin context today.


Agree, if we want to fix those for v2, we need make sure the default rule
is admin.

And do you mean [3] want to fix this for v2 both in Kilo and Liberty?

For liberty, we can do that, but I think we will switch to v2.1 very soon.
Not sure it is still worth to do that.

For kilo, some of api is pretty easy to fix by just removing
'require_admin_context()'. But there still have many of policy patches
didn't merged into the master yet. like:
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z

Should we back-port them all?


>
> --
>
> Thanks,
>
> Matt Riedemann
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150423/ca1ac933/attachment.html>
"
NOVA 1434855 - CG,msg51248,<CANWE-CnOs0L454G3VpHcV4-+oPDOn9vVar+P49pUZjar1GFswg@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

Le 23 avr. 2015 04:49, ""Alex Xu"" <soulxu at gmail.com> a ?crit :
>
>
>
> 2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com>:
>>
>>
>>
>> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>>>
>>> Hi,
>>>
>>> By discussing on a specific bug [1], I just discovered that the admin
>>> context check which was done at the DB level has been moved to the API
>>> level thanks to the api-policy-v3 blueprint [2]
>>>
>>> That behaviour still leads to a bug if the operator wants to change an
>>> endpoint policy by leaving it end-user but still continues to be denied
>>> because of that, as it will forbid any non-admin user to call the
>>> methods (even if authorize() grants the request)
>>>
>>> I consequently opened a bug [3] for this but I'm also concerned about
>>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>>
>>> Releasing the check by removing it sounds an acceptable change, as it
>>> fixes a bug without changing the expected behaviour [4]. The impact of
>>> the change sounds also minimal with a very precise scope (ie. leave the
>>> policy rules work as they are expected) [5]
>>>
>>> Folks, thoughts ?
>>>
>>> -Sylvain
>>>
>>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>>> [2]
>>>
https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>>
>>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>>> [4]
>>>
https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>>> ""Fixing a bug so that a request which resulted in an error response
>>> before is now successful""
>>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>>
>>>
__________________________________________________________________________
>>> OpenStack Development Mailing List (not for usage questions)
>>> Unsubscribe:
OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>
>> I don't disagree, see bug 1168488 from way back in grizzly.
>>
>> The only thing would be we'd have to make sure the default rule is admin
for any v2 extensions which are enforcing an admin context today.
>
>
> Agree, if we want to fix those for v2, we need make sure the default rule
is admin.
>
> And do you mean [3] want to fix this for v2 both in Kilo and Liberty?
>
> For liberty, we can do that, but I think we will switch to v2.1 very
soon. Not sure it is still worth to do that.
>
> For kilo, some of api is pretty easy to fix by just removing
'require_admin_context()'. But there still have many of policy patches
didn't merged into the master yet. like:
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z
>
> Should we back-port them all?

Wrt all the necessary backports, I'm eventually rather in favor of an
opportunistic approach and only backport what has been reported as a bug,
ie. [1]

That has also the benefit of not proposing a stable patch which was not
cherry-picked (like providing an elevated context), which I disapprove.

-Sylvain

>
>>
>>
>> --
>>
>> Thanks,
>>
>> Matt Riedemann
>>
>>
>>
>>
__________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150423/431de072/attachment.html>
"
NOVA 1434855 - CG,msg51552,<553E39D7.9080909@redhat.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--



Le 23/04/2015 09:10, Sylvain Bauza a ?crit :
>
>
> Le 23 avr. 2015 04:49, ""Alex Xu"" <soulxu at gmail.com 
> <mailto:soulxu at gmail.com>> a ?crit :
> >
> >
> >
> > 2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com 
> <mailto:mriedem at linux.vnet.ibm.com>>:
> >>
> >>
> >>
> >> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
> >>>
> >>> Hi,
> >>>
> >>> By discussing on a specific bug [1], I just discovered that the admin
> >>> context check which was done at the DB level has been moved to the API
> >>> level thanks to the api-policy-v3 blueprint [2]
> >>>
> >>> That behaviour still leads to a bug if the operator wants to change an
> >>> endpoint policy by leaving it end-user but still continues to be 
> denied
> >>> because of that, as it will forbid any non-admin user to call the
> >>> methods (even if authorize() grants the request)
> >>>
> >>> I consequently opened a bug [3] for this but I'm also concerned about
> >>> the backportability of that and why it shouldn't fixed in v2.0 too.
> >>>
> >>> Releasing the check by removing it sounds an acceptable change, as it
> >>> fixes a bug without changing the expected behaviour [4]. The impact of
> >>> the change sounds also minimal with a very precise scope (ie. 
> leave the
> >>> policy rules work as they are expected) [5]
> >>>
> >>> Folks, thoughts ?
> >>>
> >>> -Sylvain
> >>>
> >>> [1] https://bugs.launchpad.net/nova/+bug/1447084
> >>> [2]
> >>> 
> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
> >>>
> >>> [3] https://bugs.launchpad.net/nova/+bug/1447164
> >>> [4]
> >>> 
> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
> >>> ""Fixing a bug so that a request which resulted in an error response
> >>> before is now successful""
> >>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
> >>>
> >>> 
> __________________________________________________________________________
> >>> OpenStack Development Mailing List (not for usage questions)
> >>> Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> >>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >>>
> >>
> >> I don't disagree, see bug 1168488 from way back in grizzly.
> >>
> >> The only thing would be we'd have to make sure the default rule is 
> admin for any v2 extensions which are enforcing an admin context today.
> >
> >
> > Agree, if we want to fix those for v2, we need make sure the default 
> rule is admin.
> >
> > And do you mean [3] want to fix this for v2 both in Kilo and Liberty?
> >
> > For liberty, we can do that, but I think we will switch to v2.1 very 
> soon. Not sure it is still worth to do that.
> >
> > For kilo, some of api is pretty easy to fix by just removing 
> 'require_admin_context()'. But there still have many of policy patches 
> didn't merged into the master yet. like:
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z
> >
> > Should we back-port them all?
>
> Wrt all the necessary backports, I'm eventually rather in favor of an 
> opportunistic approach and only backport what has been reported as a 
> bug, ie. [1]
>
> That has also the benefit of not proposing a stable patch which was 
> not cherry-picked (like providing an elevated context), which I 
> disapprove.
>
> -Sylvain
>

Just a follow-up on that, I'm proposing a change to the approved spec to 
match that discussion :
https://review.openstack.org/177764

Basically, the idea is not to backport all of the efforts, just make 
sure that default policies are admin-only for the corresponding API 
endpoints that call DB API methods which are decorated by 
require_admin_context()

-Sylvain

> >
> >>
> >>
> >> --
> >>
> >> Thanks,
> >>
> >> Matt Riedemann
> >>
> >>
> >>
> >> 
> __________________________________________________________________________
> >> OpenStack Development Mailing List (not for usage questions)
> >> Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> >> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >
> >
> >
> > 
> __________________________________________________________________________
> > OpenStack Development Mailing List (not for usage questions)
> > Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150427/852d7f2f/attachment.html>
"
NOVA 1434855 - CG,msg45356,<54D53AC1.7060902@linux.vnet.ibm.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
	stand up?

--

I'm not going to hide it, I don't know what's going on with the v2.1 API 
status, i.e. what is the criteria to that thing dropping it's 
'experimental' label?

I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I 
was a bit lost. It's not my fault, I'm more good looks than brains. :)

When I look at approved specs for Kilo, three pop out:

1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api

2. https://blueprints.launchpad.net/nova/+spec/api-microversions

3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy

The only one of those that has a dependency in launchpad is the last one 
and it's dependency is on:

https://blueprints.launchpad.net/nova/+spec/nova-v3-api

Which looks like it was replaced by the v2-on-v3-api blueprint.

If I look at the open changes for each, there are a lot:

1. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z

2. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/api-microversions,n,z

3. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z

Do those all need to merge before the v2.1 API is no longer experimental?

Is the, for lack of a better term, 'completion criteria', being tracked 
in an etherpad or wiki page somewhere?  I see stuff in the priorities 
etherpad https://etherpad.openstack.org/p/kilo-nova-priorities-tracking 
but it's not clear to me at a high level what makes v2.1 no longer 
experimental.

Can someone provide that in less than 500 words?

-- 

Thanks,

Matt Riedemann


"
NOVA 1434855 - CG,msg45386,<CANCY3ee4zZN0eccxoYhQQNrQV0NV3TsOW108g0+HkagSM_6QFw@mail.gmail.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
 stand up?

--

Hi,

On Sat, Feb 7, 2015 at 8:35 AM, Matt Riedemann <mriedem at linux.vnet.ibm.com>
wrote:

> I'm not going to hide it, I don't know what's going on with the v2.1 API
> status, i.e. what is the criteria to that thing dropping it's
> 'experimental' label?
>
>
So I caught up with Matt on IRC, repeating some references and discussion
here for everyone else


> I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I
> was a bit lost. It's not my fault, I'm more good looks than brains. :)
>
> When I look at approved specs for Kilo, three pop out:
>
> 1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
>
> 2. https://blueprints.launchpad.net/nova/+spec/api-microversions
>
> 3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy
>
>
So we need the first to blueprints for v2.1 microversions. We don't need
v3-api-policy merged to release
 v2.1 microversions though I believe it is a separate important bit of work
to reduce tech debt and make
life easier for operators.




> The only one of those that has a dependency in launchpad is the last one
> and it's dependency is on:
>
> https://blueprints.launchpad.net/nova/+spec/nova-v3-api
>
> Which looks like it was replaced by the v2-on-v3-api blueprint.
>
> If I look at the open changes for each, there are a lot:
>
> 1. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z
>
> 2. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/api-microversions,n,z
>
> 3. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
> Do those all need to merge before the v2.1 API is no longer experimental?
>
>
We have an etherpad here which tracks our release criteria for v2.1 and
microversions:

https://etherpad.openstack.org/p/v2_1_ReleaseCriteria

As mentioned above it doesn't include api-policy

To make life easier for us I'd also like to request that if you review a
changeset that modifies the v2 api that you ensure it also if required is
applied to v2.1(v3 code). If it doesn't apply to v3 then ensure a
v2-only tag is in the commit message. That will help us verify v2 does not
diverge from v2.1 just before
release. After that I think v2 code will be essentially frozen except for
bug fixes and any api changes will
only be made through microversions.

Regards,

Chris

Is the, for lack of a better term, 'completion criteria', being tracked in
> an etherpad or wiki page somewhere?  I see stuff in the priorities etherpad
> https://etherpad.openstack.org/p/kilo-nova-priorities-tracking but it's
> not clear to me at a high level what makes v2.1 no longer experimental.
>
> Can someone provide that in less than 500 words?
>
> --
>
> Thanks,
>
> Matt Riedemann
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150209/32a0a5af/attachment.html>
"
NOVA 1434855 - CG,msg45388,<CAA393viO0_oZed6QKO9AZrrDyXgbq--LzmdprjtbPrSHoEkJWw@mail.gmail.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
 stand up?

--

2015-02-09 9:26 GMT+09:00 Christopher Yeoh <cbkyeoh at gmail.com>:
>>
>> I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I
>> was a bit lost. It's not my fault, I'm more good looks than brains. :)
>>
>> When I look at approved specs for Kilo, three pop out:
>>
>> 1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
>>
>> 2. https://blueprints.launchpad.net/nova/+spec/api-microversions
>>
>> 3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy
>>
>
> So we need the first to blueprints for v2.1 microversions. We don't need
> v3-api-policy merged to release
>  v2.1 microversions though I believe it is a separate important bit of work
> to reduce tech debt and make
> life easier for operators.

I think the first one(bp/v2-on-v3-api) only is necessary for dropping
'experimental' label from v2.1 API, because raw v2.1 API(meaning
v2.1 without microversions) has already implemented some merits
(strong validation, clean code, easy deployment, etc) for developers
and operators.

>> The only one of those that has a dependency in launchpad is the last one
>> and it's dependency is on:
>>
>> https://blueprints.launchpad.net/nova/+spec/nova-v3-api
>>
>> Which looks like it was replaced by the v2-on-v3-api blueprint.
>>
>> If I look at the open changes for each, there are a lot:
>>
>> 1.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z
>>
>> 2.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/api-microversions,n,z
>>
>> 3.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> Do those all need to merge before the v2.1 API is no longer experimental?

As I said at the above, the first branch is necessary for dropping
experimental from v2.1 API. In addition, the first branch contains
some cleanup patches also. The important patches are just four:

* Provide full v2 compatibility
  https://review.openstack.org/#/c/138599/
  https://review.openstack.org/#/c/153466/
  https://review.openstack.org/#/c/153137

* Drop experimental from v2
  https://review.openstack.org/149948

So we have already done most works for v2.1 API.

Thanks
Ken Ohmichi

"
NOVA 1438331 - CG,msg32281,<CANWE-C=MvSicV5gWoVDmukirb-QFHcgBSK_wiqryf7_+ipWr-A@mail.gmail.com>,"[openstack-dev] [nova] Review priorities as we approach juno-3

--

Le 14 ao?t 2014 22:02, ""Michael Still"" <mikal at stillhq.com> a ?crit :
>
> Hi.
>
> We're rapidly approaching j-3, so I want to remind people of the
> current reviews that are high priority. The definition of high
> priority I am using here is blueprints that are marked high priority
> in launchpad that have outstanding code for review -- I am sure there
> are other reviews that are important as well, but I want us to try to
> land more blueprints than we have so far. These are listed in the
> order they appear in launchpad.
>
> == Compute Manager uses Objects (Juno Work) ==
>
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/compute-manager-objects-juno,n,z
>
> This is ongoing work, but if you're after some quick code review
> points they're very easy to review and help push the project forward
> in an important manner.
>
> == Move Virt Drivers to use Objects (Juno Work) ==
>
> I couldn't actually find any code out for review for this one apart
> from https://review.openstack.org/#/c/94477/, is there more out there?
>
> == Add a virt driver for Ironic ==
>
> This one is in progress, but we need to keep going at it or we wont
> get it merged in time.
>
> * https://review.openstack.org/#/c/111223/ was approved, but a rebased
> ate it. Should be quick to re-approve.
> * https://review.openstack.org/#/c/111423/
> * https://review.openstack.org/#/c/111425/
> * ...there are more reviews in this series, but I'd be super happy to
> see even a few reviewed
>
> == Create Scheduler Python Library ==
>
> * https://review.openstack.org/#/c/82778/
> * https://review.openstack.org/#/c/104556/
>
> (There are a few abandoned patches in this series, I think those two
> are the active ones but please correct me if I am wrong).
>

Nope that's OK, those are the only one implementing the spec. We had a +2
on 82778 but it needed a rebase, FYI.

> == VMware: spawn refactor ==
>
> * https://review.openstack.org/#/c/104145/
> * https://review.openstack.org/#/c/104147/ (Dan Smith's -2 on this one
> seems procedural to me)
> * https://review.openstack.org/#/c/105738/
> * ...another chain with many more patches to review
>
> Thanks,
> Michael
>
> --
> Rackspace Australia
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20140814/240cb35e/attachment.html>
"
NOVA 1438331 - CG,msg32283,<CAEd1pt794YYikAmM1bDp6AyFw0C-hGW3VXjKzhTXgfEZtYeQrA@mail.gmail.com>,"[openstack-dev] [nova] Review priorities as we approach juno-3

--

I have also been reminded that http://54.201.139.117/nova-bugs.html
tracks bugs with outstanding code reviews (click on ""ready for
review""). There are 179 at the moment, so it sure would be cool to
land some bug fixes.

Thanks,
Michael

On Fri, Aug 15, 2014 at 5:57 AM, Michael Still <mikal at stillhq.com> wrote:
> Hi.
>
> We're rapidly approaching j-3, so I want to remind people of the
> current reviews that are high priority. The definition of high
> priority I am using here is blueprints that are marked high priority
> in launchpad that have outstanding code for review -- I am sure there
> are other reviews that are important as well, but I want us to try to
> land more blueprints than we have so far. These are listed in the
> order they appear in launchpad.
>
> == Compute Manager uses Objects (Juno Work) ==
>
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/compute-manager-objects-juno,n,z
>
> This is ongoing work, but if you're after some quick code review
> points they're very easy to review and help push the project forward
> in an important manner.
>
> == Move Virt Drivers to use Objects (Juno Work) ==
>
> I couldn't actually find any code out for review for this one apart
> from https://review.openstack.org/#/c/94477/, is there more out there?
>
> == Add a virt driver for Ironic ==
>
> This one is in progress, but we need to keep going at it or we wont
> get it merged in time.
>
> * https://review.openstack.org/#/c/111223/ was approved, but a rebased
> ate it. Should be quick to re-approve.
> * https://review.openstack.org/#/c/111423/
> * https://review.openstack.org/#/c/111425/
> * ...there are more reviews in this series, but I'd be super happy to
> see even a few reviewed
>
> == Create Scheduler Python Library ==
>
> * https://review.openstack.org/#/c/82778/
> * https://review.openstack.org/#/c/104556/
>
> (There are a few abandoned patches in this series, I think those two
> are the active ones but please correct me if I am wrong).
>
> == VMware: spawn refactor ==
>
> * https://review.openstack.org/#/c/104145/
> * https://review.openstack.org/#/c/104147/ (Dan Smith's -2 on this one
> seems procedural to me)
> * https://review.openstack.org/#/c/105738/
> * ...another chain with many more patches to review
>
> Thanks,
> Michael
>
> --
> Rackspace Australia



-- 
Rackspace Australia

"
NOVA 1443697 - CG,msg09183,<CANCY3ed2KTw0xMQa2Bf7xxFMEsLwoo+7=mujmrz4cztjgL7P8w@mail.gmail.com>,"[openstack-dev] [nova] V3 API blueprints

--

Hi,

Some people have expressed interest in helping out with the V3 API related
work during Icehouse. The following are the relevant blueprints based on
ongoing work from Havana and discussions during the summit sessions:

Ongoing blueprints

https://blueprints.launchpad.net/nova/+spec/nova-v3-api (capstone blueprint)
https://blueprints.launchpad.net/nova/+spec/v3-api-cleanup-misc
https://blueprints.launchpad.net/nova/+spec/v3-api-specification
https://blueprints.launchpad.net/python-novaclient/+spec/v3-api
https://blueprints.launchpad.net/tempest/+spec/nova-v3-api-tests
https://blueprints.launchpad.net/nova/+spec/nova-api-validation-fw

New in Icehouse

https://blueprints.launchpad.net/nova/+spec/api-v3-remove-disk-config
https://blueprints.launchpad.net/nova/+spec/v3-api-core
https://blueprints.launchpad.net/nova/+spec/v3-api-admin-actions-split
https://blueprints.launchpad.net/nova/+spec/v3-api-remove-nova-network
https://blueprints.launchpad.net/nova/+spec/v3-api-remove-extensions
https://blueprints.launchpad.net/nova/+spec/v3-api-pecan
https://blueprints.launchpad.net/nova/+spec/v3-api-policy
https://blueprints.launchpad.net/nova/+spec/v3-api-core-policy

There'll be a few people working on these so if you decide to work on
something please add a relevant entry in the work item section before
starting in order to avoid duplication of effort. Please ping me if you
think anything needs further explanation
or you want a bit of a hand getting started on something.

Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20131119/316e6b5c/attachment.html>
"
NOVA 1443697 - CG,msg51155,<5537A2FD.2050907@redhat.com>,"[openstack-dev] [nova] Policy rules are killed by the context admin
	check

--

Hi,

By discussing on a specific bug [1], I just discovered that the admin 
context check which was done at the DB level has been moved to the API 
level thanks to the api-policy-v3 blueprint [2]

That behaviour still leads to a bug if the operator wants to change an 
endpoint policy by leaving it end-user but still continues to be denied 
because of that, as it will forbid any non-admin user to call the 
methods (even if authorize() grants the request)

I consequently opened a bug [3] for this but I'm also concerned about 
the backportability of that and why it shouldn't fixed in v2.0 too.

Releasing the check by removing it sounds an acceptable change, as it 
fixes a bug without changing the expected behaviour [4]. The impact of 
the change sounds also minimal with a very precise scope (ie. leave the 
policy rules work as they are expected) [5]

Folks, thoughts ?

-Sylvain

[1] https://bugs.launchpad.net/nova/+bug/1447084
[2] 
https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
[3] https://bugs.launchpad.net/nova/+bug/1447164
[4] 
https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK 
""Fixing a bug so that a request which resulted in an error response 
before is now successful""
[5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy

"
NOVA 1443697 - CG,msg51212,<553826F8.9040904@linux.vnet.ibm.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--



On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
> Hi,
>
> By discussing on a specific bug [1], I just discovered that the admin
> context check which was done at the DB level has been moved to the API
> level thanks to the api-policy-v3 blueprint [2]
>
> That behaviour still leads to a bug if the operator wants to change an
> endpoint policy by leaving it end-user but still continues to be denied
> because of that, as it will forbid any non-admin user to call the
> methods (even if authorize() grants the request)
>
> I consequently opened a bug [3] for this but I'm also concerned about
> the backportability of that and why it shouldn't fixed in v2.0 too.
>
> Releasing the check by removing it sounds an acceptable change, as it
> fixes a bug without changing the expected behaviour [4]. The impact of
> the change sounds also minimal with a very precise scope (ie. leave the
> policy rules work as they are expected) [5]
>
> Folks, thoughts ?
>
> -Sylvain
>
> [1] https://bugs.launchpad.net/nova/+bug/1447084
> [2]
> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
> [3] https://bugs.launchpad.net/nova/+bug/1447164
> [4]
> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
> ""Fixing a bug so that a request which resulted in an error response
> before is now successful""
> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

I don't disagree, see bug 1168488 from way back in grizzly.

The only thing would be we'd have to make sure the default rule is admin 
for any v2 extensions which are enforcing an admin context today.

-- 

Thanks,

Matt Riedemann


"
NOVA 1443697 - CG,msg51213,<CAGnj6auHXehCyr_b5tYUWVVEOFz9eoDMO-GngBiwxpA4Bij_DQ@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

On Wednesday, April 22, 2015, Matt Riedemann <mriedem at linux.vnet.ibm.com>
wrote:

>
>
> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>
>> Hi,
>>
>> By discussing on a specific bug [1], I just discovered that the admin
>> context check which was done at the DB level has been moved to the API
>> level thanks to the api-policy-v3 blueprint [2]
>>
>> That behaviour still leads to a bug if the operator wants to change an
>> endpoint policy by leaving it end-user but still continues to be denied
>> because of that, as it will forbid any non-admin user to call the
>> methods (even if authorize() grants the request)
>>
>> I consequently opened a bug [3] for this but I'm also concerned about
>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>
>> Releasing the check by removing it sounds an acceptable change, as it
>> fixes a bug without changing the expected behaviour [4]. The impact of
>> the change sounds also minimal with a very precise scope (ie. leave the
>> policy rules work as they are expected) [5]
>>
>> Folks, thoughts ?
>>
>> -Sylvain
>>
>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>> [2]
>>
>> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>> [4]
>>
>> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>> ""Fixing a bug so that a request which resulted in an error response
>> before is now successful""
>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>
>> __________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
>> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
> I don't disagree, see bug 1168488 from way back in grizzly.
>
> The only thing would be we'd have to make sure the default rule is admin
> for any v2 extensions which are enforcing an admin context today.
>
>
This sounds like a sane approach.

--Morgan

> --
>
> Thanks,
>
> Matt Riedemann
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150422/8769514e/attachment.html>
"
NOVA 1443697 - CG,msg51240,<CAH7mGatE4Ua_fxmrAHCC6kbFE_uHnvoJ8u5vxVNFnOAQdrcVsg@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com>:

>
>
> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>
>> Hi,
>>
>> By discussing on a specific bug [1], I just discovered that the admin
>> context check which was done at the DB level has been moved to the API
>> level thanks to the api-policy-v3 blueprint [2]
>>
>> That behaviour still leads to a bug if the operator wants to change an
>> endpoint policy by leaving it end-user but still continues to be denied
>> because of that, as it will forbid any non-admin user to call the
>> methods (even if authorize() grants the request)
>>
>> I consequently opened a bug [3] for this but I'm also concerned about
>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>
>> Releasing the check by removing it sounds an acceptable change, as it
>> fixes a bug without changing the expected behaviour [4]. The impact of
>> the change sounds also minimal with a very precise scope (ie. leave the
>> policy rules work as they are expected) [5]
>>
>> Folks, thoughts ?
>>
>> -Sylvain
>>
>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>> [2]
>>
>> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>> [4]
>>
>> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>> ""Fixing a bug so that a request which resulted in an error response
>> before is now successful""
>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>
>> __________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
>> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>
>>
> I don't disagree, see bug 1168488 from way back in grizzly.
>
> The only thing would be we'd have to make sure the default rule is admin
> for any v2 extensions which are enforcing an admin context today.


Agree, if we want to fix those for v2, we need make sure the default rule
is admin.

And do you mean [3] want to fix this for v2 both in Kilo and Liberty?

For liberty, we can do that, but I think we will switch to v2.1 very soon.
Not sure it is still worth to do that.

For kilo, some of api is pretty easy to fix by just removing
'require_admin_context()'. But there still have many of policy patches
didn't merged into the master yet. like:
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z

Should we back-port them all?


>
> --
>
> Thanks,
>
> Matt Riedemann
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150423/ca1ac933/attachment.html>
"
NOVA 1443697 - CG,msg51248,<CANWE-CnOs0L454G3VpHcV4-+oPDOn9vVar+P49pUZjar1GFswg@mail.gmail.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--

Le 23 avr. 2015 04:49, ""Alex Xu"" <soulxu at gmail.com> a ?crit :
>
>
>
> 2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com>:
>>
>>
>>
>> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
>>>
>>> Hi,
>>>
>>> By discussing on a specific bug [1], I just discovered that the admin
>>> context check which was done at the DB level has been moved to the API
>>> level thanks to the api-policy-v3 blueprint [2]
>>>
>>> That behaviour still leads to a bug if the operator wants to change an
>>> endpoint policy by leaving it end-user but still continues to be denied
>>> because of that, as it will forbid any non-admin user to call the
>>> methods (even if authorize() grants the request)
>>>
>>> I consequently opened a bug [3] for this but I'm also concerned about
>>> the backportability of that and why it shouldn't fixed in v2.0 too.
>>>
>>> Releasing the check by removing it sounds an acceptable change, as it
>>> fixes a bug without changing the expected behaviour [4]. The impact of
>>> the change sounds also minimal with a very precise scope (ie. leave the
>>> policy rules work as they are expected) [5]
>>>
>>> Folks, thoughts ?
>>>
>>> -Sylvain
>>>
>>> [1] https://bugs.launchpad.net/nova/+bug/1447084
>>> [2]
>>>
https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>>
>>> [3] https://bugs.launchpad.net/nova/+bug/1447164
>>> [4]
>>>
https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
>>> ""Fixing a bug so that a request which resulted in an error response
>>> before is now successful""
>>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
>>>
>>>
__________________________________________________________________________
>>> OpenStack Development Mailing List (not for usage questions)
>>> Unsubscribe:
OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>>>
>>
>> I don't disagree, see bug 1168488 from way back in grizzly.
>>
>> The only thing would be we'd have to make sure the default rule is admin
for any v2 extensions which are enforcing an admin context today.
>
>
> Agree, if we want to fix those for v2, we need make sure the default rule
is admin.
>
> And do you mean [3] want to fix this for v2 both in Kilo and Liberty?
>
> For liberty, we can do that, but I think we will switch to v2.1 very
soon. Not sure it is still worth to do that.
>
> For kilo, some of api is pretty easy to fix by just removing
'require_admin_context()'. But there still have many of policy patches
didn't merged into the master yet. like:
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
>
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z
>
> Should we back-port them all?

Wrt all the necessary backports, I'm eventually rather in favor of an
opportunistic approach and only backport what has been reported as a bug,
ie. [1]

That has also the benefit of not proposing a stable patch which was not
cherry-picked (like providing an elevated context), which I disapprove.

-Sylvain

>
>>
>>
>> --
>>
>> Thanks,
>>
>> Matt Riedemann
>>
>>
>>
>>
__________________________________________________________________________
>> OpenStack Development Mailing List (not for usage questions)
>> Unsubscribe:
OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150423/431de072/attachment.html>
"
NOVA 1443697 - CG,msg51552,<553E39D7.9080909@redhat.com>,"[openstack-dev] [nova] Policy rules are killed by the context
 admin check

--



Le 23/04/2015 09:10, Sylvain Bauza a ?crit :
>
>
> Le 23 avr. 2015 04:49, ""Alex Xu"" <soulxu at gmail.com 
> <mailto:soulxu at gmail.com>> a ?crit :
> >
> >
> >
> > 2015-04-23 6:55 GMT+08:00 Matt Riedemann <mriedem at linux.vnet.ibm.com 
> <mailto:mriedem at linux.vnet.ibm.com>>:
> >>
> >>
> >>
> >> On 4/22/2015 8:32 AM, Sylvain Bauza wrote:
> >>>
> >>> Hi,
> >>>
> >>> By discussing on a specific bug [1], I just discovered that the admin
> >>> context check which was done at the DB level has been moved to the API
> >>> level thanks to the api-policy-v3 blueprint [2]
> >>>
> >>> That behaviour still leads to a bug if the operator wants to change an
> >>> endpoint policy by leaving it end-user but still continues to be 
> denied
> >>> because of that, as it will forbid any non-admin user to call the
> >>> methods (even if authorize() grants the request)
> >>>
> >>> I consequently opened a bug [3] for this but I'm also concerned about
> >>> the backportability of that and why it shouldn't fixed in v2.0 too.
> >>>
> >>> Releasing the check by removing it sounds an acceptable change, as it
> >>> fixes a bug without changing the expected behaviour [4]. The impact of
> >>> the change sounds also minimal with a very precise scope (ie. 
> leave the
> >>> policy rules work as they are expected) [5]
> >>>
> >>> Folks, thoughts ?
> >>>
> >>> -Sylvain
> >>>
> >>> [1] https://bugs.launchpad.net/nova/+bug/1447084
> >>> [2]
> >>> 
> https://review.openstack.org/#/q/project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
> >>>
> >>> [3] https://bugs.launchpad.net/nova/+bug/1447164
> >>> [4]
> >>> 
> https://wiki.openstack.org/wiki/APIChangeGuidelines#Generally_Considered_OK
> >>> ""Fixing a bug so that a request which resulted in an error response
> >>> before is now successful""
> >>> [5] https://wiki.openstack.org/wiki/StableBranch#Stable_branch_policy
> >>>
> >>> 
> __________________________________________________________________________
> >>> OpenStack Development Mailing List (not for usage questions)
> >>> Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> >>> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >>>
> >>
> >> I don't disagree, see bug 1168488 from way back in grizzly.
> >>
> >> The only thing would be we'd have to make sure the default rule is 
> admin for any v2 extensions which are enforcing an admin context today.
> >
> >
> > Agree, if we want to fix those for v2, we need make sure the default 
> rule is admin.
> >
> > And do you mean [3] want to fix this for v2 both in Kilo and Liberty?
> >
> > For liberty, we can do that, but I think we will switch to v2.1 very 
> soon. Not sure it is still worth to do that.
> >
> > For kilo, some of api is pretty easy to fix by just removing 
> 'require_admin_context()'. But there still have many of policy patches 
> didn't merged into the master yet. like:
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/nova-api-policy-final-part,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_qutoa_hardcode_permission,n,z
> > 
> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:remove_quotaclass_hardcode_permission,n,z
> >
> > Should we back-port them all?
>
> Wrt all the necessary backports, I'm eventually rather in favor of an 
> opportunistic approach and only backport what has been reported as a 
> bug, ie. [1]
>
> That has also the benefit of not proposing a stable patch which was 
> not cherry-picked (like providing an elevated context), which I 
> disapprove.
>
> -Sylvain
>

Just a follow-up on that, I'm proposing a change to the approved spec to 
match that discussion :
https://review.openstack.org/177764

Basically, the idea is not to backport all of the efforts, just make 
sure that default policies are admin-only for the corresponding API 
endpoints that call DB API methods which are decorated by 
require_admin_context()

-Sylvain

> >
> >>
> >>
> >> --
> >>
> >> Thanks,
> >>
> >> Matt Riedemann
> >>
> >>
> >>
> >> 
> __________________________________________________________________________
> >> OpenStack Development Mailing List (not for usage questions)
> >> Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> >> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >
> >
> >
> > 
> __________________________________________________________________________
> > OpenStack Development Mailing List (not for usage questions)
> > Unsubscribe: 
> OpenStack-dev-request at lists.openstack.org?subject:unsubscribe 
> <http://OpenStack-dev-request at lists.openstack.org?subject:unsubscribe>
> > http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
> >
>
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150427/852d7f2f/attachment.html>
"
NOVA 1443697 - CG,msg45356,<54D53AC1.7060902@linux.vnet.ibm.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
	stand up?

--

I'm not going to hide it, I don't know what's going on with the v2.1 API 
status, i.e. what is the criteria to that thing dropping it's 
'experimental' label?

I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I 
was a bit lost. It's not my fault, I'm more good looks than brains. :)

When I look at approved specs for Kilo, three pop out:

1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api

2. https://blueprints.launchpad.net/nova/+spec/api-microversions

3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy

The only one of those that has a dependency in launchpad is the last one 
and it's dependency is on:

https://blueprints.launchpad.net/nova/+spec/nova-v3-api

Which looks like it was replaced by the v2-on-v3-api blueprint.

If I look at the open changes for each, there are a lot:

1. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z

2. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/api-microversions,n,z

3. 
https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z

Do those all need to merge before the v2.1 API is no longer experimental?

Is the, for lack of a better term, 'completion criteria', being tracked 
in an etherpad or wiki page somewhere?  I see stuff in the priorities 
etherpad https://etherpad.openstack.org/p/kilo-nova-priorities-tracking 
but it's not clear to me at a high level what makes v2.1 no longer 
experimental.

Can someone provide that in less than 500 words?

-- 

Thanks,

Matt Riedemann


"
NOVA 1443697 - CG,msg45386,<CANCY3ee4zZN0eccxoYhQQNrQV0NV3TsOW108g0+HkagSM_6QFw@mail.gmail.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
 stand up?

--

Hi,

On Sat, Feb 7, 2015 at 8:35 AM, Matt Riedemann <mriedem at linux.vnet.ibm.com>
wrote:

> I'm not going to hide it, I don't know what's going on with the v2.1 API
> status, i.e. what is the criteria to that thing dropping it's
> 'experimental' label?
>
>
So I caught up with Matt on IRC, repeating some references and discussion
here for everyone else


> I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I
> was a bit lost. It's not my fault, I'm more good looks than brains. :)
>
> When I look at approved specs for Kilo, three pop out:
>
> 1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
>
> 2. https://blueprints.launchpad.net/nova/+spec/api-microversions
>
> 3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy
>
>
So we need the first to blueprints for v2.1 microversions. We don't need
v3-api-policy merged to release
 v2.1 microversions though I believe it is a separate important bit of work
to reduce tech debt and make
life easier for operators.




> The only one of those that has a dependency in launchpad is the last one
> and it's dependency is on:
>
> https://blueprints.launchpad.net/nova/+spec/nova-v3-api
>
> Which looks like it was replaced by the v2-on-v3-api blueprint.
>
> If I look at the open changes for each, there are a lot:
>
> 1. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z
>
> 2. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/api-microversions,n,z
>
> 3. https://review.openstack.org/#/q/status:open+project:
> openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>
> Do those all need to merge before the v2.1 API is no longer experimental?
>
>
We have an etherpad here which tracks our release criteria for v2.1 and
microversions:

https://etherpad.openstack.org/p/v2_1_ReleaseCriteria

As mentioned above it doesn't include api-policy

To make life easier for us I'd also like to request that if you review a
changeset that modifies the v2 api that you ensure it also if required is
applied to v2.1(v3 code). If it doesn't apply to v3 then ensure a
v2-only tag is in the commit message. That will help us verify v2 does not
diverge from v2.1 just before
release. After that I think v2 code will be essentially frozen except for
bug fixes and any api changes will
only be made through microversions.

Regards,

Chris

Is the, for lack of a better term, 'completion criteria', being tracked in
> an etherpad or wiki page somewhere?  I see stuff in the priorities etherpad
> https://etherpad.openstack.org/p/kilo-nova-priorities-tracking but it's
> not clear to me at a high level what makes v2.1 no longer experimental.
>
> Can someone provide that in less than 500 words?
>
> --
>
> Thanks,
>
> Matt Riedemann
>
>
> __________________________________________________________________________
> OpenStack Development Mailing List (not for usage questions)
> Unsubscribe: OpenStack-dev-request at lists.openstack.org?subject:unsubscribe
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150209/32a0a5af/attachment.html>
"
NOVA 1443697 - CG,msg45388,<CAA393viO0_oZed6QKO9AZrrDyXgbq--LzmdprjtbPrSHoEkJWw@mail.gmail.com>,"[openstack-dev] [nova] will the real v2.1/v3 API status please
 stand up?

--

2015-02-09 9:26 GMT+09:00 Christopher Yeoh <cbkyeoh at gmail.com>:
>>
>> I wasn't at the mid-cycle meetup for Kilo but even for Juno I'll admit I
>> was a bit lost. It's not my fault, I'm more good looks than brains. :)
>>
>> When I look at approved specs for Kilo, three pop out:
>>
>> 1. https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
>>
>> 2. https://blueprints.launchpad.net/nova/+spec/api-microversions
>>
>> 3. https://blueprints.launchpad.net/nova/+spec/v3-api-policy
>>
>
> So we need the first to blueprints for v2.1 microversions. We don't need
> v3-api-policy merged to release
>  v2.1 microversions though I believe it is a separate important bit of work
> to reduce tech debt and make
> life easier for operators.

I think the first one(bp/v2-on-v3-api) only is necessary for dropping
'experimental' label from v2.1 API, because raw v2.1 API(meaning
v2.1 without microversions) has already implemented some merits
(strong validation, clean code, easy deployment, etc) for developers
and operators.

>> The only one of those that has a dependency in launchpad is the last one
>> and it's dependency is on:
>>
>> https://blueprints.launchpad.net/nova/+spec/nova-v3-api
>>
>> Which looks like it was replaced by the v2-on-v3-api blueprint.
>>
>> If I look at the open changes for each, there are a lot:
>>
>> 1.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v2-on-v3-api,n,z
>>
>> 2.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/api-microversions,n,z
>>
>> 3.
>> https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/v3-api-policy,n,z
>>
>> Do those all need to merge before the v2.1 API is no longer experimental?

As I said at the above, the first branch is necessary for dropping
experimental from v2.1 API. In addition, the first branch contains
some cleanup patches also. The important patches are just four:

* Provide full v2 compatibility
  https://review.openstack.org/#/c/138599/
  https://review.openstack.org/#/c/153466/
  https://review.openstack.org/#/c/153137

* Drop experimental from v2
  https://review.openstack.org/149948

So we have already done most works for v2.1 API.

Thanks
Ken Ohmichi

"
NOVA 1443970,msg58950,<CAM25k1Q=q-NU2jd==Opj+CNLWwMcS54SWFQne-+WOsPK8LM52w@mail.gmail.com>,"[openstack-dev]  [nova] [stable] Freze exception

--

Hello

I'd like to ask for a freeze exception for the follow bug fix:

https://review.openstack.org/#/c/198385/

bug: https://bugs.launchpad.net/nova/+bug/1443970

merged bug fix in master: https://review.openstack.org/#/c/173362/

Incorrect usage of argument 'dhcp_server' may be cause of some problems
when we using nova-network. Please consider this bug fix to be a part of
Kilo.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.openstack.org/pipermail/openstack-dev/attachments/20150721/f1c24f6f/attachment.html>
"
NOVA 1486541 - CG,msg11310,<52A78D60.204@linux.vnet.ibm.com>,"[openstack-dev] [Nova][Cells] compute api and objects

--



On Monday, December 09, 2013 4:58:31 PM, Sam Morrison wrote:
> Hi,
>
> I?m trying to fix up some cells issues related to objects. Do all compute api methods take objects now?
> cells is still sending DB objects for most methods (except start and stop) and I know there are more than that.
>
> Eg. I know lock/unlock, shelve/unshelve take objects, I assume there are others if not all methods now?
>
> Cheers,
> Sam
>
>
>
> _______________________________________________
> OpenStack-dev mailing list
> OpenStack-dev at lists.openstack.org
> http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev
>

I don't know the answer about cells, but posting a few bugs you've 
opened on the topic:

https://bugs.launchpad.net/nova/+bug/1251043
https://bugs.launchpad.net/nova/+bug/1257168

As for ""Do all compute api methods take objects now?"", I believe the 
answer is 'no'.  There are still some objects blueprints in the works.  
Here is a big one:

https://blueprints.launchpad.net/nova/+spec/compute-manager-objects

--

Thanks,

Matt Riedemann


"
NOVA 1505471 - CG,msg02715,<074729CE-981E-443B-BDB2-AD543F5A8375@linux.vnet.ibm.com>,"[openstack-dev] Change in openstack/keystone[master]: Implement
	domain specific Identity backends

--

Hi Mark,

Of particular interest are your views on the changes to keystone/common/config.py.  The requirement is that we need to be able to instantiate multiple conf objects (built from different sets of config files).  We tried two approaches to this:

https://review.openstack.org/#/c/39530/11 which attempts to keep the current keystone config helper apps (register_bool() etc.) by passing on the conf instance, and
https://review.openstack.org/#/c/39530/12 which removes these helper apps and just calls the methods on the conf itself (conf.register_opt())

Both functionally work, but interested in your views on both approaches.

Henry
On 6 Aug 2013, at 19:26, ayoung (Code Review) wrote:

> Hello Mark McLoughlin,
> 
> I'd like you to do a code review.  Please visit
> 
>   https://review.openstack.org/39530
> 
> to review the following change.
> 
> Change subject: Implement domain specific Identity backends
> ......................................................................
> 
> Implement domain specific Identity backends
> 
> A common scenario in shared clouds will be that a cloud provider will
> want to be able to offer larger customers the ability to interface to
> their chosen identity provider. In the base case, this might well be
> their own corporate LDAP/AD directory.  A cloud provider might also
> want smaller customers to have their identity managed solely
> within the OpenStack cloud, perhaps in a shared SQL database.
> 
> This patch allows domain specifc backends for identity objects
> (namely User and groups), which are specified by creation of a domain
> configuration file for each domain that requires its own backend.
> 
> A side benefit of this change is that it clearly separates the
> backends into those that are domain-aware and those that are not,
> allowing, for example, the removal of domain validation from the
> LDAP identity backend.
> 
> Implements bp multiple-ldap-servers
> 
> Change-Id: I489e8e50035f88eca4235908ae8b1a532645daab
> ---
> M doc/source/configuration.rst
> M etc/keystone.conf.sample
> M keystone/auth/plugins/password.py
> M keystone/catalog/backends/templated.py
> M keystone/common/config.py
> M keystone/common/controller.py
> M keystone/common/ldap/fakeldap.py
> M keystone/common/utils.py
> M keystone/config.py
> M keystone/identity/backends/kvs.py
> M keystone/identity/backends/ldap.py
> M keystone/identity/backends/pam.py
> M keystone/identity/backends/sql.py
> M keystone/identity/controllers.py
> M keystone/identity/core.py
> M keystone/test.py
> M keystone/token/backends/memcache.py
> M keystone/token/core.py
> A tests/backend_multi_ldap_sql.conf
> A tests/keystone.Default.conf
> A tests/keystone.domain1.conf
> A tests/keystone.domain2.conf
> M tests/test_backend.py
> M tests/test_backend_ldap.py
> 24 files changed, 1,028 insertions(+), 372 deletions(-)
> 
> 
> git pull ssh://review.openstack.org:29418/openstack/keystone refs/changes/30/39530/12
> --
> To view, visit https://review.openstack.org/39530
> To unsubscribe, visit https://review.openstack.org/settings
> 
> Gerrit-MessageType: newchange
> Gerrit-Change-Id: I489e8e50035f88eca4235908ae8b1a532645daab
> Gerrit-PatchSet: 12
> Gerrit-Project: openstack/keystone
> Gerrit-Branch: master
> Gerrit-Owner: henry-nash <henryn at linux.vnet.ibm.com>
> Gerrit-Reviewer: Brant Knudson <bknudson at us.ibm.com>
> Gerrit-Reviewer: Dolph Mathews <dolph.mathews at gmail.com>
> Gerrit-Reviewer: Jenkins
> Gerrit-Reviewer: Mark McLoughlin <markmc at redhat.com>
> Gerrit-Reviewer: Sahdev Zala <spzala at us.ibm.com>
> Gerrit-Reviewer: SmokeStack
> Gerrit-Reviewer: ayoung <ayoung at redhat.com>
> Gerrit-Reviewer: henry-nash <henryn at linux.vnet.ibm.com>
> 



"
NOVA 1505471 - CG,msg02913,<1376001320.24389.285.camel@sorcha>,"[openstack-dev] Change in openstack/keystone[master]: Implement
 domain specific Identity backends

--

Hi Henry,

On Tue, 2013-08-06 at 22:10 +0100, Henry Nash wrote:
> Hi Mark,
> 
> Of particular interest are your views on the changes to keystone/common/config.py.  The requirement is that we need to be able to instantiate multiple conf objects (built from different sets of config files).  We tried two approaches to this:
> 
> https://review.openstack.org/#/c/39530/11 which attempts to keep the current keystone config helper apps (register_bool() etc.) by passing on the conf instance, and
> https://review.openstack.org/#/c/39530/12 which removes these helper apps and just calls the methods on the conf itself (conf.register_opt())
> 
> Both functionally work, but interested in your views on both approaches.

Definitely prefer more of the latter, since I've proposed it myself
previously :)

  https://review.openstack.org/4547

There are some common patterns of cfg usage which keystone is unusual in
not adopting:

  - declare options as a list, or multiple lists at the top of modules:

      foo_opts = [
          cfg.StrOpt('bar'),
          cfg.ListOpt('foo'),
      ]

  - declare options in the modules in which they're used, rather than 
    having a single module which declares all options for the project. 
    See this blueprint:
 
      https://blueprints.launchpad.net/nova/+spec/scope-config-opts

    for where I moved all of the option declarations out of the 
    nova.config module.

    I recall this being a problem for keystone recently - I think it 
    may have been a keystone.middleware module imported keystone.config 
    which defined logging options which may have been defined 
    elsewhere. This kind of thing is easier to avoid if the config 
    options are scoped to the module which uses them.

  - iff the code in this module needs to only work with cfg.CONF, then 
    register the options with cfg.CONF at the top of the module:

      CONF = cfg.CONF
      CONF.register_opts(foo_opts)

    otherwise register the options before they're used:

      def bar(conf, ..):
          ...
          conf.register_opts(foo_opts)
          if conf.foo:
              ...

      def blaa(conf, ..):
          ...
          conf.register_opts(foo_opts)
          if conf.bar:
              ...

Hope that helps,
Mark.


"
